{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single AI crypto concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xdata_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pandas/core/arrays/masked.py:59: UserWarning: Pandas requires version '1.3.2' or newer of 'bottleneck' (version '1.2.1' currently installed).\n",
      "  from pandas.core import (\n",
      "2023-03-19 21:08:27.912983: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-19 21:08:27.913392: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-19 21:08:28.276942: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-03-19 21:08:34.046384: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 21:08:34.046734: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-19 21:08:34.046760: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from functions_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_expand5(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function\n",
    "    \"\"\"\n",
    "    This function takes in several parameters to calculate technical indicators and returns a merged dataframe.\n",
    "    \n",
    "    :param pair: str, default \"GMT/USDT\"\n",
    "        The trading pair to analyze.\n",
    "        \n",
    "    :param start_index: int, default 0\n",
    "        The start index for selecting data.\n",
    "        \n",
    "    :param end_index: int, default 10000\n",
    "        The end index for selecting data.\n",
    "    \n",
    "    :param window_size: int, default 2\n",
    "        The window size to use for analyzing the data.\n",
    "    \n",
    "    :param metadata: MetaData\n",
    "        The metadata to use for analyzing the data.\n",
    "    \n",
    "    :param high_weight: int, default 1\n",
    "        The weight to use for calculating the high.\n",
    "    \n",
    "    :param BUY_PCT: float, default BUY_PCT\n",
    "        The buy pct to use for analyzing the data.\n",
    "    \n",
    "    :param SELL_PCT: float, default SELL_PCT\n",
    "        The sell pct to use for analyzing the data.\n",
    "    \n",
    "    :param buy_fn: function, default buy_min_up\n",
    "        The buy function to use for analyzing the data.\n",
    "    \n",
    "    :return: pd.DataFrame\n",
    "        A merged dataframe containing the calculated technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"mini_expand : {pair}\")\n",
    "    # Select data\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    # Calculate technical indicators\n",
    "    pair_full = full_expand(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair], window_size)\n",
    "    btc_full = full_expand(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], window_size)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    open_high_low_close_cols = merged.columns.str.contains(\"open|high|low|close\")\n",
    "    # merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"BTC_price\"] - merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")]) / merged[\"BTC_price\"]\n",
    "    # )\n",
    "    # merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"price\"] - merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")]) / merged[\"price\"]\n",
    "    # )\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  mini_expand5 {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')\n",
    "    return merged\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special list if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Binance_USDT_HALAL.index(\"ROSE/USDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# TICKERS = \"../Binance-Fast-Trade-Bot/volatile_volume_\" + str(date.today()) + \".txt\"\n",
    "# VOLATILE_COINS=[line.strip() for line in open(TICKERS)]\n",
    "# PAIR_WITH=\"USDT\"\n",
    "# VOLATILE_USDT_PAIRS=[coin+\"/USDT\" for coin in VOLATILE_COINS]\n",
    "# VOLATILE_BUSD_PAIRS=[coin+\"/BUSD\" for coin in VOLATILE_COINS]\n",
    "# VOLATILE_USDT_PAIRS\n",
    "\n",
    "# coins_to_download=''\n",
    "# for coin in VOLATILE_COINS:\n",
    "#     coins_to_download=coins_to_download+\" \"+coin\n",
    "# f\"node database/ddargs.js {coins_to_download} {PAIR_WITH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coins_to_download=''\n",
    "# for coin in VOLATILE_COINS:\n",
    "#     coins_to_download=coins_to_download+\" \"+coin\n",
    "# os.system(f\"node database/ddargs.js {coins_to_download} {PAIR_WITH}\")#node database/ddargs.js ORN BUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_list = find_intersection(VOLATILE_USDT_PAIRS,Binance_USDT_HALAL)\n",
    "# #tf = '1m'\n",
    "# oldest_pair = \"BTC/USDT\"\n",
    "# if oldest_pair not in pair_list: pair_list.append(oldest_pair)\n",
    "# df_list1m = {}\n",
    "# df_list1d = {}\n",
    "# df_list1h = {}\n",
    "# df_list5m = {}\n",
    "# df_list15m = {}\n",
    "\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1m', path=\"./database/\")\n",
    "#     df_list1m[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1d', path=\"./database/\")\n",
    "#     df_list1d[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1h', path=\"./database/\")\n",
    "#     df_list1h[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '5m', path=\"./database/\")\n",
    "#     df_list5m[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(\n",
    "#         ccxt.binance(), pair, '15m', path=\"./database/\")\n",
    "#     df_list15m[pair] = df.loc[:]\n",
    "# del(df)\n",
    "# df_list = df_list1m\n",
    "# prerr(\"Data load 100% use df_list1d[\\\"BTC/USDT\\\"] for exemple to access\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Pair</th>\n",
       "      <th>launch_week_stamp</th>\n",
       "      <th>launch_day_stamp</th>\n",
       "      <th>launch_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNMBUSD</td>\n",
       "      <td>SNM/BUSD</td>\n",
       "      <td>1661126400000</td>\n",
       "      <td>1661472000000</td>\n",
       "      <td>2022-08-26 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>1502668800000</td>\n",
       "      <td>1502928000000</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LUNAUSDT</td>\n",
       "      <td>LUNA/USDT</td>\n",
       "      <td>1597622400000</td>\n",
       "      <td>1597968000000</td>\n",
       "      <td>2020-08-21 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>ETH/USDT</td>\n",
       "      <td>1502668800000</td>\n",
       "      <td>1502928000000</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMTUSDT</td>\n",
       "      <td>GMT/USDT</td>\n",
       "      <td>1646611200000</td>\n",
       "      <td>1646784000000</td>\n",
       "      <td>2022-03-09 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>XMRBUSD</td>\n",
       "      <td>XMR/BUSD</td>\n",
       "      <td>1582502400000</td>\n",
       "      <td>1582761600000</td>\n",
       "      <td>2020-02-27 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>RENBUSD</td>\n",
       "      <td>REN/BUSD</td>\n",
       "      <td>1632096000000</td>\n",
       "      <td>1632268800000</td>\n",
       "      <td>2021-09-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>DFBUSD</td>\n",
       "      <td>DF/BUSD</td>\n",
       "      <td>1607299200000</td>\n",
       "      <td>1607644800000</td>\n",
       "      <td>2020-12-11 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>JASMYBUSD</td>\n",
       "      <td>JASMY/BUSD</td>\n",
       "      <td>1637539200000</td>\n",
       "      <td>1637539200000</td>\n",
       "      <td>2021-11-22 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>ZRXBUSD</td>\n",
       "      <td>ZRX/BUSD</td>\n",
       "      <td>1595808000000</td>\n",
       "      <td>1596067200000</td>\n",
       "      <td>2020-07-30 05:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>225 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        Pair  launch_week_stamp  launch_day_stamp  \\\n",
       "0      SNMBUSD    SNM/BUSD      1661126400000     1661472000000   \n",
       "1      BTCUSDT    BTC/USDT      1502668800000     1502928000000   \n",
       "2     LUNAUSDT   LUNA/USDT      1597622400000     1597968000000   \n",
       "3      ETHUSDT    ETH/USDT      1502668800000     1502928000000   \n",
       "4      GMTUSDT    GMT/USDT      1646611200000     1646784000000   \n",
       "..         ...         ...                ...               ...   \n",
       "220    XMRBUSD    XMR/BUSD      1582502400000     1582761600000   \n",
       "221    RENBUSD    REN/BUSD      1632096000000     1632268800000   \n",
       "222     DFBUSD     DF/BUSD      1607299200000     1607644800000   \n",
       "223  JASMYBUSD  JASMY/BUSD      1637539200000     1637539200000   \n",
       "224    ZRXBUSD    ZRX/BUSD      1595808000000     1596067200000   \n",
       "\n",
       "           launch_minute  \n",
       "0    2022-08-26 08:00:00  \n",
       "1    2017-08-17 04:00:00  \n",
       "2    2020-08-21 10:00:00  \n",
       "3    2017-08-17 04:00:00  \n",
       "4    2022-03-09 12:00:00  \n",
       "..                   ...  \n",
       "220  2020-02-27 11:00:00  \n",
       "221  2021-09-22 12:00:00  \n",
       "222  2020-12-11 10:00:00  \n",
       "223  2021-11-22 12:00:00  \n",
       "224  2020-07-30 05:00:00  \n",
       "\n",
       "[225 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chking import\n",
    "MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/UltimeTradingBot/Data/BUY_OPTIMAL'\n",
      "Results dir: /UltimeTradingBot/Data/BUY_OPTIMAL\n"
     ]
    }
   ],
   "source": [
    "if BUY_MODE==\"BUY_ONLY\":\n",
    "    buy_function=buy_up_only\n",
    "elif BUY_MODE==\"BUY_UP\":\n",
    "    buy_function=buy_up\n",
    "elif  BUY_MODE==\"BUY_DIP\":\n",
    "    buy_function=buy_min_up\n",
    "elif  BUY_MODE==\"AFTER_DEPTH\":\n",
    "    buy_function=buy_after_depth\n",
    "elif  BUY_MODE==\"BUY_UP_CLOSE\":\n",
    "    buy_function=buy_up_close\n",
    "elif  BUY_MODE==\"AFTER_DEPTH_CLOSE\":\n",
    "    buy_function=buy_after_depth_close\n",
    "elif  BUY_MODE==\"BUY_TEST\":\n",
    "    buy_function=buy_test\n",
    "elif BUY_MODE==\"BUY_MIN_CLOSE\":\n",
    "    buy_function=buy_min_close\n",
    "elif  BUY_MODE==\"SELL_TEST\":\n",
    "    buy_function=sell_test\n",
    "elif  BUY_MODE==\"BUY_FIX\":\n",
    "    buy_function=buy_fix\n",
    "elif  BUY_MODE==\"BUY_OPTIMAL\":\n",
    "    buy_function=buy_optimal\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(DATA_DIR, mode = 0o777)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(f\"Results dir: {DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: SNM/BUSD -->mini_expand : SNM/BUSD\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.787%\n",
      "######################  mini_expand5 SNM/BUSD - shape (156489, 287)  buy mean : 0.787 ############################\n",
      "df original shape (156489, 287)\n",
      "df original shape buy mean : 0.7872757829623809\n",
      "SNM/BUSD is processed -- 0/112\n",
      "working on: LUNA/USDT -->mini_expand : LUNA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.285%\n",
      "######################  mini_expand5 LUNA/USDT - shape (953805, 287)  buy mean : 0.285 ############################\n",
      "df original shape (953805, 287)\n",
      "df original shape buy mean : 0.2850687509501418\n",
      "LUNA/USDT is processed -- 1/112\n",
      "working on: GMT/USDT -->mini_expand : GMT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.249%\n",
      "######################  mini_expand5 GMT/USDT - shape (358605, 287)  buy mean : 0.249 ############################\n",
      "df original shape (358605, 287)\n",
      "df original shape buy mean : 0.2490205100319293\n",
      "GMT/USDT is processed -- 2/112\n",
      "working on: UST/USDT -->mini_expand : UST/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.010%\n",
      "######################  mini_expand5 UST/USDT - shape (180050, 287)  buy mean : 0.01 ############################\n",
      "df original shape (180050, 287)\n",
      "df original shape buy mean : 0.009997222993612885\n",
      "UST/USDT is processed -- 3/112\n",
      "working on: SOL/USDT -->mini_expand : SOL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.222%\n",
      "######################  mini_expand5 SOL/USDT - shape (980042, 287)  buy mean : 0.222 ############################\n",
      "df original shape (980042, 287)\n",
      "df original shape buy mean : 0.22203130069935778\n",
      "SOL/USDT is processed -- 4/112\n",
      "working on: APE/USDT -->mini_expand : APE/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.243%\n",
      "######################  mini_expand5 APE/USDT - shape (347087, 287)  buy mean : 0.243 ############################\n",
      "df original shape (347087, 287)\n",
      "df original shape buy mean : 0.24316669883919592\n",
      "APE/USDT is processed -- 5/112\n",
      "working on: XRP/USDT -->mini_expand : XRP/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.151%\n",
      "######################  mini_expand5 XRP/USDT - shape (980036, 287)  buy mean : 0.151 ############################\n",
      "df original shape (980036, 287)\n",
      "df original shape buy mean : 0.15132097188266552\n",
      "XRP/USDT is processed -- 6/112\n",
      "working on: IDEX/USDT -->mini_expand : IDEX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.336%\n",
      "######################  mini_expand5 IDEX/USDT - shape (455090, 287)  buy mean : 0.336 ############################\n",
      "df original shape (455090, 287)\n",
      "df original shape buy mean : 0.3355380254455163\n",
      "IDEX/USDT is processed -- 7/112\n",
      "working on: AVAX/USDT -->mini_expand : AVAX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.289%\n",
      "######################  mini_expand5 AVAX/USDT - shape (980043, 287)  buy mean : 0.289 ############################\n",
      "df original shape (980043, 287)\n",
      "df original shape buy mean : 0.2889669126762805\n",
      "AVAX/USDT is processed -- 8/112\n",
      "working on: DOT/USDT -->mini_expand : DOT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.206%\n",
      "######################  mini_expand5 DOT/USDT - shape (980044, 287)  buy mean : 0.206 ############################\n",
      "df original shape (980044, 287)\n",
      "df original shape buy mean : 0.20611319491777919\n",
      "DOT/USDT is processed -- 9/112\n",
      "working on: ADA/USDT -->mini_expand : ADA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.171%\n",
      "######################  mini_expand5 ADA/USDT - shape (980034, 287)  buy mean : 0.171 ############################\n",
      "df original shape (980034, 287)\n",
      "df original shape buy mean : 0.17101447500800993\n",
      "ADA/USDT is processed -- 10/112\n",
      "working on: JASMY/USDT -->mini_expand : JASMY/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.818%\n",
      "######################  mini_expand5 JASMY/USDT - shape (455092, 287)  buy mean : 0.818 ############################\n",
      "df original shape (455092, 287)\n",
      "df original shape buy mean : 0.8176368734233956\n",
      "JASMY/USDT is processed -- 11/112\n",
      "working on: TRX/USDT -->mini_expand : TRX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.051%\n",
      "######################  mini_expand5 TRX/USDT - shape (455093, 287)  buy mean : 0.051 ############################\n",
      "df original shape (455093, 287)\n",
      "df original shape buy mean : 0.051198326495903036\n",
      "TRX/USDT is processed -- 12/112\n",
      "working on: NEAR/USDT -->mini_expand : NEAR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.318%\n",
      "######################  mini_expand5 NEAR/USDT - shape (980048, 287)  buy mean : 0.318 ############################\n",
      "df original shape (980048, 287)\n",
      "df original shape buy mean : 0.31753546765056406\n",
      "NEAR/USDT is processed -- 13/112\n",
      "working on: AXS/USDT -->mini_expand : AXS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.227%\n",
      "######################  mini_expand5 AXS/USDT - shape (455094, 287)  buy mean : 0.227 ############################\n",
      "df original shape (455094, 287)\n",
      "df original shape buy mean : 0.22654660355882522\n",
      "AXS/USDT is processed -- 14/112\n",
      "working on: GAL/USDT -->mini_expand : GAL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.327%\n",
      "######################  mini_expand5 GAL/USDT - shape (276534, 287)  buy mean : 0.327 ############################\n",
      "df original shape (276534, 287)\n",
      "df original shape buy mean : 0.32726536339112006\n",
      "GAL/USDT is processed -- 15/112\n",
      "working on: GALA/USDT -->mini_expand : GALA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.192%\n",
      "######################  mini_expand5 GALA/USDT - shape (455096, 287)  buy mean : 0.192 ############################\n",
      "df original shape (455096, 287)\n",
      "df original shape buy mean : 0.19182765834021834\n",
      "GALA/USDT is processed -- 16/112\n",
      "working on: SHIB/USDT -->mini_expand : SHIB/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.198%\n",
      "######################  mini_expand5 SHIB/USDT - shape (455097, 287)  buy mean : 0.198 ############################\n",
      "df original shape (455097, 287)\n",
      "df original shape buy mean : 0.19819950472097156\n",
      "SHIB/USDT is processed -- 17/112\n",
      "working on: ZIL/USDT -->mini_expand : ZIL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.166%\n",
      "######################  mini_expand5 ZIL/USDT - shape (455098, 287)  buy mean : 0.166 ############################\n",
      "df original shape (455098, 287)\n",
      "df original shape buy mean : 0.1661180668779032\n",
      "ZIL/USDT is processed -- 18/112\n",
      "working on: ENS/USDT -->mini_expand : ENS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.431%\n",
      "######################  mini_expand5 ENS/USDT - shape (455098, 287)  buy mean : 0.431 ############################\n",
      "df original shape (455098, 287)\n",
      "df original shape buy mean : 0.43089620257614847\n",
      "ENS/USDT is processed -- 19/112\n",
      "working on: DOGE/USDT -->mini_expand : DOGE/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.201%\n",
      "######################  mini_expand5 DOGE/USDT - shape (980040, 287)  buy mean : 0.201 ############################\n",
      "df original shape (980040, 287)\n",
      "df original shape buy mean : 0.2012162768866577\n",
      "DOGE/USDT is processed -- 20/112\n",
      "working on: LTC/USDT -->mini_expand : LTC/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.175%\n",
      "######################  mini_expand5 LTC/USDT - shape (980039, 287)  buy mean : 0.175 ############################\n",
      "df original shape (980039, 287)\n",
      "df original shape buy mean : 0.17540118301414537\n",
      "LTC/USDT is processed -- 21/112\n",
      "working on: MANA/USDT -->mini_expand : MANA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.128%\n",
      "######################  mini_expand5 MANA/USDT - shape (449670, 287)  buy mean : 0.128 ############################\n",
      "df original shape (449670, 287)\n",
      "df original shape buy mean : 0.12764916494318054\n",
      "MANA/USDT is processed -- 22/112\n",
      "working on: DAR/USDT -->mini_expand : DAR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.266%\n",
      "######################  mini_expand5 DAR/USDT - shape (455101, 287)  buy mean : 0.266 ############################\n",
      "df original shape (455101, 287)\n",
      "df original shape buy mean : 0.2660947789611537\n",
      "DAR/USDT is processed -- 23/112\n",
      "working on: WAVES/USDT -->mini_expand : WAVES/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.299%\n",
      "######################  mini_expand5 WAVES/USDT - shape (455102, 287)  buy mean : 0.299 ############################\n",
      "df original shape (455102, 287)\n",
      "df original shape buy mean : 0.2994933004029866\n",
      "WAVES/USDT is processed -- 24/112\n",
      "working on: LAZIO/USDT -->mini_expand : LAZIO/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.432%\n",
      "######################  mini_expand5 LAZIO/USDT - shape (455102, 287)  buy mean : 0.432 ############################\n",
      "df original shape (455102, 287)\n",
      "df original shape buy mean : 0.43177133917231736\n",
      "LAZIO/USDT is processed -- 25/112\n",
      "working on: ALICE/USDT -->mini_expand : ALICE/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.282%\n",
      "######################  mini_expand5 ALICE/USDT - shape (455103, 287)  buy mean : 0.282 ############################\n",
      "df original shape (455103, 287)\n",
      "df original shape buy mean : 0.28235366499451775\n",
      "ALICE/USDT is processed -- 26/112\n",
      "working on: ROSE/USDT -->mini_expand : ROSE/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.203%\n",
      "######################  mini_expand5 ROSE/USDT - shape (455103, 287)  buy mean : 0.203 ############################\n",
      "df original shape (455103, 287)\n",
      "df original shape buy mean : 0.20325069270033377\n",
      "ROSE/USDT is processed -- 27/112\n",
      "working on: ZEC/USDT -->mini_expand : ZEC/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.308%\n",
      "######################  mini_expand5 ZEC/USDT - shape (455104, 287)  buy mean : 0.308 ############################\n",
      "df original shape (455104, 287)\n",
      "df original shape buy mean : 0.3076219940936577\n",
      "ZEC/USDT is processed -- 28/112\n",
      "working on: ALGO/USDT -->mini_expand : ALGO/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.146%\n",
      "######################  mini_expand5 ALGO/USDT - shape (455105, 287)  buy mean : 0.146 ############################\n",
      "df original shape (455105, 287)\n",
      "df original shape buy mean : 0.14568066709880137\n",
      "ALGO/USDT is processed -- 29/112\n",
      "working on: GRT/USDT -->mini_expand : GRT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.266%\n",
      "######################  mini_expand5 GRT/USDT - shape (455105, 287)  buy mean : 0.266 ############################\n",
      "df original shape (455105, 287)\n",
      "df original shape buy mean : 0.26631216971907584\n",
      "GRT/USDT is processed -- 30/112\n",
      "working on: PSG/USDT -->mini_expand : PSG/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.454%\n",
      "######################  mini_expand5 PSG/USDT - shape (455106, 287)  buy mean : 0.454 ############################\n",
      "df original shape (455106, 287)\n",
      "df original shape buy mean : 0.4541799053407338\n",
      "PSG/USDT is processed -- 31/112\n",
      "working on: SLP/USDT -->mini_expand : SLP/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 2.134%\n",
      "######################  mini_expand5 SLP/USDT - shape (455106, 287)  buy mean : 2.134 ############################\n",
      "df original shape (455106, 287)\n",
      "df original shape buy mean : 2.1335688828536647\n",
      "SLP/USDT is processed -- 32/112\n",
      "working on: EOS/USDT -->mini_expand : EOS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.139%\n",
      "######################  mini_expand5 EOS/USDT - shape (455107, 287)  buy mean : 0.139 ############################\n",
      "df original shape (455107, 287)\n",
      "df original shape buy mean : 0.13864871337949097\n",
      "EOS/USDT is processed -- 33/112\n",
      "working on: PORTO/USDT -->mini_expand : PORTO/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.434%\n",
      "######################  mini_expand5 PORTO/USDT - shape (449670, 287)  buy mean : 0.434 ############################\n",
      "df original shape (449670, 287)\n",
      "df original shape buy mean : 0.4340961149287255\n",
      "PORTO/USDT is processed -- 34/112\n",
      "working on: ICP/USDT -->mini_expand : ICP/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.350%\n",
      "######################  mini_expand5 ICP/USDT - shape (455108, 287)  buy mean : 0.35 ############################\n",
      "df original shape (455108, 287)\n",
      "df original shape buy mean : 0.3504662629529694\n",
      "ICP/USDT is processed -- 35/112\n",
      "working on: EGLD/USDT -->mini_expand : EGLD/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.205%\n",
      "######################  mini_expand5 EGLD/USDT - shape (980049, 287)  buy mean : 0.205 ############################\n",
      "df original shape (980049, 287)\n",
      "df original shape buy mean : 0.20519382194155597\n",
      "EGLD/USDT is processed -- 36/112\n",
      "working on: XMR/USDT -->mini_expand : XMR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.135%\n",
      "######################  mini_expand5 XMR/USDT - shape (455109, 287)  buy mean : 0.135 ############################\n",
      "df original shape (455109, 287)\n",
      "df original shape buy mean : 0.13535219035439863\n",
      "XMR/USDT is processed -- 37/112\n",
      "working on: KDA/USDT -->mini_expand : KDA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.316%\n",
      "######################  mini_expand5 KDA/USDT - shape (355750, 287)  buy mean : 0.316 ############################\n",
      "df original shape (355750, 287)\n",
      "df original shape buy mean : 0.31623330990864373\n",
      "KDA/USDT is processed -- 38/112\n",
      "working on: ETC/USDT -->mini_expand : ETC/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.190%\n",
      "######################  mini_expand5 ETC/USDT - shape (455111, 287)  buy mean : 0.19 ############################\n",
      "df original shape (455111, 287)\n",
      "df original shape buy mean : 0.19006352296472728\n",
      "ETC/USDT is processed -- 39/112\n",
      "working on: MBOX/USDT -->mini_expand : MBOX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.299%\n",
      "######################  mini_expand5 MBOX/USDT - shape (455111, 287)  buy mean : 0.299 ############################\n",
      "df original shape (455111, 287)\n",
      "df original shape buy mean : 0.2994873778045356\n",
      "MBOX/USDT is processed -- 40/112\n",
      "working on: OGN/USDT -->mini_expand : OGN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.287%\n",
      "######################  mini_expand5 OGN/USDT - shape (455112, 287)  buy mean : 0.287 ############################\n",
      "df original shape (455112, 287)\n",
      "df original shape buy mean : 0.2871820562850463\n",
      "OGN/USDT is processed -- 41/112\n",
      "working on: AR/USDT -->mini_expand : AR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.347%\n",
      "######################  mini_expand5 AR/USDT - shape (455112, 287)  buy mean : 0.347 ############################\n",
      "df original shape (455112, 287)\n",
      "df original shape buy mean : 0.346947564555538\n",
      "AR/USDT is processed -- 42/112\n",
      "working on: GLMR/USDT -->mini_expand : GLMR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.285%\n",
      "######################  mini_expand5 GLMR/USDT - shape (440713, 287)  buy mean : 0.285 ############################\n",
      "df original shape (440713, 287)\n",
      "df original shape buy mean : 0.28453891761758787\n",
      "GLMR/USDT is processed -- 43/112\n",
      "working on: LOKA/USDT -->mini_expand : LOKA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.403%\n",
      "######################  mini_expand5 LOKA/USDT - shape (427754, 287)  buy mean : 0.403 ############################\n",
      "df original shape (427754, 287)\n",
      "df original shape buy mean : 0.4030353894995722\n",
      "LOKA/USDT is processed -- 44/112\n",
      "working on: XLM/USDT -->mini_expand : XLM/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.102%\n",
      "######################  mini_expand5 XLM/USDT - shape (455114, 287)  buy mean : 0.102 ############################\n",
      "df original shape (455114, 287)\n",
      "df original shape buy mean : 0.1015130275051965\n",
      "XLM/USDT is processed -- 45/112\n",
      "working on: MTL/USDT -->mini_expand : MTL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.327%\n",
      "######################  mini_expand5 MTL/USDT - shape (455115, 287)  buy mean : 0.327 ############################\n",
      "df original shape (455115, 287)\n",
      "df original shape buy mean : 0.3269503312349626\n",
      "MTL/USDT is processed -- 46/112\n",
      "working on: SNX/USDT -->mini_expand : SNX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.301%\n",
      "######################  mini_expand5 SNX/USDT - shape (455116, 287)  buy mean : 0.301 ############################\n",
      "df original shape (455116, 287)\n",
      "df original shape buy mean : 0.3005827085841851\n",
      "SNX/USDT is processed -- 47/112\n",
      "working on: PYR/USDT -->mini_expand : PYR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.908%\n",
      "######################  mini_expand5 PYR/USDT - shape (455116, 287)  buy mean : 0.908 ############################\n",
      "df original shape (455116, 287)\n",
      "df original shape buy mean : 0.9079004034136352\n",
      "PYR/USDT is processed -- 48/112\n",
      "working on: DASH/USDT -->mini_expand : DASH/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.214%\n",
      "######################  mini_expand5 DASH/USDT - shape (455117, 287)  buy mean : 0.214 ############################\n",
      "df original shape (455117, 287)\n",
      "df original shape buy mean : 0.21445035012974686\n",
      "DASH/USDT is processed -- 49/112\n",
      "working on: CITY/USDT -->mini_expand : CITY/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.526%\n",
      "######################  mini_expand5 CITY/USDT - shape (433170, 287)  buy mean : 0.526 ############################\n",
      "df original shape (433170, 287)\n",
      "df original shape buy mean : 0.5263522404598656\n",
      "CITY/USDT is processed -- 50/112\n",
      "working on: ASTR/USDT -->mini_expand : ASTR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.634%\n",
      "######################  mini_expand5 ASTR/USDT - shape (371598, 287)  buy mean : 0.634 ############################\n",
      "df original shape (371598, 287)\n",
      "df original shape buy mean : 0.6340184823384409\n",
      "ASTR/USDT is processed -- 51/112\n",
      "working on: IOTA/USDT -->mini_expand : IOTA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.118%\n",
      "######################  mini_expand5 IOTA/USDT - shape (455119, 287)  buy mean : 0.118 ############################\n",
      "df original shape (455119, 287)\n",
      "df original shape buy mean : 0.11755167329863178\n",
      "IOTA/USDT is processed -- 52/112\n",
      "working on: ACM/USDT -->mini_expand : ACM/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.525%\n",
      "######################  mini_expand5 ACM/USDT - shape (445170, 287)  buy mean : 0.525 ############################\n",
      "df original shape (445170, 287)\n",
      "df original shape buy mean : 0.5251926230428825\n",
      "ACM/USDT is processed -- 53/112\n",
      "working on: BAR/USDT -->mini_expand : BAR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.937%\n",
      "######################  mini_expand5 BAR/USDT - shape (455120, 287)  buy mean : 0.937 ############################\n",
      "df original shape (455120, 287)\n",
      "df original shape buy mean : 0.9373352082967129\n",
      "BAR/USDT is processed -- 54/112\n",
      "working on: JUV/USDT -->mini_expand : JUV/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.870%\n",
      "######################  mini_expand5 JUV/USDT - shape (455121, 287)  buy mean : 0.87 ############################\n",
      "df original shape (455121, 287)\n",
      "df original shape buy mean : 0.8703180033441658\n",
      "JUV/USDT is processed -- 55/112\n",
      "working on: SYS/USDT -->mini_expand : SYS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.392%\n",
      "######################  mini_expand5 SYS/USDT - shape (455121, 287)  buy mean : 0.392 ############################\n",
      "df original shape (455121, 287)\n",
      "df original shape buy mean : 0.3922033920649674\n",
      "SYS/USDT is processed -- 56/112\n",
      "working on: RVN/USDT -->mini_expand : RVN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.212%\n",
      "######################  mini_expand5 RVN/USDT - shape (455122, 287)  buy mean : 0.212 ############################\n",
      "df original shape (455122, 287)\n",
      "df original shape buy mean : 0.21225078110924103\n",
      "RVN/USDT is processed -- 57/112\n",
      "working on: MBL/USDT -->mini_expand : MBL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.300%\n",
      "######################  mini_expand5 MBL/USDT - shape (455123, 287)  buy mean : 0.3 ############################\n",
      "df original shape (455123, 287)\n",
      "df original shape buy mean : 0.2999189230164154\n",
      "MBL/USDT is processed -- 58/112\n",
      "working on: REN/USDT -->mini_expand : REN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.402%\n",
      "######################  mini_expand5 REN/USDT - shape (455123, 287)  buy mean : 0.402 ############################\n",
      "df original shape (455123, 287)\n",
      "df original shape buy mean : 0.4023088264051696\n",
      "REN/USDT is processed -- 59/112\n",
      "working on: JST/USDT -->mini_expand : JST/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.130%\n",
      "######################  mini_expand5 JST/USDT - shape (455124, 287)  buy mean : 0.13 ############################\n",
      "df original shape (455124, 287)\n",
      "df original shape buy mean : 0.1302941615911268\n",
      "JST/USDT is processed -- 60/112\n",
      "working on: OMG/USDT -->mini_expand : OMG/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.190%\n",
      "######################  mini_expand5 OMG/USDT - shape (455124, 287)  buy mean : 0.19 ############################\n",
      "df original shape (455124, 287)\n",
      "df original shape buy mean : 0.18983837371793183\n",
      "OMG/USDT is processed -- 61/112\n",
      "working on: ATM/USDT -->mini_expand : ATM/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 1.006%\n",
      "######################  mini_expand5 ATM/USDT - shape (455125, 287)  buy mean : 1.006 ############################\n",
      "df original shape (455125, 287)\n",
      "df original shape buy mean : 1.005877506179621\n",
      "ATM/USDT is processed -- 62/112\n",
      "working on: XEC/USDT -->mini_expand : XEC/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.180%\n",
      "######################  mini_expand5 XEC/USDT - shape (455126, 287)  buy mean : 0.18 ############################\n",
      "df original shape (455126, 287)\n",
      "df original shape buy mean : 0.17995016764588267\n",
      "XEC/USDT is processed -- 63/112\n",
      "working on: STORJ/USDT -->mini_expand : STORJ/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.259%\n",
      "######################  mini_expand5 STORJ/USDT - shape (455126, 287)  buy mean : 0.259 ############################\n",
      "df original shape (455126, 287)\n",
      "df original shape buy mean : 0.25926886180969666\n",
      "STORJ/USDT is processed -- 64/112\n",
      "working on: ZRX/USDT -->mini_expand : ZRX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.216%\n",
      "######################  mini_expand5 ZRX/USDT - shape (455127, 287)  buy mean : 0.216 ############################\n",
      "df original shape (455127, 287)\n",
      "df original shape buy mean : 0.21620338938362257\n",
      "ZRX/USDT is processed -- 65/112\n",
      "working on: SRM/USDT -->mini_expand : SRM/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.214%\n",
      "######################  mini_expand5 SRM/USDT - shape (455127, 287)  buy mean : 0.214 ############################\n",
      "df original shape (455127, 287)\n",
      "df original shape buy mean : 0.2144456382504224\n",
      "SRM/USDT is processed -- 66/112\n",
      "working on: ICX/USDT -->mini_expand : ICX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.492%\n",
      "######################  mini_expand5 ICX/USDT - shape (455128, 287)  buy mean : 0.492 ############################\n",
      "df original shape (455128, 287)\n",
      "df original shape buy mean : 0.49151008068059976\n",
      "ICX/USDT is processed -- 67/112\n",
      "working on: API3/USDT -->mini_expand : API3/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.482%\n",
      "######################  mini_expand5 API3/USDT - shape (426329, 287)  buy mean : 0.482 ############################\n",
      "df original shape (426329, 287)\n",
      "df original shape buy mean : 0.4820221003028178\n",
      "API3/USDT is processed -- 68/112\n",
      "working on: ONT/USDT -->mini_expand : ONT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.132%\n",
      "######################  mini_expand5 ONT/USDT - shape (455129, 287)  buy mean : 0.132 ############################\n",
      "df original shape (455129, 287)\n",
      "df original shape buy mean : 0.13205047360198977\n",
      "ONT/USDT is processed -- 69/112\n",
      "working on: SKL/USDT -->mini_expand : SKL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.449%\n",
      "######################  mini_expand5 SKL/USDT - shape (455130, 287)  buy mean : 0.449 ############################\n",
      "df original shape (455130, 287)\n",
      "df original shape buy mean : 0.4488827368004745\n",
      "SKL/USDT is processed -- 70/112\n",
      "working on: MULTI/USDT -->mini_expand : MULTI/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.536%\n",
      "######################  mini_expand5 MULTI/USDT - shape (318330, 287)  buy mean : 0.536 ############################\n",
      "df original shape (318330, 287)\n",
      "df original shape buy mean : 0.53592184211353\n",
      "MULTI/USDT is processed -- 71/112\n",
      "working on: QTUM/USDT -->mini_expand : QTUM/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.141%\n",
      "######################  mini_expand5 QTUM/USDT - shape (455131, 287)  buy mean : 0.141 ############################\n",
      "df original shape (455131, 287)\n",
      "df original shape buy mean : 0.14105828871248058\n",
      "QTUM/USDT is processed -- 72/112\n",
      "working on: COCOS/USDT -->mini_expand : COCOS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.263%\n",
      "######################  mini_expand5 COCOS/USDT - shape (455132, 287)  buy mean : 0.263 ############################\n",
      "df original shape (455132, 287)\n",
      "df original shape buy mean : 0.26278090751694017\n",
      "COCOS/USDT is processed -- 73/112\n",
      "working on: VOXEL/USDT -->mini_expand : VOXEL/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.368%\n",
      "######################  mini_expand5 VOXEL/USDT - shape (444420, 287)  buy mean : 0.368 ############################\n",
      "df original shape (444420, 287)\n",
      "df original shape buy mean : 0.36767022186220244\n",
      "VOXEL/USDT is processed -- 74/112\n",
      "working on: HIVE/USDT -->mini_expand : HIVE/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.279%\n",
      "######################  mini_expand5 HIVE/USDT - shape (455133, 287)  buy mean : 0.279 ############################\n",
      "df original shape (455133, 287)\n",
      "df original shape buy mean : 0.2794787457732136\n",
      "HIVE/USDT is processed -- 75/112\n",
      "working on: KP3R/USDT -->mini_expand : KP3R/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.371%\n",
      "######################  mini_expand5 KP3R/USDT - shape (455134, 287)  buy mean : 0.371 ############################\n",
      "df original shape (455134, 287)\n",
      "df original shape buy mean : 0.3706600693422157\n",
      "KP3R/USDT is processed -- 76/112\n",
      "working on: ATA/USDT -->mini_expand : ATA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.296%\n",
      "######################  mini_expand5 ATA/USDT - shape (455134, 287)  buy mean : 0.296 ############################\n",
      "df original shape (455134, 287)\n",
      "df original shape buy mean : 0.29595679514165063\n",
      "ATA/USDT is processed -- 77/112\n",
      "working on: STMX/USDT -->mini_expand : STMX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.409%\n",
      "######################  mini_expand5 STMX/USDT - shape (455135, 287)  buy mean : 0.409 ############################\n",
      "df original shape (455135, 287)\n",
      "df original shape buy mean : 0.408889670097883\n",
      "STMX/USDT is processed -- 78/112\n",
      "working on: ADX/USDT -->mini_expand : ADX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.275%\n",
      "######################  mini_expand5 ADX/USDT - shape (455136, 287)  buy mean : 0.275 ############################\n",
      "df original shape (455136, 287)\n",
      "df original shape buy mean : 0.27464318357589823\n",
      "ADX/USDT is processed -- 79/112\n",
      "working on: HIGH/USDT -->mini_expand : HIGH/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 1.425%\n",
      "######################  mini_expand5 HIGH/USDT - shape (455136, 287)  buy mean : 1.425 ############################\n",
      "df original shape (455136, 287)\n",
      "df original shape buy mean : 1.4250685509386205\n",
      "HIGH/USDT is processed -- 80/112\n",
      "working on: NULS/USDT -->mini_expand : NULS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.501%\n",
      "######################  mini_expand5 NULS/USDT - shape (455137, 287)  buy mean : 0.501 ############################\n",
      "df original shape (455137, 287)\n",
      "df original shape buy mean : 0.5007283521225477\n",
      "NULS/USDT is processed -- 81/112\n",
      "working on: MLN/USDT -->mini_expand : MLN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.986%\n",
      "######################  mini_expand5 MLN/USDT - shape (455137, 287)  buy mean : 0.986 ############################\n",
      "df original shape (455137, 287)\n",
      "df original shape buy mean : 0.9856372916286745\n",
      "MLN/USDT is processed -- 82/112\n",
      "working on: YGG/USDT -->mini_expand : YGG/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.521%\n",
      "######################  mini_expand5 YGG/USDT - shape (455138, 287)  buy mean : 0.521 ############################\n",
      "df original shape (455138, 287)\n",
      "df original shape buy mean : 0.5207211878595064\n",
      "YGG/USDT is processed -- 83/112\n",
      "working on: SC/USDT -->mini_expand : SC/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.408%\n",
      "######################  mini_expand5 SC/USDT - shape (455139, 287)  buy mean : 0.408 ############################\n",
      "df original shape (455139, 287)\n",
      "df original shape buy mean : 0.40778751106804734\n",
      "SC/USDT is processed -- 84/112\n",
      "working on: CKB/USDT -->mini_expand : CKB/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.332%\n",
      "######################  mini_expand5 CKB/USDT - shape (455139, 287)  buy mean : 0.332 ############################\n",
      "df original shape (455139, 287)\n",
      "df original shape buy mean : 0.33242591823596745\n",
      "CKB/USDT is processed -- 85/112\n",
      "working on: TOMO/USDT -->mini_expand : TOMO/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.404%\n",
      "######################  mini_expand5 TOMO/USDT - shape (455140, 287)  buy mean : 0.404 ############################\n",
      "df original shape (455140, 287)\n",
      "df original shape buy mean : 0.40361207540536975\n",
      "TOMO/USDT is processed -- 86/112\n",
      "working on: STX/USDT -->mini_expand : STX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.592%\n",
      "######################  mini_expand5 STX/USDT - shape (455140, 287)  buy mean : 0.592 ############################\n",
      "df original shape (455140, 287)\n",
      "df original shape buy mean : 0.5921254998462012\n",
      "STX/USDT is processed -- 87/112\n",
      "working on: FLUX/USDT -->mini_expand : FLUX/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.651%\n",
      "######################  mini_expand5 FLUX/USDT - shape (455141, 287)  buy mean : 0.651 ############################\n",
      "df original shape (455141, 287)\n",
      "df original shape buy mean : 0.6510070505623532\n",
      "FLUX/USDT is processed -- 88/112\n",
      "working on: DNT/USDT -->mini_expand : DNT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.836%\n",
      "######################  mini_expand5 DNT/USDT - shape (416700, 287)  buy mean : 0.836 ############################\n",
      "df original shape (416700, 287)\n",
      "df original shape buy mean : 0.8363330933525318\n",
      "DNT/USDT is processed -- 89/112\n",
      "working on: ORN/USDT -->mini_expand : ORN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.404%\n",
      "######################  mini_expand5 ORN/USDT - shape (455142, 287)  buy mean : 0.404 ############################\n",
      "df original shape (455142, 287)\n",
      "df original shape buy mean : 0.40383001349029535\n",
      "ORN/USDT is processed -- 90/112\n",
      "working on: PLA/USDT -->mini_expand : PLA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.992%\n",
      "######################  mini_expand5 PLA/USDT - shape (455143, 287)  buy mean : 0.992 ############################\n",
      "df original shape (455143, 287)\n",
      "df original shape buy mean : 0.9924353444961254\n",
      "PLA/USDT is processed -- 91/112\n",
      "working on: BADGER/USDT -->mini_expand : BADGER/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.743%\n",
      "######################  mini_expand5 BADGER/USDT - shape (455143, 287)  buy mean : 0.743 ############################\n",
      "df original shape (455143, 287)\n",
      "df original shape buy mean : 0.7430631691578251\n",
      "BADGER/USDT is processed -- 92/112\n",
      "working on: DF/USDT -->mini_expand : DF/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 1.190%\n",
      "######################  mini_expand5 DF/USDT - shape (455144, 287)  buy mean : 1.19 ############################\n",
      "df original shape (455144, 287)\n",
      "df original shape buy mean : 1.1903924911676305\n",
      "DF/USDT is processed -- 93/112\n",
      "working on: MOB/USDT -->mini_expand : MOB/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.375%\n",
      "######################  mini_expand5 MOB/USDT - shape (285225, 287)  buy mean : 0.375 ############################\n",
      "df original shape (285225, 287)\n",
      "df original shape buy mean : 0.37479183101060565\n",
      "MOB/USDT is processed -- 94/112\n",
      "working on: LPT/USDT -->mini_expand : LPT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.297%\n",
      "######################  mini_expand5 LPT/USDT - shape (455145, 287)  buy mean : 0.297 ############################\n",
      "df original shape (455145, 287)\n",
      "df original shape buy mean : 0.29660877302837557\n",
      "LPT/USDT is processed -- 95/112\n",
      "working on: SCRT/USDT -->mini_expand : SCRT/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.274%\n",
      "######################  mini_expand5 SCRT/USDT - shape (426346, 287)  buy mean : 0.274 ############################\n",
      "df original shape (426346, 287)\n",
      "df original shape buy mean : 0.27442499753721156\n",
      "SCRT/USDT is processed -- 96/112\n",
      "working on: RAD/USDT -->mini_expand : RAD/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.529%\n",
      "######################  mini_expand5 RAD/USDT - shape (621579, 287)  buy mean : 0.529 ############################\n",
      "df original shape (621579, 287)\n",
      "df original shape buy mean : 0.5288145191520306\n",
      "RAD/USDT is processed -- 97/112\n",
      "working on: NMR/USDT -->mini_expand : NMR/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.521%\n",
      "######################  mini_expand5 NMR/USDT - shape (455147, 287)  buy mean : 0.521 ############################\n",
      "df original shape (455147, 287)\n",
      "df original shape buy mean : 0.5211503096801693\n",
      "NMR/USDT is processed -- 98/112\n",
      "working on: ELF/USDT -->mini_expand : ELF/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.294%\n",
      "######################  mini_expand5 ELF/USDT - shape (455148, 287)  buy mean : 0.294 ############################\n",
      "df original shape (455148, 287)\n",
      "df original shape buy mean : 0.29440973046130053\n",
      "ELF/USDT is processed -- 99/112\n",
      "working on: TORN/USDT -->mini_expand : TORN/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.669%\n",
      "######################  mini_expand5 TORN/USDT - shape (444420, 287)  buy mean : 0.669 ############################\n",
      "df original shape (444420, 287)\n",
      "df original shape buy mean : 0.6694118176499708\n",
      "TORN/USDT is processed -- 100/112\n",
      "working on: T/USDT -->mini_expand : T/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.650%\n",
      "######################  mini_expand5 T/USDT - shape (375949, 287)  buy mean : 0.65 ############################\n",
      "df original shape (375949, 287)\n",
      "df original shape buy mean : 0.6498221833280577\n",
      "T/USDT is processed -- 101/112\n",
      "working on: QUICK/USDT -->mini_expand : QUICK/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.671%\n",
      "######################  mini_expand5 QUICK/USDT - shape (455150, 287)  buy mean : 0.671 ############################\n",
      "df original shape (455150, 287)\n",
      "df original shape buy mean : 0.6712072942985828\n",
      "QUICK/USDT is processed -- 102/112\n",
      "working on: LSK/USDT -->mini_expand : LSK/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.514%\n",
      "######################  mini_expand5 LSK/USDT - shape (455150, 287)  buy mean : 0.514 ############################\n",
      "df original shape (455150, 287)\n",
      "df original shape buy mean : 0.51389651763155\n",
      "LSK/USDT is processed -- 103/112\n",
      "working on: FIDA/USDT -->mini_expand : FIDA/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.435%\n",
      "######################  mini_expand5 FIDA/USDT - shape (455151, 287)  buy mean : 0.435 ############################\n",
      "df original shape (455151, 287)\n",
      "df original shape buy mean : 0.4354598803474012\n",
      "FIDA/USDT is processed -- 104/112\n",
      "working on: XNO/USDT -->mini_expand : XNO/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.398%\n",
      "######################  mini_expand5 XNO/USDT - shape (416272, 287)  buy mean : 0.398 ############################\n",
      "df original shape (416272, 287)\n",
      "df original shape buy mean : 0.3978168120844064\n",
      "XNO/USDT is processed -- 105/112\n",
      "working on: BTG/USDT -->mini_expand : BTG/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.641%\n",
      "######################  mini_expand5 BTG/USDT - shape (416700, 287)  buy mean : 0.641 ############################\n",
      "df original shape (416700, 287)\n",
      "df original shape buy mean : 0.640748740100792\n",
      "BTG/USDT is processed -- 106/112\n",
      "working on: GHST/USDT -->mini_expand : GHST/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.077%\n",
      "######################  mini_expand5 GHST/USDT - shape (455153, 287)  buy mean : 0.077 ############################\n",
      "df original shape (455153, 287)\n",
      "df original shape buy mean : 0.07667751283634294\n",
      "GHST/USDT is processed -- 107/112\n",
      "working on: EPS/USDT -->mini_expand : EPS/USDT\n",
      "after mark = : 1\n",
      "optimalbuy buy maximum forcast size=7 at 0.92% of the current price \n",
      "Precent Mean: 0.431%\n",
      "######################  mini_expand5 EPS/USDT - shape (174480, 287)  buy mean : 0.431 ############################\n",
      "df original shape (174480, 287)\n",
      "df original shape buy mean : 0.43099495644199914\n",
      "EPS/USDT is processed -- 108/112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf=pd.DataFrame()\n",
    "count=0\n",
    "row_numbers=15000\n",
    "for pair in pair_list:\n",
    "    if pair != \"BTC/USDT\" and pair != \"EUR/USDT\" and pair != \"ETH/USDT\" :\n",
    "        print(\"working on: \"+pair ,end=\" -->\")\n",
    "        try:\n",
    "            \n",
    "            df=mini_expand5(pair=pair,i=0,j=len(df_list1m[pair]),window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function)\n",
    "            print(\"df original shape \"+str(df.shape))\n",
    "            print(f\"df original shape buy mean : {df.buy.mean()*100}\")\n",
    "            df=df.reset_index()\n",
    "            try:df.pop(\"num_index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"date\")\n",
    "            except: pass\n",
    "            df=data_shufler(df)            \n",
    "            #df=data_chooser(df,weight=50,row_numbers=df.buy.sum()*2)\n",
    "            df=data_chooser50(df,row_numbers=row_numbers)\n",
    "            gc.collect()\n",
    "            df=data_cleanup(df)\n",
    "            df=df.dropna()\n",
    "            print(pair+f\" is processed -- {count}/{len(pair_list)}\")\n",
    "            # print(df.iloc[0:1])\n",
    "        except Exception as e:\n",
    "            print(f\"error while processing {pair} {count}/{len(pair_list)}\")\n",
    "            print(e)\n",
    "        xdf=pd.concat([xdf,df],axis=0)\n",
    "        count+=1\n",
    "        del(df)\n",
    "        gc.collect()\n",
    "df=xdf\n",
    "del xdf\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-6_5min</th>\n",
       "      <th>BTC_high-7_5min</th>\n",
       "      <th>BTC_low-7_5min</th>\n",
       "      <th>BTC_close-7_5min</th>\n",
       "      <th>BTC_volume-7_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.336000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.20</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>149.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>484.14547</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>367.34044</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>-597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.942000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1859.10</td>\n",
       "      <td>-0.006458</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.002379</td>\n",
       "      <td>513.60</td>\n",
       "      <td>-0.009517</td>\n",
       "      <td>...</td>\n",
       "      <td>2327.68933</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>1794.86067</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>-420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>169.47344</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>209.14587</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>-786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086850</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>...</td>\n",
       "      <td>44.43141</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>55.46427</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>-314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050738</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>1042202.70</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>238566.20</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>...</td>\n",
       "      <td>87.14242</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>66.51237</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269995</th>\n",
       "      <td>4.232500</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>573.79</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>590.91</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>...</td>\n",
       "      <td>190.90840</td>\n",
       "      <td>-0.003800</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>127.50173</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>-355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269996</th>\n",
       "      <td>0.051873</td>\n",
       "      <td>-0.003807</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>953549.20</td>\n",
       "      <td>-0.004964</td>\n",
       "      <td>-0.002072</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>896115.20</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>...</td>\n",
       "      <td>141.73621</td>\n",
       "      <td>-0.012202</td>\n",
       "      <td>-0.008474</td>\n",
       "      <td>-0.008902</td>\n",
       "      <td>370.41948</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269997</th>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>918.20</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>5223.80</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>...</td>\n",
       "      <td>3812.98248</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.008904</td>\n",
       "      <td>-0.009005</td>\n",
       "      <td>2781.25451</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>-786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269998</th>\n",
       "      <td>6.510000</td>\n",
       "      <td>-0.003072</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>1520.04</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1458.12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>245.17035</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>70.51997</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269999</th>\n",
       "      <td>0.131772</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>1238057.10</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.001043</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>578009.70</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>64.54673</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>-0.002613</td>\n",
       "      <td>113.80173</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3270000 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price    high-1     low-1   close-1    volume-1    high-2  \\\n",
       "0        1.336000  0.000000  0.000000  0.000000       83.20 -0.000749   \n",
       "1        2.942000  0.000000  0.000680  0.000000     1859.10 -0.006458   \n",
       "2        0.042600  0.000000  0.000000  0.000000        0.00  0.000000   \n",
       "3        0.086850  0.000576  0.000576  0.000576        0.00  0.000576   \n",
       "4        0.050738 -0.000049  0.005666  0.000739  1042202.70  0.004090   \n",
       "...           ...       ...       ...       ...         ...       ...   \n",
       "3269995  4.232500  0.002953  0.005316  0.005316      573.79  0.002953   \n",
       "3269996  0.051873 -0.003807  0.000241  0.000048   953549.20 -0.004964   \n",
       "3269997  0.019300  0.000000  0.000000  0.000000      918.20 -0.005181   \n",
       "3269998  6.510000 -0.003072  0.001536  0.001536     1520.04 -0.001536   \n",
       "3269999  0.131772 -0.003017  0.001006  0.000171  1238057.10 -0.001878   \n",
       "\n",
       "            low-2   close-2   volume-2    high-3  ...  BTC_volume-6_5min  \\\n",
       "0       -0.000749 -0.000749     149.50  0.000000  ...          484.14547   \n",
       "1       -0.001360 -0.002379     513.60 -0.009517  ...         2327.68933   \n",
       "2        0.000000  0.000000       0.00  0.000000  ...          169.47344   \n",
       "3        0.000576  0.000576       0.00  0.000576  ...           44.43141   \n",
       "4        0.008426  0.005272  238566.20  0.007046  ...           87.14242   \n",
       "...           ...       ...        ...       ...  ...                ...   \n",
       "3269995  0.005316  0.005316     590.91  0.002953  ...          190.90840   \n",
       "3269996 -0.002072 -0.003036  896115.20 -0.003615  ...          141.73621   \n",
       "3269997 -0.005181 -0.005181    5223.80 -0.005181  ...         3812.98248   \n",
       "3269998  0.000000  0.000000    1458.12  0.000000  ...          245.17035   \n",
       "3269999 -0.001043 -0.001802  578009.70 -0.001878  ...           64.54673   \n",
       "\n",
       "         BTC_high-7_5min  BTC_low-7_5min  BTC_close-7_5min  BTC_volume-7_5min  \\\n",
       "0               0.005987        0.008202          0.007551          367.34044   \n",
       "1               0.000221        0.014053          0.010287         1794.86067   \n",
       "2              -0.000922        0.000811          0.000335          209.14587   \n",
       "3               0.002454        0.004531          0.002615           55.46427   \n",
       "4               0.001242        0.003422          0.001313           66.51237   \n",
       "...                  ...             ...               ...                ...   \n",
       "3269995        -0.003800       -0.002317         -0.002317          127.50173   \n",
       "3269996        -0.012202       -0.008474         -0.008902          370.41948   \n",
       "3269997        -0.015690       -0.008904         -0.009005         2781.25451   \n",
       "3269998         0.003131        0.005387          0.003919           70.51997   \n",
       "3269999        -0.003049       -0.002040         -0.002613          113.80173   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          3     8      22       -597    1  \n",
       "1          2     2      37       -420    1  \n",
       "2          5     3      48       -786    0  \n",
       "3          6    20      45       -314    0  \n",
       "4          5    22      23        316    0  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "3269995    5    23      58       -355    1  \n",
       "3269996    1     2      21        316    1  \n",
       "3269997    3    13      37       -786    1  \n",
       "3269998    2     0      31        348    1  \n",
       "3269999    4     4      34        316    1  \n",
       "\n",
       "[3270000 rows x 287 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index().drop(columns=\"num_index\")\n",
    "gc.collect()\n",
    "for i in range(1):\n",
    "    df = df.reindex(np.random.permutation(df.index)).reset_index().drop(columns=\"index\")\n",
    "    gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-6_5min</th>\n",
       "      <th>BTC_high-7_5min</th>\n",
       "      <th>BTC_low-7_5min</th>\n",
       "      <th>BTC_close-7_5min</th>\n",
       "      <th>BTC_volume-7_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.336000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.20</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>-0.000749</td>\n",
       "      <td>149.50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>484.14547</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.007551</td>\n",
       "      <td>367.34044</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>-597</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.942000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1859.10</td>\n",
       "      <td>-0.006458</td>\n",
       "      <td>-0.001360</td>\n",
       "      <td>-0.002379</td>\n",
       "      <td>513.60</td>\n",
       "      <td>-0.009517</td>\n",
       "      <td>...</td>\n",
       "      <td>2327.68933</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>0.014053</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>1794.86067</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>-420</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.042600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>169.47344</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>209.14587</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>48</td>\n",
       "      <td>-786</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.086850</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>...</td>\n",
       "      <td>44.43141</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.004531</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>55.46427</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>-314</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.050738</td>\n",
       "      <td>-0.000049</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>1042202.70</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.008426</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>238566.20</td>\n",
       "      <td>0.007046</td>\n",
       "      <td>...</td>\n",
       "      <td>87.14242</td>\n",
       "      <td>0.001242</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>66.51237</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269995</th>\n",
       "      <td>4.232500</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>573.79</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>590.91</td>\n",
       "      <td>0.002953</td>\n",
       "      <td>...</td>\n",
       "      <td>190.90840</td>\n",
       "      <td>-0.003800</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>-0.002317</td>\n",
       "      <td>127.50173</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>-355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269996</th>\n",
       "      <td>0.051873</td>\n",
       "      <td>-0.003807</td>\n",
       "      <td>0.000241</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>953549.20</td>\n",
       "      <td>-0.004964</td>\n",
       "      <td>-0.002072</td>\n",
       "      <td>-0.003036</td>\n",
       "      <td>896115.20</td>\n",
       "      <td>-0.003615</td>\n",
       "      <td>...</td>\n",
       "      <td>141.73621</td>\n",
       "      <td>-0.012202</td>\n",
       "      <td>-0.008474</td>\n",
       "      <td>-0.008902</td>\n",
       "      <td>370.41948</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269997</th>\n",
       "      <td>0.019300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>918.20</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>5223.80</td>\n",
       "      <td>-0.005181</td>\n",
       "      <td>...</td>\n",
       "      <td>3812.98248</td>\n",
       "      <td>-0.015690</td>\n",
       "      <td>-0.008904</td>\n",
       "      <td>-0.009005</td>\n",
       "      <td>2781.25451</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>37</td>\n",
       "      <td>-786</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269998</th>\n",
       "      <td>6.510000</td>\n",
       "      <td>-0.003072</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>1520.04</td>\n",
       "      <td>-0.001536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1458.12</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>245.17035</td>\n",
       "      <td>0.003131</td>\n",
       "      <td>0.005387</td>\n",
       "      <td>0.003919</td>\n",
       "      <td>70.51997</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3269999</th>\n",
       "      <td>0.131772</td>\n",
       "      <td>-0.003017</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000171</td>\n",
       "      <td>1238057.10</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>-0.001043</td>\n",
       "      <td>-0.001802</td>\n",
       "      <td>578009.70</td>\n",
       "      <td>-0.001878</td>\n",
       "      <td>...</td>\n",
       "      <td>64.54673</td>\n",
       "      <td>-0.003049</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>-0.002613</td>\n",
       "      <td>113.80173</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3270000 rows Ã— 287 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price    high-1     low-1   close-1    volume-1    high-2  \\\n",
       "0        1.336000  0.000000  0.000000  0.000000       83.20 -0.000749   \n",
       "1        2.942000  0.000000  0.000680  0.000000     1859.10 -0.006458   \n",
       "2        0.042600  0.000000  0.000000  0.000000        0.00  0.000000   \n",
       "3        0.086850  0.000576  0.000576  0.000576        0.00  0.000576   \n",
       "4        0.050738 -0.000049  0.005666  0.000739  1042202.70  0.004090   \n",
       "...           ...       ...       ...       ...         ...       ...   \n",
       "3269995  4.232500  0.002953  0.005316  0.005316      573.79  0.002953   \n",
       "3269996  0.051873 -0.003807  0.000241  0.000048   953549.20 -0.004964   \n",
       "3269997  0.019300  0.000000  0.000000  0.000000      918.20 -0.005181   \n",
       "3269998  6.510000 -0.003072  0.001536  0.001536     1520.04 -0.001536   \n",
       "3269999  0.131772 -0.003017  0.001006  0.000171  1238057.10 -0.001878   \n",
       "\n",
       "            low-2   close-2   volume-2    high-3  ...  BTC_volume-6_5min  \\\n",
       "0       -0.000749 -0.000749     149.50  0.000000  ...          484.14547   \n",
       "1       -0.001360 -0.002379     513.60 -0.009517  ...         2327.68933   \n",
       "2        0.000000  0.000000       0.00  0.000000  ...          169.47344   \n",
       "3        0.000576  0.000576       0.00  0.000576  ...           44.43141   \n",
       "4        0.008426  0.005272  238566.20  0.007046  ...           87.14242   \n",
       "...           ...       ...        ...       ...  ...                ...   \n",
       "3269995  0.005316  0.005316     590.91  0.002953  ...          190.90840   \n",
       "3269996 -0.002072 -0.003036  896115.20 -0.003615  ...          141.73621   \n",
       "3269997 -0.005181 -0.005181    5223.80 -0.005181  ...         3812.98248   \n",
       "3269998  0.000000  0.000000    1458.12  0.000000  ...          245.17035   \n",
       "3269999 -0.001043 -0.001802  578009.70 -0.001878  ...           64.54673   \n",
       "\n",
       "         BTC_high-7_5min  BTC_low-7_5min  BTC_close-7_5min  BTC_volume-7_5min  \\\n",
       "0               0.005987        0.008202          0.007551          367.34044   \n",
       "1               0.000221        0.014053          0.010287         1794.86067   \n",
       "2              -0.000922        0.000811          0.000335          209.14587   \n",
       "3               0.002454        0.004531          0.002615           55.46427   \n",
       "4               0.001242        0.003422          0.001313           66.51237   \n",
       "...                  ...             ...               ...                ...   \n",
       "3269995        -0.003800       -0.002317         -0.002317          127.50173   \n",
       "3269996        -0.012202       -0.008474         -0.008902          370.41948   \n",
       "3269997        -0.015690       -0.008904         -0.009005         2781.25451   \n",
       "3269998         0.003131        0.005387          0.003919           70.51997   \n",
       "3269999        -0.003049       -0.002040         -0.002613          113.80173   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          3     8      22       -597    1  \n",
       "1          2     2      37       -420    1  \n",
       "2          5     3      48       -786    0  \n",
       "3          6    20      45       -314    0  \n",
       "4          5    22      23        316    0  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "3269995    5    23      58       -355    1  \n",
       "3269996    1     2      21        316    1  \n",
       "3269997    3    13      37       -786    1  \n",
       "3269998    2     0      31        348    1  \n",
       "3269999    4     4      34        316    1  \n",
       "\n",
       "[3270000 rows x 287 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_feather(f\"../Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df choosen data shape(3270000, 287)\n",
      "pair: True\n",
      "654000\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"df choosen data shape\"+str(df.shape))\n",
    "print(f\"pair: {(df.shape[0]/2)==df.buy.sum()}\")\n",
    "dt=df.to_numpy(dtype=np.float32)\n",
    "#dt=df.to_numpy()\n",
    "dt=np.nan_to_num(dt,nan=0)\n",
    "#dt=dt.astype(np.float32)\n",
    "dt=np.nan_to_num(dt, neginf=0) \n",
    "dt=np.nan_to_num(dt, posinf=0) \n",
    "\n",
    "index_20pct= int(0.2*len(dt[:,0]))\n",
    "print(index_20pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feather loading\n",
    "# df=pd.read_feather(f\"../Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n",
    "# dt=df.to_numpy(dtype=np.float32)\n",
    "# dt=fixdt(dt)\n",
    "# index_20pct= int(0.2*len(dt[:,0]))\n",
    "# gc.collect()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Normalized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 11:22:06.494168: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-17 11:22:06.496212: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-17 11:22:06.496519: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: abj-K93SV\n",
      "2023-03-17 11:22:06.496541: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: abj-K93SV\n",
      "2023-03-17 11:22:06.497077: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-03-17 11:22:06.497921: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.154.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 286)              1144      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 250)               71750     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 250)              1000      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                5020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                1050      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,015\n",
      "Trainable params: 78,943\n",
      "Non-trainable params: 1,072\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_v5.h5\n",
      "Epoch 1/6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 11:22:14.526895: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2992704000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 53s 51ms/step - loss: 0.3977 - accuracy: 0.8165 - val_loss: 0.3487 - val_accuracy: 0.8419\n",
      "Epoch 2/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.3261 - accuracy: 0.8550 - val_loss: 0.3255 - val_accuracy: 0.8552\n",
      "Epoch 3/6000\n",
      "1022/1022 [==============================] - 44s 43ms/step - loss: 0.3064 - accuracy: 0.8652 - val_loss: 0.3189 - val_accuracy: 0.8565\n",
      "Epoch 4/6000\n",
      "1022/1022 [==============================] - 44s 43ms/step - loss: 0.2932 - accuracy: 0.8722 - val_loss: 0.3089 - val_accuracy: 0.8679\n",
      "Epoch 5/6000\n",
      "1022/1022 [==============================] - 45s 44ms/step - loss: 0.2825 - accuracy: 0.8775 - val_loss: 0.3003 - val_accuracy: 0.8680\n",
      "Epoch 6/6000\n",
      "1022/1022 [==============================] - 45s 44ms/step - loss: 0.2742 - accuracy: 0.8819 - val_loss: 0.2891 - val_accuracy: 0.8737\n",
      "Epoch 7/6000\n",
      "1022/1022 [==============================] - 52s 51ms/step - loss: 0.2672 - accuracy: 0.8857 - val_loss: 0.2839 - val_accuracy: 0.8780\n",
      "Epoch 8/6000\n",
      "1022/1022 [==============================] - 61s 59ms/step - loss: 0.2618 - accuracy: 0.8885 - val_loss: 0.2873 - val_accuracy: 0.8730\n",
      "Epoch 9/6000\n",
      "1022/1022 [==============================] - 53s 52ms/step - loss: 0.2573 - accuracy: 0.8910 - val_loss: 0.2723 - val_accuracy: 0.8827\n",
      "Epoch 10/6000\n",
      "1022/1022 [==============================] - 54s 52ms/step - loss: 0.2536 - accuracy: 0.8930 - val_loss: 0.2849 - val_accuracy: 0.8785\n",
      "Epoch 11/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2506 - accuracy: 0.8945 - val_loss: 0.2785 - val_accuracy: 0.8804\n",
      "Epoch 12/6000\n",
      "1022/1022 [==============================] - 55s 54ms/step - loss: 0.2469 - accuracy: 0.8964 - val_loss: 0.2800 - val_accuracy: 0.8789\n",
      "Epoch 13/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2437 - accuracy: 0.8983 - val_loss: 0.2796 - val_accuracy: 0.8776\n",
      "Epoch 14/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2408 - accuracy: 0.8997 - val_loss: 0.2695 - val_accuracy: 0.8845\n",
      "Epoch 15/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2389 - accuracy: 0.9008 - val_loss: 0.2745 - val_accuracy: 0.8813\n",
      "Epoch 16/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2361 - accuracy: 0.9022 - val_loss: 0.2689 - val_accuracy: 0.8845\n",
      "Epoch 17/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2343 - accuracy: 0.9032 - val_loss: 0.2606 - val_accuracy: 0.8895\n",
      "Epoch 18/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2317 - accuracy: 0.9043 - val_loss: 0.2697 - val_accuracy: 0.8842\n",
      "Epoch 19/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2303 - accuracy: 0.9053 - val_loss: 0.2617 - val_accuracy: 0.8887\n",
      "Epoch 20/6000\n",
      "1022/1022 [==============================] - 54s 53ms/step - loss: 0.2281 - accuracy: 0.9064 - val_loss: 0.2629 - val_accuracy: 0.8910\n",
      "Epoch 21/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2264 - accuracy: 0.9073 - val_loss: 0.2714 - val_accuracy: 0.8833\n",
      "Epoch 22/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2245 - accuracy: 0.9084 - val_loss: 0.2653 - val_accuracy: 0.8886\n",
      "Epoch 23/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2230 - accuracy: 0.9091 - val_loss: 0.2779 - val_accuracy: 0.8787\n",
      "Epoch 24/6000\n",
      "1022/1022 [==============================] - 55s 54ms/step - loss: 0.2216 - accuracy: 0.9099 - val_loss: 0.2574 - val_accuracy: 0.8915\n",
      "Epoch 25/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2205 - accuracy: 0.9105 - val_loss: 0.2561 - val_accuracy: 0.8945\n",
      "Epoch 26/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2189 - accuracy: 0.9114 - val_loss: 0.2609 - val_accuracy: 0.8921\n",
      "Epoch 27/6000\n",
      "1022/1022 [==============================] - 56s 54ms/step - loss: 0.2182 - accuracy: 0.9115 - val_loss: 0.2725 - val_accuracy: 0.8820\n",
      "Epoch 28/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2163 - accuracy: 0.9126 - val_loss: 0.2559 - val_accuracy: 0.8923\n",
      "Epoch 29/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2156 - accuracy: 0.9130 - val_loss: 0.2662 - val_accuracy: 0.8869\n",
      "Epoch 30/6000\n",
      "1022/1022 [==============================] - 55s 54ms/step - loss: 0.2143 - accuracy: 0.9137 - val_loss: 0.2581 - val_accuracy: 0.8939\n",
      "Epoch 31/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2131 - accuracy: 0.9144 - val_loss: 0.2554 - val_accuracy: 0.8936\n",
      "Epoch 32/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2126 - accuracy: 0.9146 - val_loss: 0.2538 - val_accuracy: 0.8947\n",
      "Epoch 33/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2113 - accuracy: 0.9153 - val_loss: 0.2550 - val_accuracy: 0.8935\n",
      "Epoch 34/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2103 - accuracy: 0.9158 - val_loss: 0.2536 - val_accuracy: 0.8943\n",
      "Epoch 35/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2094 - accuracy: 0.9162 - val_loss: 0.2543 - val_accuracy: 0.8945\n",
      "Epoch 36/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2086 - accuracy: 0.9167 - val_loss: 0.2681 - val_accuracy: 0.8862\n",
      "Epoch 37/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.2083 - accuracy: 0.9168 - val_loss: 0.2629 - val_accuracy: 0.8896\n",
      "Epoch 38/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2073 - accuracy: 0.9172 - val_loss: 0.2478 - val_accuracy: 0.8980\n",
      "Epoch 39/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2064 - accuracy: 0.9179 - val_loss: 0.2447 - val_accuracy: 0.8990\n",
      "Epoch 40/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2059 - accuracy: 0.9181 - val_loss: 0.2644 - val_accuracy: 0.8887\n",
      "Epoch 41/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2052 - accuracy: 0.9184 - val_loss: 0.2605 - val_accuracy: 0.8919\n",
      "Epoch 42/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2044 - accuracy: 0.9190 - val_loss: 0.2491 - val_accuracy: 0.9002\n",
      "Epoch 43/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.2034 - accuracy: 0.9194 - val_loss: 0.2504 - val_accuracy: 0.8967\n",
      "Epoch 44/6000\n",
      "1022/1022 [==============================] - 58s 57ms/step - loss: 0.2030 - accuracy: 0.9196 - val_loss: 0.2438 - val_accuracy: 0.9000\n",
      "Epoch 45/6000\n",
      "1022/1022 [==============================] - 59s 57ms/step - loss: 0.2026 - accuracy: 0.9197 - val_loss: 0.2696 - val_accuracy: 0.8865\n",
      "Epoch 46/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2017 - accuracy: 0.9203 - val_loss: 0.2463 - val_accuracy: 0.8992\n",
      "Epoch 47/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2014 - accuracy: 0.9204 - val_loss: 0.2533 - val_accuracy: 0.8950\n",
      "Epoch 48/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2009 - accuracy: 0.9206 - val_loss: 0.2547 - val_accuracy: 0.8948\n",
      "Epoch 49/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.2005 - accuracy: 0.9210 - val_loss: 0.2612 - val_accuracy: 0.8911\n",
      "Epoch 50/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1992 - accuracy: 0.9215 - val_loss: 0.2499 - val_accuracy: 0.8979\n",
      "Epoch 51/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1992 - accuracy: 0.9216 - val_loss: 0.2609 - val_accuracy: 0.8921\n",
      "Epoch 52/6000\n",
      "1022/1022 [==============================] - 56s 54ms/step - loss: 0.1983 - accuracy: 0.9221 - val_loss: 0.2546 - val_accuracy: 0.8946\n",
      "Epoch 53/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1981 - accuracy: 0.9222 - val_loss: 0.2439 - val_accuracy: 0.9010\n",
      "Epoch 54/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1977 - accuracy: 0.9225 - val_loss: 0.2448 - val_accuracy: 0.9010\n",
      "Epoch 55/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1968 - accuracy: 0.9229 - val_loss: 0.2570 - val_accuracy: 0.8940\n",
      "Epoch 56/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1967 - accuracy: 0.9228 - val_loss: 0.2628 - val_accuracy: 0.8911\n",
      "Epoch 57/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1958 - accuracy: 0.9233 - val_loss: 0.2734 - val_accuracy: 0.8863\n",
      "Epoch 58/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1954 - accuracy: 0.9235 - val_loss: 0.2455 - val_accuracy: 0.9002\n",
      "Epoch 59/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.1950 - accuracy: 0.9237 - val_loss: 0.2458 - val_accuracy: 0.9005\n",
      "Epoch 60/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.1948 - accuracy: 0.9238 - val_loss: 0.2559 - val_accuracy: 0.8941\n",
      "Epoch 61/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1948 - accuracy: 0.9240 - val_loss: 0.2464 - val_accuracy: 0.9002\n",
      "Epoch 62/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1939 - accuracy: 0.9244 - val_loss: 0.2367 - val_accuracy: 0.9051\n",
      "Epoch 63/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1937 - accuracy: 0.9246 - val_loss: 0.2551 - val_accuracy: 0.8951\n",
      "Epoch 64/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1933 - accuracy: 0.9246 - val_loss: 0.2410 - val_accuracy: 0.9020\n",
      "Epoch 65/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1922 - accuracy: 0.9252 - val_loss: 0.2444 - val_accuracy: 0.9015\n",
      "Epoch 66/6000\n",
      "1022/1022 [==============================] - 56s 54ms/step - loss: 0.1926 - accuracy: 0.9250 - val_loss: 0.2428 - val_accuracy: 0.9016\n",
      "Epoch 67/6000\n",
      "1022/1022 [==============================] - 58s 57ms/step - loss: 0.1920 - accuracy: 0.9255 - val_loss: 0.2432 - val_accuracy: 0.9020\n",
      "Epoch 68/6000\n",
      "1022/1022 [==============================] - 56s 54ms/step - loss: 0.1912 - accuracy: 0.9257 - val_loss: 0.2443 - val_accuracy: 0.9014\n",
      "Epoch 69/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1914 - accuracy: 0.9257 - val_loss: 0.2550 - val_accuracy: 0.8970\n",
      "Epoch 70/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1906 - accuracy: 0.9260 - val_loss: 0.2539 - val_accuracy: 0.8960\n",
      "Epoch 71/6000\n",
      "1022/1022 [==============================] - 58s 57ms/step - loss: 0.1902 - accuracy: 0.9261 - val_loss: 0.2525 - val_accuracy: 0.8974\n",
      "Epoch 72/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1905 - accuracy: 0.9262 - val_loss: 0.2738 - val_accuracy: 0.8863\n",
      "Epoch 73/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1898 - accuracy: 0.9265 - val_loss: 0.2449 - val_accuracy: 0.9015\n",
      "Epoch 74/6000\n",
      "1022/1022 [==============================] - 57s 55ms/step - loss: 0.1892 - accuracy: 0.9269 - val_loss: 0.2676 - val_accuracy: 0.8898\n",
      "Epoch 75/6000\n",
      "1022/1022 [==============================] - 56s 55ms/step - loss: 0.1890 - accuracy: 0.9269 - val_loss: 0.2530 - val_accuracy: 0.8970\n",
      "Epoch 76/6000\n",
      "1022/1022 [==============================] - 55s 54ms/step - loss: 0.1883 - accuracy: 0.9272 - val_loss: 0.2404 - val_accuracy: 0.9036\n",
      "Epoch 77/6000\n",
      "1022/1022 [==============================] - 57s 56ms/step - loss: 0.1882 - accuracy: 0.9274 - val_loss: 0.2429 - val_accuracy: 0.9021\n",
      "Epoch 77: early stopping\n",
      "##########################################################################\n",
      "------val_accuracy-----> 90.51 | 92.74 <----------accuracy----------\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Norm_v5.json\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_v5.h5\n",
      "save to: /UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_vInit.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Normalzed Model\n",
    "IN_DIM=dt.shape[1]-1\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(250),activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20),activation='relu')) \n",
    "model.add(Dense(int(50),activation='relu')) \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "callbacks_a = ModelCheckpoint(filepath =Model_FileName,monitor ='val_accuracy',save_best_only = True, save_weights = True)\n",
    "callbacks_b = EarlyStopping(monitor ='val_accuracy',mode='auto',patience=15,verbose=1)\n",
    "print(\"saving file in: \"+Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, 0:-1],\n",
    "                dt[index_20pct:,-1],\n",
    "                validation_data=(dt[:index_20pct, :-1],dt[:index_20pct,-1]),\n",
    "                epochs=6000,\n",
    "                batch_size=256*10,\n",
    "                callbacks=[callbacks_a,callbacks_b])\n",
    "\n",
    "print('##########################################################################')\n",
    "print(f\"------val_accuracy-----> {'{0:.4g}'.format(max(history.history['val_accuracy'])*100)} | {'{0:.4g}'.format(max(history.history['accuracy'])*100)} <----------accuracy----------\")\n",
    "print(Normalization_File)\n",
    "print(Model_FileName)\n",
    "model_init_file=Model_FileName.replace(f\"_v{VERSION}\", \"_vInit\")\n",
    "print(f\"save to: {model_init_file}\")\n",
    "model.save(model_init_file)\n",
    "model_init=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_init_file=Model_FileName.replace(f\"_v{VERSION}\", \"_vInit\")\n",
    "# print(f\"save to: {model_init_file}\")\n",
    "# model.save(model_init_file)\n",
    "# model_init=model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Model Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_2 (Batc  (None, 286)              1144      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 300)               86100     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 20)               80        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 194,765\n",
      "Trainable params: 192,353\n",
      "Non-trainable params: 2,412\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 12:34:09.260052: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2992704000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022/1022 [==============================] - 165s 157ms/step - loss: 0.4113 - accuracy: 0.8076 - val_loss: 0.3843 - val_accuracy: 0.8263\n",
      "Epoch 2/500\n",
      "1022/1022 [==============================] - 157s 153ms/step - loss: 0.3356 - accuracy: 0.8482 - val_loss: 0.3285 - val_accuracy: 0.8474\n",
      "Epoch 3/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.3150 - accuracy: 0.8588 - val_loss: 0.3689 - val_accuracy: 0.8428\n",
      "Epoch 4/500\n",
      "1022/1022 [==============================] - 156s 153ms/step - loss: 0.3044 - accuracy: 0.8644 - val_loss: 0.2987 - val_accuracy: 0.8653\n",
      "Epoch 5/500\n",
      "1022/1022 [==============================] - 158s 154ms/step - loss: 0.2981 - accuracy: 0.8678 - val_loss: 0.2911 - val_accuracy: 0.8708\n",
      "Epoch 6/500\n",
      "1022/1022 [==============================] - 157s 153ms/step - loss: 0.2913 - accuracy: 0.8716 - val_loss: 0.2862 - val_accuracy: 0.8738\n",
      "Epoch 7/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2866 - accuracy: 0.8741 - val_loss: 0.2890 - val_accuracy: 0.8714\n",
      "Epoch 8/500\n",
      "1022/1022 [==============================] - 158s 155ms/step - loss: 0.2837 - accuracy: 0.8757 - val_loss: 0.2896 - val_accuracy: 0.8694\n",
      "Epoch 9/500\n",
      "1022/1022 [==============================] - 157s 153ms/step - loss: 0.2807 - accuracy: 0.8771 - val_loss: 0.2905 - val_accuracy: 0.8683\n",
      "Epoch 10/500\n",
      "1022/1022 [==============================] - 156s 153ms/step - loss: 0.2783 - accuracy: 0.8785 - val_loss: 0.2752 - val_accuracy: 0.8795\n",
      "Epoch 11/500\n",
      "1022/1022 [==============================] - 159s 156ms/step - loss: 0.2756 - accuracy: 0.8799 - val_loss: 0.2820 - val_accuracy: 0.8762\n",
      "Epoch 12/500\n",
      "1022/1022 [==============================] - 159s 155ms/step - loss: 0.2735 - accuracy: 0.8810 - val_loss: 0.2777 - val_accuracy: 0.8773\n",
      "Epoch 13/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2714 - accuracy: 0.8822 - val_loss: 0.2821 - val_accuracy: 0.8766\n",
      "Epoch 14/500\n",
      "1022/1022 [==============================] - 159s 155ms/step - loss: 0.2695 - accuracy: 0.8830 - val_loss: 0.2750 - val_accuracy: 0.8795\n",
      "Epoch 15/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2678 - accuracy: 0.8837 - val_loss: 0.2857 - val_accuracy: 0.8710\n",
      "Epoch 16/500\n",
      "1022/1022 [==============================] - 158s 155ms/step - loss: 0.2659 - accuracy: 0.8849 - val_loss: 0.2638 - val_accuracy: 0.8858\n",
      "Epoch 17/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2638 - accuracy: 0.8861 - val_loss: 0.2601 - val_accuracy: 0.8874\n",
      "Epoch 18/500\n",
      "1022/1022 [==============================] - 156s 153ms/step - loss: 0.2623 - accuracy: 0.8867 - val_loss: 0.2735 - val_accuracy: 0.8790\n",
      "Epoch 19/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2608 - accuracy: 0.8875 - val_loss: 0.2617 - val_accuracy: 0.8862\n",
      "Epoch 20/500\n",
      "1022/1022 [==============================] - 157s 154ms/step - loss: 0.2592 - accuracy: 0.8884 - val_loss: 0.2705 - val_accuracy: 0.8825\n",
      "Epoch 21/500\n",
      "1022/1022 [==============================] - 159s 156ms/step - loss: 0.2579 - accuracy: 0.8890 - val_loss: 0.2628 - val_accuracy: 0.8864\n",
      "Epoch 22/500\n",
      "1022/1022 [==============================] - 158s 154ms/step - loss: 0.2561 - accuracy: 0.8902 - val_loss: 0.2693 - val_accuracy: 0.8800\n",
      "Epoch 23/500\n",
      "1022/1022 [==============================] - 157s 153ms/step - loss: 0.2547 - accuracy: 0.8906 - val_loss: 0.2594 - val_accuracy: 0.8863\n",
      "Epoch 23: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_VeryDeep.h5\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import BinaryQuantization\n",
    "\n",
    "# # Define the class weights\n",
    "# class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# SizeTunner = 1\n",
    "# IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(BinaryQuantization(input_shape=(IN_DIM,)))\n",
    "\n",
    "# model.add(Dense(int(300 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(200 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(20 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "# ]\n",
    "\n",
    "# print(\"saving file in: \" + Model_FileName)\n",
    "# history = model.fit(dt[index_20pct:, :-1],\n",
    "#                     dt[index_20pct:, -1],\n",
    "#                     validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "#                     epochs=500,\n",
    "#                     batch_size=256*10,\n",
    "#                     callbacks=callbacks,\n",
    "#                     class_weight=class_weights)\n",
    "# # Save the model\n",
    "# binary_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_BINARY.h5\"\n",
    "# model.save(binary_model_file)\n",
    "# binary_model_file=tf.keras.models.load_model(binary_model_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_range_start=0\n",
    "# mini_range_stop=200000\n",
    "# model.evaluate(dt[mini_range_start:mini_range_stop,:-1],dt[mini_range_start:mini_range_stop,-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-  Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-17 13:34:52.645220: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3740880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102188/102188 [==============================] - 415s 4ms/step\n",
      "Precent Mean: 57.639%\n",
      "Precent Mean: 0.476%\n",
      "ModelAccuracy: 88.813%\n",
      "True Win Predictions Mean of all: 48.226%\n",
      "XXX Loss Buy Mean of all: 9.413%\n",
      "Missed good deal off all: 1.774%\n",
      "Good Zero prediction Mean: 40.587%\n",
      "good fiability\n",
      "========= Win Ratio:83.66904352955464 ====================\n"
     ]
    }
   ],
   "source": [
    "USED_MODEL=very_deep_model\n",
    "#model_init=model\n",
    "#USED_MODEL=model_init#load_model(\"/UltimeTradingBot/Data/BUY_UP_CLOSE/tp60_w6_max3min_Model_GoodVeryDeep.h5\")\n",
    "Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "prediction2=Prediction_Note.round()\n",
    "hp(prediction2[:,0].mean())\n",
    "PesemisticPrediction=(Prediction_Note[:,0]-0.49).round()\n",
    "hp(PesemisticPrediction.mean())\n",
    "Y=dt[:,-1].copy()\n",
    "Pred01=prediction2[:,-1]\n",
    "Original_Traget_Data=Y\n",
    "Predicted_Data=Pred01\n",
    "\n",
    "TruePred=((Original_Traget_Data==Predicted_Data)).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTI Retrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_dt=dt[TruePred]\n",
    "# good_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad_dt=dt[ np.logical_not(TruePred)]\n",
    "bad_dt=dt[Predicted_Data==1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Size is : 615610\n",
      "Precent Mean: 50.000%\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_11 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp92_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "481/481 [==============================] - 15s 25ms/step - loss: 0.3117 - accuracy: 0.6623 - val_loss: 0.6256 - val_accuracy: 0.6667\n",
      "Epoch 2/500\n",
      "481/481 [==============================] - 14s 29ms/step - loss: 0.3059 - accuracy: 0.6734 - val_loss: 0.5772 - val_accuracy: 0.6842\n",
      "Epoch 3/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.3024 - accuracy: 0.6793 - val_loss: 0.5853 - val_accuracy: 0.6667\n",
      "Epoch 4/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2991 - accuracy: 0.6849 - val_loss: 0.5679 - val_accuracy: 0.6491\n",
      "Epoch 5/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2954 - accuracy: 0.6911 - val_loss: 0.5330 - val_accuracy: 0.7193\n",
      "Epoch 6/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2917 - accuracy: 0.6979 - val_loss: 0.5370 - val_accuracy: 0.7368\n",
      "Epoch 7/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2882 - accuracy: 0.7036 - val_loss: 0.5412 - val_accuracy: 0.7018\n",
      "Epoch 8/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2854 - accuracy: 0.7069 - val_loss: 0.5446 - val_accuracy: 0.6667\n",
      "Epoch 9/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2827 - accuracy: 0.7120 - val_loss: 0.5119 - val_accuracy: 0.7193\n",
      "Epoch 10/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2804 - accuracy: 0.7150 - val_loss: 0.5218 - val_accuracy: 0.7018\n",
      "Epoch 11/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2783 - accuracy: 0.7188 - val_loss: 0.5192 - val_accuracy: 0.6842\n",
      "Epoch 12/500\n",
      "481/481 [==============================] - 15s 32ms/step - loss: 0.2765 - accuracy: 0.7213 - val_loss: 0.5392 - val_accuracy: 0.7544\n",
      "Epoch 13/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2747 - accuracy: 0.7239 - val_loss: 0.5041 - val_accuracy: 0.7193\n",
      "Epoch 14/500\n",
      "481/481 [==============================] - 16s 32ms/step - loss: 0.2732 - accuracy: 0.7260 - val_loss: 0.5452 - val_accuracy: 0.7193\n",
      "Epoch 15/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2719 - accuracy: 0.7279 - val_loss: 0.5388 - val_accuracy: 0.6842\n",
      "Epoch 16/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2704 - accuracy: 0.7302 - val_loss: 0.5347 - val_accuracy: 0.7193\n",
      "Epoch 17/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2691 - accuracy: 0.7316 - val_loss: 0.5060 - val_accuracy: 0.7368\n",
      "Epoch 18/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2682 - accuracy: 0.7328 - val_loss: 0.5324 - val_accuracy: 0.6667\n",
      "Epoch 19/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2670 - accuracy: 0.7346 - val_loss: 0.5617 - val_accuracy: 0.7368\n",
      "Epoch 20/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2659 - accuracy: 0.7369 - val_loss: 0.5666 - val_accuracy: 0.7193\n",
      "Epoch 21/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2651 - accuracy: 0.7374 - val_loss: 0.5565 - val_accuracy: 0.6842\n",
      "Epoch 22/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2641 - accuracy: 0.7390 - val_loss: 0.5113 - val_accuracy: 0.7018\n",
      "Epoch 23/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2631 - accuracy: 0.7402 - val_loss: 0.5441 - val_accuracy: 0.7018\n",
      "Epoch 24/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2625 - accuracy: 0.7412 - val_loss: 0.5379 - val_accuracy: 0.6842\n",
      "Epoch 25/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2618 - accuracy: 0.7423 - val_loss: 0.5778 - val_accuracy: 0.6491\n",
      "Epoch 26/500\n",
      "481/481 [==============================] - 16s 33ms/step - loss: 0.2608 - accuracy: 0.7439 - val_loss: 0.5312 - val_accuracy: 0.6842\n",
      "Epoch 27/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2598 - accuracy: 0.7446 - val_loss: 0.5520 - val_accuracy: 0.7018\n",
      "Epoch 28/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2591 - accuracy: 0.7457 - val_loss: 0.5414 - val_accuracy: 0.7018\n",
      "Epoch 29/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2586 - accuracy: 0.7468 - val_loss: 0.5195 - val_accuracy: 0.7193\n",
      "Epoch 30/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2580 - accuracy: 0.7478 - val_loss: 0.5500 - val_accuracy: 0.6667\n",
      "Epoch 31/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2575 - accuracy: 0.7477 - val_loss: 0.5379 - val_accuracy: 0.6842\n",
      "Epoch 32/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2567 - accuracy: 0.7495 - val_loss: 0.5266 - val_accuracy: 0.6842\n",
      "Epoch 33/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2564 - accuracy: 0.7496 - val_loss: 0.4970 - val_accuracy: 0.7544\n",
      "Epoch 34/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2557 - accuracy: 0.7504 - val_loss: 0.4919 - val_accuracy: 0.7368\n",
      "Epoch 35/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2554 - accuracy: 0.7516 - val_loss: 0.5425 - val_accuracy: 0.7018\n",
      "Epoch 36/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2549 - accuracy: 0.7518 - val_loss: 0.5490 - val_accuracy: 0.7018\n",
      "Epoch 37/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2545 - accuracy: 0.7525 - val_loss: 0.5017 - val_accuracy: 0.7368\n",
      "Epoch 38/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2539 - accuracy: 0.7530 - val_loss: 0.5062 - val_accuracy: 0.6842\n",
      "Epoch 39/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2534 - accuracy: 0.7535 - val_loss: 0.5083 - val_accuracy: 0.7368\n",
      "Epoch 40/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2528 - accuracy: 0.7544 - val_loss: 0.5030 - val_accuracy: 0.7193\n",
      "Epoch 41/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2526 - accuracy: 0.7551 - val_loss: 0.5211 - val_accuracy: 0.7018\n",
      "Epoch 42/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2521 - accuracy: 0.7557 - val_loss: 0.5373 - val_accuracy: 0.6842\n",
      "Epoch 43/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2512 - accuracy: 0.7564 - val_loss: 0.5270 - val_accuracy: 0.7193\n",
      "Epoch 44/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2514 - accuracy: 0.7564 - val_loss: 0.4948 - val_accuracy: 0.7368\n",
      "Epoch 45/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2506 - accuracy: 0.7579 - val_loss: 0.5328 - val_accuracy: 0.7193\n",
      "Epoch 46/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2502 - accuracy: 0.7578 - val_loss: 0.5212 - val_accuracy: 0.7193\n",
      "Epoch 47/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2501 - accuracy: 0.7582 - val_loss: 0.5347 - val_accuracy: 0.7018\n",
      "Epoch 48/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2500 - accuracy: 0.7584 - val_loss: 0.5532 - val_accuracy: 0.7018\n",
      "Epoch 49/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2493 - accuracy: 0.7591 - val_loss: 0.5036 - val_accuracy: 0.7544\n",
      "Epoch 50/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2490 - accuracy: 0.7596 - val_loss: 0.5545 - val_accuracy: 0.7193\n",
      "Epoch 51/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2483 - accuracy: 0.7598 - val_loss: 0.5316 - val_accuracy: 0.7193\n",
      "Epoch 52/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2481 - accuracy: 0.7607 - val_loss: 0.5189 - val_accuracy: 0.7368\n",
      "Epoch 53/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2483 - accuracy: 0.7607 - val_loss: 0.5346 - val_accuracy: 0.6842\n",
      "Epoch 54/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2477 - accuracy: 0.7609 - val_loss: 0.5141 - val_accuracy: 0.7018\n",
      "Epoch 55/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2477 - accuracy: 0.7610 - val_loss: 0.5604 - val_accuracy: 0.7544\n",
      "Epoch 56/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2471 - accuracy: 0.7623 - val_loss: 0.5377 - val_accuracy: 0.7193\n",
      "Epoch 57/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2468 - accuracy: 0.7625 - val_loss: 0.5385 - val_accuracy: 0.6842\n",
      "Epoch 58/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2466 - accuracy: 0.7628 - val_loss: 0.5470 - val_accuracy: 0.6667\n",
      "Epoch 59/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2461 - accuracy: 0.7634 - val_loss: 0.5455 - val_accuracy: 0.7018\n",
      "Epoch 60/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2459 - accuracy: 0.7639 - val_loss: 0.5467 - val_accuracy: 0.7018\n",
      "Epoch 61/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2456 - accuracy: 0.7635 - val_loss: 0.5239 - val_accuracy: 0.6842\n",
      "Epoch 62/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2452 - accuracy: 0.7645 - val_loss: 0.5499 - val_accuracy: 0.7018\n",
      "Epoch 63/500\n",
      "481/481 [==============================] - 16s 34ms/step - loss: 0.2453 - accuracy: 0.7643 - val_loss: 0.5671 - val_accuracy: 0.7018\n",
      "Epoch 64/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2448 - accuracy: 0.7648 - val_loss: 0.5160 - val_accuracy: 0.6842\n",
      "Epoch 65/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2444 - accuracy: 0.7656 - val_loss: 0.5107 - val_accuracy: 0.7018\n",
      "Epoch 66/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2446 - accuracy: 0.7651 - val_loss: 0.5657 - val_accuracy: 0.7368\n",
      "Epoch 67/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2444 - accuracy: 0.7657 - val_loss: 0.5255 - val_accuracy: 0.7193\n",
      "Epoch 68/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2441 - accuracy: 0.7661 - val_loss: 0.5123 - val_accuracy: 0.7544\n",
      "Epoch 69/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2435 - accuracy: 0.7666 - val_loss: 0.5427 - val_accuracy: 0.6667\n",
      "Epoch 70/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2434 - accuracy: 0.7675 - val_loss: 0.5269 - val_accuracy: 0.7193\n",
      "Epoch 71/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2432 - accuracy: 0.7672 - val_loss: 0.5519 - val_accuracy: 0.7193\n",
      "Epoch 72/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2431 - accuracy: 0.7674 - val_loss: 0.5147 - val_accuracy: 0.7193\n",
      "Epoch 73/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2429 - accuracy: 0.7675 - val_loss: 0.5231 - val_accuracy: 0.6842\n",
      "Epoch 74/500\n",
      "481/481 [==============================] - 20s 41ms/step - loss: 0.2426 - accuracy: 0.7682 - val_loss: 0.5536 - val_accuracy: 0.6667\n",
      "Epoch 75/500\n",
      "481/481 [==============================] - 23s 48ms/step - loss: 0.2424 - accuracy: 0.7684 - val_loss: 0.5491 - val_accuracy: 0.7368\n",
      "Epoch 76/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2424 - accuracy: 0.7682 - val_loss: 0.5621 - val_accuracy: 0.6842\n",
      "Epoch 77/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2422 - accuracy: 0.7684 - val_loss: 0.5451 - val_accuracy: 0.7193\n",
      "Epoch 78/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2414 - accuracy: 0.7693 - val_loss: 0.5533 - val_accuracy: 0.7544\n",
      "Epoch 79/500\n",
      "481/481 [==============================] - 17s 36ms/step - loss: 0.2416 - accuracy: 0.7691 - val_loss: 0.5376 - val_accuracy: 0.7368\n",
      "Epoch 80/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2417 - accuracy: 0.7688 - val_loss: 0.5021 - val_accuracy: 0.7719\n",
      "Epoch 81/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2410 - accuracy: 0.7703 - val_loss: 0.5554 - val_accuracy: 0.7193\n",
      "Epoch 82/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2410 - accuracy: 0.7698 - val_loss: 0.5170 - val_accuracy: 0.7719\n",
      "Epoch 83/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2410 - accuracy: 0.7704 - val_loss: 0.5526 - val_accuracy: 0.7544\n",
      "Epoch 84/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2403 - accuracy: 0.7708 - val_loss: 0.4990 - val_accuracy: 0.7544\n",
      "Epoch 85/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2405 - accuracy: 0.7705 - val_loss: 0.5326 - val_accuracy: 0.7193\n",
      "Epoch 86/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2400 - accuracy: 0.7716 - val_loss: 0.5905 - val_accuracy: 0.7368\n",
      "Epoch 87/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2401 - accuracy: 0.7712 - val_loss: 0.5405 - val_accuracy: 0.7368\n",
      "Epoch 88/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2398 - accuracy: 0.7718 - val_loss: 0.5645 - val_accuracy: 0.7544\n",
      "Epoch 89/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2400 - accuracy: 0.7712 - val_loss: 0.5379 - val_accuracy: 0.7193\n",
      "Epoch 90/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2395 - accuracy: 0.7722 - val_loss: 0.5111 - val_accuracy: 0.6842\n",
      "Epoch 91/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2394 - accuracy: 0.7724 - val_loss: 0.5493 - val_accuracy: 0.7368\n",
      "Epoch 92/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2391 - accuracy: 0.7728 - val_loss: 0.5565 - val_accuracy: 0.7193\n",
      "Epoch 93/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2390 - accuracy: 0.7726 - val_loss: 0.5308 - val_accuracy: 0.7544\n",
      "Epoch 94/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2388 - accuracy: 0.7730 - val_loss: 0.5114 - val_accuracy: 0.7719\n",
      "Epoch 95/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2387 - accuracy: 0.7735 - val_loss: 0.5240 - val_accuracy: 0.7193\n",
      "Epoch 96/500\n",
      "481/481 [==============================] - 17s 34ms/step - loss: 0.2387 - accuracy: 0.7733 - val_loss: 0.5275 - val_accuracy: 0.7719\n",
      "Epoch 97/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2384 - accuracy: 0.7730 - val_loss: 0.5236 - val_accuracy: 0.7719\n",
      "Epoch 98/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2381 - accuracy: 0.7738 - val_loss: 0.5092 - val_accuracy: 0.7544\n",
      "Epoch 99/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2380 - accuracy: 0.7741 - val_loss: 0.5408 - val_accuracy: 0.7544\n",
      "Epoch 100/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2379 - accuracy: 0.7741 - val_loss: 0.5697 - val_accuracy: 0.6842\n",
      "Epoch 101/500\n",
      "481/481 [==============================] - 17s 36ms/step - loss: 0.2379 - accuracy: 0.7741 - val_loss: 0.5266 - val_accuracy: 0.7368\n",
      "Epoch 102/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2380 - accuracy: 0.7742 - val_loss: 0.5133 - val_accuracy: 0.7368\n",
      "Epoch 103/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2374 - accuracy: 0.7745 - val_loss: 0.5443 - val_accuracy: 0.7018\n",
      "Epoch 104/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2374 - accuracy: 0.7748 - val_loss: 0.5283 - val_accuracy: 0.7368\n",
      "Epoch 105/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2372 - accuracy: 0.7747 - val_loss: 0.4968 - val_accuracy: 0.7719\n",
      "Epoch 106/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2369 - accuracy: 0.7755 - val_loss: 0.4889 - val_accuracy: 0.7719\n",
      "Epoch 107/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2366 - accuracy: 0.7752 - val_loss: 0.5509 - val_accuracy: 0.6842\n",
      "Epoch 108/500\n",
      "481/481 [==============================] - 19s 40ms/step - loss: 0.2367 - accuracy: 0.7753 - val_loss: 0.5470 - val_accuracy: 0.7018\n",
      "Epoch 109/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2368 - accuracy: 0.7757 - val_loss: 0.5180 - val_accuracy: 0.7544\n",
      "Epoch 110/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2366 - accuracy: 0.7756 - val_loss: 0.5337 - val_accuracy: 0.7018\n",
      "Epoch 111/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2363 - accuracy: 0.7760 - val_loss: 0.5011 - val_accuracy: 0.7895\n",
      "Epoch 112/500\n",
      "481/481 [==============================] - 17s 35ms/step - loss: 0.2362 - accuracy: 0.7760 - val_loss: 0.5120 - val_accuracy: 0.7193\n",
      "Epoch 113/500\n",
      "481/481 [==============================] - 21s 44ms/step - loss: 0.2362 - accuracy: 0.7759 - val_loss: 0.5534 - val_accuracy: 0.7018\n",
      "Epoch 114/500\n",
      "481/481 [==============================] - 21s 43ms/step - loss: 0.2363 - accuracy: 0.7757 - val_loss: 0.5302 - val_accuracy: 0.7544\n",
      "Epoch 115/500\n",
      "481/481 [==============================] - 19s 39ms/step - loss: 0.2360 - accuracy: 0.7765 - val_loss: 0.5114 - val_accuracy: 0.7719\n",
      "Epoch 116/500\n",
      "481/481 [==============================] - 17s 36ms/step - loss: 0.2355 - accuracy: 0.7765 - val_loss: 0.4657 - val_accuracy: 0.7544\n",
      "Epoch 117/500\n",
      "481/481 [==============================] - 17s 36ms/step - loss: 0.2359 - accuracy: 0.7768 - val_loss: 0.5134 - val_accuracy: 0.8070\n",
      "Epoch 118/500\n",
      "481/481 [==============================] - 17s 36ms/step - loss: 0.2357 - accuracy: 0.7767 - val_loss: 0.5445 - val_accuracy: 0.7193\n",
      "Epoch 119/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2355 - accuracy: 0.7768 - val_loss: 0.5431 - val_accuracy: 0.7368\n",
      "Epoch 120/500\n",
      "481/481 [==============================] - 19s 39ms/step - loss: 0.2354 - accuracy: 0.7771 - val_loss: 0.5067 - val_accuracy: 0.7544\n",
      "Epoch 121/500\n",
      "481/481 [==============================] - 18s 37ms/step - loss: 0.2352 - accuracy: 0.7775 - val_loss: 0.5273 - val_accuracy: 0.6842\n",
      "Epoch 122/500\n",
      "481/481 [==============================] - 29s 61ms/step - loss: 0.2350 - accuracy: 0.7774 - val_loss: 0.5185 - val_accuracy: 0.7719\n",
      "Epoch 123/500\n",
      "481/481 [==============================] - 27s 57ms/step - loss: 0.2352 - accuracy: 0.7773 - val_loss: 0.5232 - val_accuracy: 0.7544\n",
      "Epoch 124/500\n",
      "481/481 [==============================] - 19s 39ms/step - loss: 0.2347 - accuracy: 0.7780 - val_loss: 0.5405 - val_accuracy: 0.7544\n",
      "Epoch 125/500\n",
      " 83/481 [====>.........................] - ETA: 14s - loss: 0.2336 - accuracy: 0.7785"
     ]
    }
   ],
   "source": [
    "#Anti prediction\n",
    "\n",
    "BadONE=bad_dt[bad_dt[:,-1]==0]\n",
    "TrueOne=bad_dt[bad_dt[:,-1]==1][:BadONE.shape[0]]\n",
    "AntiPrediction_DT=np.concatenate((BadONE,TrueOne),axis=0)\n",
    "np.random.shuffle(AntiPrediction_DT)\n",
    "\n",
    "retrain_dt=AntiPrediction_DT\n",
    "print(f\"Dataset Size is : {retrain_dt.shape[0]}\")\n",
    "class_1_weight=hp(retrain_dt[:,-1].mean())/100\n",
    "\n",
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "index_20pct=int(retrain_dt.shape[1]*0.2)\n",
    "\n",
    "class_weights = {0: 1-class_1_weight, 1: class_1_weight}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='loss', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='loss', mode='auto', patience=20, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                    retrain_dt[index_20pct:, -1],\n",
    "                    validation_data=(retrain_dt[:index_20pct, :-1], retrain_dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*5,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "justgood_good_model=model\n",
    "justgood_good_model_wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Anti-Model_v2.h5'\n",
    "justgood_good_model.save(justgood_good_model_wheretosave)\n",
    "print(justgood_good_model_wheretosave)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True PredONly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_15 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:07:37.086144: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3740814792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 125s 48ms/step - loss: 0.1347 - accuracy: 0.8935 - val_loss: 0.2585 - val_accuracy: 0.9123\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 130s 51ms/step - loss: 0.1172 - accuracy: 0.9079 - val_loss: 0.2198 - val_accuracy: 0.9474\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 132s 51ms/step - loss: 0.1081 - accuracy: 0.9156 - val_loss: 0.1978 - val_accuracy: 0.8947\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 139s 54ms/step - loss: 0.1018 - accuracy: 0.9208 - val_loss: 0.1864 - val_accuracy: 0.9123\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 136s 53ms/step - loss: 0.0974 - accuracy: 0.9247 - val_loss: 0.1570 - val_accuracy: 0.9123\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 132s 52ms/step - loss: 0.0939 - accuracy: 0.9279 - val_loss: 0.1668 - val_accuracy: 0.9298\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 129s 51ms/step - loss: 0.0913 - accuracy: 0.9302 - val_loss: 0.1630 - val_accuracy: 0.9123\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 133s 52ms/step - loss: 0.0890 - accuracy: 0.9321 - val_loss: 0.1898 - val_accuracy: 0.9123\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 133s 52ms/step - loss: 0.0870 - accuracy: 0.9338 - val_loss: 0.1783 - val_accuracy: 0.9123\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 130s 51ms/step - loss: 0.0854 - accuracy: 0.9353 - val_loss: 0.1780 - val_accuracy: 0.9298\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 131s 51ms/step - loss: 0.0841 - accuracy: 0.9364 - val_loss: 0.1649 - val_accuracy: 0.9298\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 133s 52ms/step - loss: 0.0826 - accuracy: 0.9377 - val_loss: 0.1707 - val_accuracy: 0.9123\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 130s 51ms/step - loss: 0.0817 - accuracy: 0.9385 - val_loss: 0.1511 - val_accuracy: 0.9123\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 132s 52ms/step - loss: 0.0808 - accuracy: 0.9392 - val_loss: 0.1538 - val_accuracy: 0.9298\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 129s 50ms/step - loss: 0.0799 - accuracy: 0.9401 - val_loss: 0.1526 - val_accuracy: 0.9123\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 128s 50ms/step - loss: 0.0790 - accuracy: 0.9408 - val_loss: 0.1762 - val_accuracy: 0.9298\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 131s 51ms/step - loss: 0.0785 - accuracy: 0.9412 - val_loss: 0.1719 - val_accuracy: 0.9298\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 131s 51ms/step - loss: 0.0776 - accuracy: 0.9420 - val_loss: 0.1593 - val_accuracy: 0.9123\n",
      "Epoch 18: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 21:48:07.010288: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3740880000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102188/102188 [==============================] - 315s 3ms/step\n",
      "ModelAccuracy: 93.516%\n",
      "True Win Predictions Mean of all: 46.658%\n",
      "XXX Loss Buy Mean of all: 3.142%\n",
      "Missed good deal off all: 3.342%\n",
      "Good Zero prediction Mean: 46.858%\n",
      "good fiability\n",
      "========= Win Ratio:93.69076305220882 ====================\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_19 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 79s 30ms/step - loss: 0.1258 - accuracy: 0.9016 - val_loss: 0.1862 - val_accuracy: 0.9474\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 82s 32ms/step - loss: 0.1067 - accuracy: 0.9170 - val_loss: 0.1543 - val_accuracy: 0.9298\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 82s 32ms/step - loss: 0.0968 - accuracy: 0.9253 - val_loss: 0.1614 - val_accuracy: 0.9649\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 84s 33ms/step - loss: 0.0906 - accuracy: 0.9304 - val_loss: 0.1032 - val_accuracy: 0.9825\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0861 - accuracy: 0.9344 - val_loss: 0.1573 - val_accuracy: 0.9649\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0828 - accuracy: 0.9371 - val_loss: 0.1342 - val_accuracy: 0.9474\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0802 - accuracy: 0.9394 - val_loss: 0.1394 - val_accuracy: 0.9474\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0779 - accuracy: 0.9417 - val_loss: 0.1403 - val_accuracy: 0.9474\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0762 - accuracy: 0.9430 - val_loss: 0.1168 - val_accuracy: 0.9649\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0746 - accuracy: 0.9446 - val_loss: 0.1200 - val_accuracy: 0.9474\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0732 - accuracy: 0.9457 - val_loss: 0.1519 - val_accuracy: 0.9298\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0722 - accuracy: 0.9465 - val_loss: 0.1050 - val_accuracy: 0.9474\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0711 - accuracy: 0.9476 - val_loss: 0.1206 - val_accuracy: 0.9474\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0703 - accuracy: 0.9484 - val_loss: 0.1524 - val_accuracy: 0.9298\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0693 - accuracy: 0.9493 - val_loss: 0.1364 - val_accuracy: 0.9474\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0686 - accuracy: 0.9497 - val_loss: 0.0900 - val_accuracy: 0.9825\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0681 - accuracy: 0.9504 - val_loss: 0.1386 - val_accuracy: 0.9474\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0674 - accuracy: 0.9510 - val_loss: 0.1214 - val_accuracy: 0.9474\n",
      "Epoch 19/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0670 - accuracy: 0.9514 - val_loss: 0.1155 - val_accuracy: 0.9298\n",
      "Epoch 20/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0663 - accuracy: 0.9520 - val_loss: 0.1030 - val_accuracy: 0.9649\n",
      "Epoch 20: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re2.h5\n",
      "102188/102188 [==============================] - 318s 3ms/step\n",
      "ModelAccuracy: 93.420%\n",
      "True Win Predictions Mean of all: 45.739%\n",
      "XXX Loss Buy Mean of all: 2.319%\n",
      "Missed good deal off all: 4.261%\n",
      "Good Zero prediction Mean: 47.681%\n",
      "good fiability\n",
      "========= Win Ratio:95.17458071496941 ====================\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_23 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 86s 32ms/step - loss: 0.1202 - accuracy: 0.9063 - val_loss: 0.1593 - val_accuracy: 0.9649\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 86s 33ms/step - loss: 0.1004 - accuracy: 0.9223 - val_loss: 0.1473 - val_accuracy: 0.9123\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0904 - accuracy: 0.9305 - val_loss: 0.1242 - val_accuracy: 0.9298\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0838 - accuracy: 0.9362 - val_loss: 0.0986 - val_accuracy: 0.9474\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0795 - accuracy: 0.9400 - val_loss: 0.1181 - val_accuracy: 0.9474\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0762 - accuracy: 0.9428 - val_loss: 0.1213 - val_accuracy: 0.9474\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0735 - accuracy: 0.9452 - val_loss: 0.0897 - val_accuracy: 0.9649\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 88s 34ms/step - loss: 0.0715 - accuracy: 0.9471 - val_loss: 0.1069 - val_accuracy: 0.9649\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0699 - accuracy: 0.9485 - val_loss: 0.1250 - val_accuracy: 0.9298\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0687 - accuracy: 0.9497 - val_loss: 0.0729 - val_accuracy: 0.9825\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0674 - accuracy: 0.9507 - val_loss: 0.1018 - val_accuracy: 0.9474\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0663 - accuracy: 0.9517 - val_loss: 0.0894 - val_accuracy: 0.9474\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0654 - accuracy: 0.9527 - val_loss: 0.0916 - val_accuracy: 0.9474\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0645 - accuracy: 0.9534 - val_loss: 0.0911 - val_accuracy: 0.9474\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0639 - accuracy: 0.9539 - val_loss: 0.0784 - val_accuracy: 0.9649\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0632 - accuracy: 0.9545 - val_loss: 0.0727 - val_accuracy: 0.9649\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0626 - accuracy: 0.9550 - val_loss: 0.0978 - val_accuracy: 0.9298\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0620 - accuracy: 0.9556 - val_loss: 0.0561 - val_accuracy: 0.9825\n",
      "Epoch 19/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0615 - accuracy: 0.9561 - val_loss: 0.1021 - val_accuracy: 0.9649\n",
      "Epoch 20/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0610 - accuracy: 0.9566 - val_loss: 0.0393 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0606 - accuracy: 0.9569 - val_loss: 0.0587 - val_accuracy: 0.9649\n",
      "Epoch 22/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0601 - accuracy: 0.9573 - val_loss: 0.0839 - val_accuracy: 0.9474\n",
      "Epoch 23/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0598 - accuracy: 0.9576 - val_loss: 0.0487 - val_accuracy: 0.9825\n",
      "Epoch 24/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0594 - accuracy: 0.9580 - val_loss: 0.0507 - val_accuracy: 1.0000\n",
      "Epoch 25/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0590 - accuracy: 0.9583 - val_loss: 0.0395 - val_accuracy: 0.9825\n",
      "Epoch 26/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0587 - accuracy: 0.9586 - val_loss: 0.0637 - val_accuracy: 0.9825\n",
      "Epoch 27/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0583 - accuracy: 0.9589 - val_loss: 0.0525 - val_accuracy: 0.9649\n",
      "Epoch 28/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0580 - accuracy: 0.9592 - val_loss: 0.0538 - val_accuracy: 0.9825\n",
      "Epoch 29/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0578 - accuracy: 0.9594 - val_loss: 0.0470 - val_accuracy: 0.9825\n",
      "Epoch 30/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0574 - accuracy: 0.9597 - val_loss: 0.0330 - val_accuracy: 1.0000\n",
      "Epoch 31/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0573 - accuracy: 0.9599 - val_loss: 0.0453 - val_accuracy: 0.9825\n",
      "Epoch 32/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0570 - accuracy: 0.9601 - val_loss: 0.0507 - val_accuracy: 0.9825\n",
      "Epoch 33/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0566 - accuracy: 0.9604 - val_loss: 0.0338 - val_accuracy: 1.0000\n",
      "Epoch 34/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0564 - accuracy: 0.9605 - val_loss: 0.0418 - val_accuracy: 0.9825\n",
      "Epoch 35/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0565 - accuracy: 0.9605 - val_loss: 0.0751 - val_accuracy: 0.9649\n",
      "Epoch 36/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0560 - accuracy: 0.9609 - val_loss: 0.0210 - val_accuracy: 1.0000\n",
      "Epoch 36: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re3.h5\n",
      "102188/102188 [==============================] - 321s 3ms/step\n",
      "ModelAccuracy: 93.393%\n",
      "True Win Predictions Mean of all: 45.495%\n",
      "XXX Loss Buy Mean of all: 2.103%\n",
      "Missed good deal off all: 4.505%\n",
      "Good Zero prediction Mean: 47.897%\n",
      "good fiability\n",
      "========= Win Ratio:95.58174713223244 ====================\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_27 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 82s 31ms/step - loss: 0.1194 - accuracy: 0.9072 - val_loss: 0.1548 - val_accuracy: 0.9649\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 84s 33ms/step - loss: 0.0988 - accuracy: 0.9239 - val_loss: 0.1392 - val_accuracy: 0.9649\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0884 - accuracy: 0.9322 - val_loss: 0.1381 - val_accuracy: 0.9649\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0817 - accuracy: 0.9378 - val_loss: 0.0862 - val_accuracy: 0.9649\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0771 - accuracy: 0.9419 - val_loss: 0.0719 - val_accuracy: 0.9649\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0739 - accuracy: 0.9448 - val_loss: 0.0758 - val_accuracy: 0.9649\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0714 - accuracy: 0.9470 - val_loss: 0.0918 - val_accuracy: 0.9649\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0692 - accuracy: 0.9490 - val_loss: 0.1031 - val_accuracy: 0.9474\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0676 - accuracy: 0.9505 - val_loss: 0.0681 - val_accuracy: 0.9649\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0664 - accuracy: 0.9517 - val_loss: 0.0608 - val_accuracy: 0.9825\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0652 - accuracy: 0.9527 - val_loss: 0.0588 - val_accuracy: 0.9825\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0638 - accuracy: 0.9539 - val_loss: 0.0662 - val_accuracy: 0.9649\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0631 - accuracy: 0.9545 - val_loss: 0.0469 - val_accuracy: 0.9825\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0622 - accuracy: 0.9554 - val_loss: 0.0612 - val_accuracy: 0.9825\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0615 - accuracy: 0.9560 - val_loss: 0.0496 - val_accuracy: 0.9825\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0608 - accuracy: 0.9567 - val_loss: 0.0382 - val_accuracy: 0.9825\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0602 - accuracy: 0.9572 - val_loss: 0.0565 - val_accuracy: 0.9825\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0597 - accuracy: 0.9576 - val_loss: 0.0512 - val_accuracy: 0.9825\n",
      "Epoch 19/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0592 - accuracy: 0.9582 - val_loss: 0.0408 - val_accuracy: 0.9825\n",
      "Epoch 20/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0587 - accuracy: 0.9586 - val_loss: 0.0405 - val_accuracy: 0.9825\n",
      "Epoch 21/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0584 - accuracy: 0.9589 - val_loss: 0.0358 - val_accuracy: 0.9825\n",
      "Epoch 22/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0579 - accuracy: 0.9593 - val_loss: 0.0423 - val_accuracy: 0.9825\n",
      "Epoch 23/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0575 - accuracy: 0.9597 - val_loss: 0.0542 - val_accuracy: 0.9825\n",
      "Epoch 24/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0571 - accuracy: 0.9601 - val_loss: 0.0555 - val_accuracy: 0.9825\n",
      "Epoch 25/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0568 - accuracy: 0.9602 - val_loss: 0.0567 - val_accuracy: 0.9825\n",
      "Epoch 26/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0566 - accuracy: 0.9605 - val_loss: 0.0418 - val_accuracy: 0.9825\n",
      "Epoch 26: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re4.h5\n",
      "102188/102188 [==============================] - 319s 3ms/step\n",
      "ModelAccuracy: 93.016%\n",
      "True Win Predictions Mean of all: 44.967%\n",
      "XXX Loss Buy Mean of all: 1.951%\n",
      "Missed good deal off all: 5.033%\n",
      "Good Zero prediction Mean: 48.049%\n",
      "good fiability\n",
      "========= Win Ratio:95.84168123108402 ====================\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_31 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 82s 31ms/step - loss: 0.1171 - accuracy: 0.9091 - val_loss: 0.1523 - val_accuracy: 0.9123\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 83s 32ms/step - loss: 0.0962 - accuracy: 0.9254 - val_loss: 0.1559 - val_accuracy: 0.9474\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0857 - accuracy: 0.9342 - val_loss: 0.1225 - val_accuracy: 0.9474\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0792 - accuracy: 0.9398 - val_loss: 0.0712 - val_accuracy: 0.9825\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0748 - accuracy: 0.9435 - val_loss: 0.0605 - val_accuracy: 0.9825\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0717 - accuracy: 0.9464 - val_loss: 0.0601 - val_accuracy: 0.9825\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 88s 35ms/step - loss: 0.0691 - accuracy: 0.9488 - val_loss: 0.0707 - val_accuracy: 0.9649\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0672 - accuracy: 0.9506 - val_loss: 0.0674 - val_accuracy: 0.9649\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0656 - accuracy: 0.9520 - val_loss: 0.0610 - val_accuracy: 0.9649\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0643 - accuracy: 0.9533 - val_loss: 0.0564 - val_accuracy: 0.9825\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0630 - accuracy: 0.9543 - val_loss: 0.0481 - val_accuracy: 0.9825\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0621 - accuracy: 0.9554 - val_loss: 0.0556 - val_accuracy: 0.9825\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0612 - accuracy: 0.9561 - val_loss: 0.0545 - val_accuracy: 0.9649\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0604 - accuracy: 0.9568 - val_loss: 0.0519 - val_accuracy: 0.9825\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0596 - accuracy: 0.9577 - val_loss: 0.0484 - val_accuracy: 0.9825\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0588 - accuracy: 0.9582 - val_loss: 0.0627 - val_accuracy: 0.9825\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0583 - accuracy: 0.9587 - val_loss: 0.0480 - val_accuracy: 0.9825\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0578 - accuracy: 0.9592 - val_loss: 0.0409 - val_accuracy: 0.9825\n",
      "Epoch 19/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0574 - accuracy: 0.9596 - val_loss: 0.0624 - val_accuracy: 0.9649\n",
      "Epoch 20/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0566 - accuracy: 0.9603 - val_loss: 0.0502 - val_accuracy: 0.9825\n",
      "Epoch 20: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re5.h5\n",
      "102188/102188 [==============================] - 319s 3ms/step\n",
      "ModelAccuracy: 92.444%\n",
      "True Win Predictions Mean of all: 44.714%\n",
      "XXX Loss Buy Mean of all: 2.270%\n",
      "Missed good deal off all: 5.286%\n",
      "Good Zero prediction Mean: 47.730%\n",
      "good fiability\n",
      "========= Win Ratio:95.16856802315681 ====================\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_35 (Bat  (None, 286)              1144      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 200)               57400     \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 200)              800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 200)               0         \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 80)                0         \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 80)               320       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 20)                1620      \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 97,145\n",
      "Trainable params: 95,853\n",
      "Non-trainable params: 1,292\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_v5.h5\n",
      "Epoch 1/500\n",
      "2555/2555 [==============================] - 82s 31ms/step - loss: 0.1156 - accuracy: 0.9105 - val_loss: 0.1786 - val_accuracy: 0.9298\n",
      "Epoch 2/500\n",
      "2555/2555 [==============================] - 83s 32ms/step - loss: 0.0944 - accuracy: 0.9276 - val_loss: 0.1442 - val_accuracy: 0.9474\n",
      "Epoch 3/500\n",
      "2555/2555 [==============================] - 86s 34ms/step - loss: 0.0835 - accuracy: 0.9363 - val_loss: 0.1326 - val_accuracy: 0.9649\n",
      "Epoch 4/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0767 - accuracy: 0.9422 - val_loss: 0.1026 - val_accuracy: 0.9649\n",
      "Epoch 5/500\n",
      "2555/2555 [==============================] - 87s 34ms/step - loss: 0.0720 - accuracy: 0.9461 - val_loss: 0.0688 - val_accuracy: 0.9649\n",
      "Epoch 6/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0689 - accuracy: 0.9491 - val_loss: 0.0493 - val_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0664 - accuracy: 0.9512 - val_loss: 0.0456 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0645 - accuracy: 0.9530 - val_loss: 0.0461 - val_accuracy: 0.9825\n",
      "Epoch 9/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0627 - accuracy: 0.9547 - val_loss: 0.0313 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0615 - accuracy: 0.9558 - val_loss: 0.0511 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0603 - accuracy: 0.9568 - val_loss: 0.0365 - val_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0594 - accuracy: 0.9578 - val_loss: 0.0371 - val_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0586 - accuracy: 0.9585 - val_loss: 0.0314 - val_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0577 - accuracy: 0.9593 - val_loss: 0.0416 - val_accuracy: 0.9825\n",
      "Epoch 15/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0571 - accuracy: 0.9598 - val_loss: 0.0426 - val_accuracy: 0.9825\n",
      "Epoch 16/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0564 - accuracy: 0.9603 - val_loss: 0.0334 - val_accuracy: 0.9825\n",
      "Epoch 17/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0557 - accuracy: 0.9610 - val_loss: 0.0456 - val_accuracy: 0.9825\n",
      "Epoch 18/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0552 - accuracy: 0.9614 - val_loss: 0.0411 - val_accuracy: 0.9825\n",
      "Epoch 19/500\n",
      "2555/2555 [==============================] - 89s 35ms/step - loss: 0.0549 - accuracy: 0.9617 - val_loss: 0.0453 - val_accuracy: 0.9825\n",
      "Epoch 20/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0543 - accuracy: 0.9622 - val_loss: 0.0402 - val_accuracy: 0.9825\n",
      "Epoch 21/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0539 - accuracy: 0.9626 - val_loss: 0.0742 - val_accuracy: 0.9825\n",
      "Epoch 22/500\n",
      "2555/2555 [==============================] - 90s 35ms/step - loss: 0.0536 - accuracy: 0.9628 - val_loss: 0.0432 - val_accuracy: 0.9825\n",
      "Epoch 22: early stopping\n",
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_true_win_model_Re6.h5\n",
      "102188/102188 [==============================] - 316s 3ms/step\n",
      "ModelAccuracy: 92.478%\n",
      "True Win Predictions Mean of all: 44.334%\n",
      "XXX Loss Buy Mean of all: 1.856%\n",
      "Missed good deal off all: 5.666%\n",
      "Good Zero prediction Mean: 48.144%\n",
      "good fiability\n",
      "========= Win Ratio:95.98181424550768 ====================\n"
     ]
    }
   ],
   "source": [
    "#Change retaindt\n",
    "for rrr in range(1,7):\n",
    "    retrain_dt=dt\n",
    "    class_1_weight=TrueWinPred.mean()\n",
    "\n",
    "    import gc\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import Nadam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    # Define the class weights\n",
    "    index_20pct=int(retrain_dt.shape[1]*0.2)\n",
    "    class_weights = {0: 1-class_1_weight, 1: class_1_weight}\n",
    "    gc.collect()\n",
    "\n",
    "    SizeTunner = 1\n",
    "    IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"saving file in: \" + Model_FileName)\n",
    "    history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                        TrueWinPred[index_20pct:],\n",
    "                        validation_data=(retrain_dt[:index_20pct, :-1], TrueWinPred[:index_20pct]),\n",
    "                        epochs=500,\n",
    "                        batch_size=256*5,\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weights)\n",
    "\n",
    "    #868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "    #Results after 380 min\n",
    "    # Epoch 133/500\n",
    "    # 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "    # Epoch 134/500\n",
    "    # 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "    # Epoch 134: early stopping\n",
    "\n",
    "    true_win_model=model\n",
    "    wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re{rrr}.h5\"\n",
    "    true_win_model.save(wheretosave)\n",
    "    print(wheretosave)\n",
    "    USED_MODEL=true_win_model\n",
    "    bad_Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "    Pred02=bad_Prediction_Note.round()\n",
    "    Original_Traget_Data=Y\n",
    "    Predicted_Data=Pred02[:,0]\n",
    "\n",
    "    BadTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    BadModelAccuracy=hp(BadTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "    BadTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    BadTrueWinPred_Mean=hp(BadTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "    BadLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    BadLossPred_Mean=hp(BadLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "    BadMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    BadMissedDeal_Mean=hp(BadMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "    BadGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    BadGoodZero_Mean=hp(BadGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "    fiability=BadTrueWinPred_Mean + BadLossPred_Mean + BadMissedDeal_Mean + BadGoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=BadTrueWinPred_Mean/(BadLossPred_Mean+BadTrueWinPred_Mean)\n",
    "    print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "    ## for retraining again\n",
    "    TrueWinPred=BadTrueWinPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/UltimeTradingBot/Data/BUY_OPTIMAL/tp320_w7_max7min_Model_GoodVeryDeep_v2.h5\n"
     ]
    }
   ],
   "source": [
    "very_deep_good_model=model\n",
    "wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_GoodVeryDeep_v2.h5\"\n",
    "very_deep_good_model.save(wheretosave)\n",
    "print(wheretosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_deep_bad_model=model\n",
    "very_deep_bad_model.save(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_BadVeryDeep_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_expand5(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function    \n",
    "    print(f\"mini_expand : {pair}\")\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    pair_full = full_expand(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair], window_size)\n",
    "    btc_full = full_expand(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], window_size)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  mini_expand5 {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test On special coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mini_expand : GMT/USDT\n",
      "fied buy window=7\n",
      "---fixed buy--- Buy pct: 0.55% MaxForcastSize: 7\n",
      "---fixed buy--- no b\n",
      "False\n",
      "False\n",
      "Buy mean pct: 13.091%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiwAAAGdCAYAAAAxCSikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAA9hAAAPYQGoP6dpAAByMUlEQVR4nO3dd3hURdsG8Hs3m55NgUAaaSSUhACBhN6rgCiCClgRRAX8FHxtICKoFH0VFUTB14oIgg0RAaVIJ4JAIIEUSO+99zbfHyErS9pudjebbO7fdc0lOTtn5tnDhn2cM2dGAkCAiIiIqA2T6jsAIiIiouYwYSEiIqI2jwkLERERtXlMWIiIiKjNY8JCREREbR4TFiIiImrzmLAQERFRm8eEhYiIiNo8mb4D0CZnZ2cUFhbqOwwiIiJSg1wuR0pKSpN1DCZhcXZ2RnJysr7DICIiohZwcXFpMmkxmISlbmTFxcWFoyxERETthFwuR3JycrPf3QaTsNQpLCxkwkJERGRgOOmWiIiI2jwmLERERNTmMWEhIiKiNo8JCxEREbV5TFiIiIiozWPCQkRERG0eExYiIiJq85iwEBERUZvHhIWIiIjaPCYsRERE1OYxYSEiIqI2jwkLERERtXlMWNRkZGwMh+4eMDI21ncoREREHYbB7dasK2ZyKyz+YgucenrBSCZD8MHD+O7V1foOi4iIqEPgCIuKuri7oZtvLxz97Gv8/sEWDJg2Gf53TdB3WERERB0CR1hUZGEtBwD8s+8gclPT0M23N2a9/jKiL11BYVa2nqMjIiIybBxhUZH5rYSlpKAAAPDz2vdQXVmJe19+Xp9hERERdQhMWFRkbi1HdVUVyotLAAAl+QW4sPd3eAUM0HNkREREho8Ji4osrK1RVlikdCz1ZjRsHLrA3NpaT1ERERF1DExYVGRuLUdJQaHSsdSb0QAAp55e+giJiIiow2DCoiILazlK70hYMuMTUFVZCSfv7vXqGxkbQyKRtFZ4REREBo0Ji4rMreUovTXhtk5NVTUyYuPh2KP+CMvSnV9g0jPzWys8IiIig8bHmlVkbi1HUU5uveNpN6PhdEfC4tyrB1x8eqIgm487ExERaQNHWFRkYW2NkvyCesdTb0bD8Y5bQv1vLSjn4OnRGqEREREZPCYsKjJvYA4LAKTeiIa53Ap2To6KY/53TUBxXj46uTjBxNy8NcMkIiIySExYVNRownLrSaG6UZZuvr1g79YNJ7d/DwDo6unWekESEREZKCYsKpBIpTCXW9V7rBkA8tLSUVpYpHi0uf9dE1CUk4tzP/wCAHDo7tmqsRIRERkiTrpVgaipwZpx01FRUtrg62lRMXDq4QWpzAj9J09AyNETKC0oRG5qGrp292jdYImIiAwQR1hUVJiVjfKSkgZfS70ZDc+B/fHCnm9g69gV/+w7AADIiImDo5dHK0ZJRERkmJiwaEFK5E3YOTmiuqoKmx56Egkh1wEAaTFx6MonhYiIiDTGW0JacPG3gyjMykHYyTOoqa5WHM+IicOohx+EkbExqisr9RghERFR+8YRFi2oLCvHtb9OKiUrAJAeHQupkRG6ePBJISIiIk0wYdGh9Jg4AIADJ94SERFphAmLDpXkF6AwO4cJCxERkYaYsKhg+gvP4j8/bG/RuekxcUxYiIiINMSERQUWNtaorqpq0bnp0bEN7uZMREREqmPCooLaZfnrb3yoisTr4ejq6Q4zK0stR0VERNRxqJWwLFq0CFevXkV+fj7y8/Nx7tw5TJkypdH6I0aMwJkzZ5CVlYWSkhKEh4dj2bJl9erZ2Nhgy5YtSElJQWlpKcLCwjB16lS134yumFvLG1yWXxXxV69BKpXCra+vlqMiIiLqONRahyUpKQnLly9HVFQUAGDevHnYt28fBgwYgLCwsHr1i4uLsWXLFoSEhKC4uBgjR47EZ599huLiYnz++ecAAGNjYxw5cgQZGRl44IEHkJSUBFdXVxQWtixB0AULa2tkxiW06NzMuASUFBTArZ8fbgT9o+XIiIiIOg6hScnOzhYLFixQuf7PP/8svv32W8XPzzzzjIiKihIymUyjOORyuRBCCLlcrlE7DZWVf/wipj6/qMXnL9z6gVj46Uatx8XCwsLCwtLei6rf3y2ewyKVSjFnzhxYWloiKChIpXP8/f0xfPhwnDx5UnHs3nvvRVBQED755BOkpaUhNDQUK1asgFTadGgmJiaQy+VKRVdq57C0fMQn/uo1uPfz02JEREREHY9amZCfn58oLCwUlZWVIjc3V0ydOrXZcxITE0VZWZmoqqoSr7/+utJr4eHhorS0VHzxxRdi4MCBYs6cOSIrK0usWrWqyTZXr14tGqLtERaJVCo2hgaJIbPuaXEbPYcNFhtDg0QXDze9Z7IsLCwsLCxtqahxh0S9ho2NjYWXl5cICAgQ69evFxkZGcLHx6fJczw8PISfn59YuHChyMrKEnPnzlW8FhkZKeLj44VUKlUce+GFF0RKSkqTbZqYmAi5XK4ozs7OOklYLGysxcbQINF34tgWt2FmZSneu3pWBN47Te8fDBYWFhYWlrZUVE1Y1N78sLKyEtHR0QCAS5cuYdCgQVi6dCkWLVrU6DlxcXEAgGvXrsHBwQFr1qzB7t27AQCpqamorKxETU2Non54eDicnJxgbGyMykY2DayoqEBFRYW64avN3NoaADS6JVRWVIz06Fi49/fDxd8Oais0IiKiDkPjdVgkEglMTU1bXP/s2bPw9vaGRCJRHOvZsydSUlIaTVZak4V17dwYTRIWAEgIuQ73fn20ERIREVGHo1bCsm7dOowcORLu7u7w8/PD2rVrMXbsWOzcuRMAsH79emzf/u8S9kuWLMH06dPh7e0Nb29vPPHEE3jppZfw3XffKeps3boVnTt3xqZNm9CjRw9MmzYNr732Gj755BMtvUXNmN9KWEpauHBcnbir1+DUwwsm5ubaCIuIiKhDUeuWkIODA3bs2AEnJyfk5+cjJCQEU6ZMwdGjRwEATk5OcHNzU9SXSqXYsGEDPD09UVVVhejoaCxfvhyfffaZok5SUhImT56MDz/8ECEhIUhOTsamTZvw7rvvauktakZbIyxJYRGQGhnBqacX4q9e00ZoREREHYreJ9xoo+hqHZYBUyeJ1X/tFxKJRKN2jIyNxX+DT4uhD96n92vFwsLCwsLSVorOJt12NMGHjiD40BGN26murERGbDyce3prISoiIqKOhZsftqLUm9Fw4s7NREREamPC0opSb0TBiSMsREREamPC0opSbkTBXG4FO2dHfYdCRETUrjBhaUUpkbW7XOtqHsuAaZOx4cJxzF6zgiM5RERkUJiwtKKCjEwU5+XrJJmQGhnhriULkZWYhF4jh+Kln3fg3leWQioz0npfRERErY1PCbUyXc1j6X/XBHRxd8UHs+ch9WY0Rsx9APf85//g1scH3/xnBYqyc7XeJxERUWvhCEsrS7kRpfVbQhKJBBOfmoewU2eRHH4DNVXVOP3dHnw6fwm6erpj8qIntdofERFRa2PC0spSb0TD3q0bjM1U33+pOX7jR8PRuzuO/u8bpeNxV0Nx7fhpePj31VpfRERE+sCEpZWl3oiC1MgIjl7dtdbm4Jn3IObSlQaX/A+/NepCRETUnjFhaWVp0TGorqpCtz69tdKeRCKBh39f3Pj7nwZfDz12EnveWKeVvoiIiPSFCUsrqywrR+L1cHgPGqiV9rp4uMHCxhpxV0K10h4REVFbxIRFD6IuXIaXlhIWzwH9UFNdjYSQ61ppj4iIqC1iwqIH0f9chrxzJ3T1dNe4LQ//fki9EY3ykhItREZERNQ2MWHRg7grIaiurIL34ACN2/Lw74vYKyFaiIqIiKjtYsKiBxWlZUi4FqbxbSFLWxt09XTn/BUiIjJ4TFj0JOqfS/AKHKBRG3Xrq8QFNz/CMmTWPRg+Z5ZG/REREekLExY9if4nGPLOneDg5VnvNYlUigdXL4ern2+916y72GPh1g8wYNpkeAzoh/z0TOSmpjXbn3s/Pwx9YIZWYiciImpt3EtIT+KuhKCqshLegwOQHh2r9Nrg++7G0AdmwMLWBttfWKH02vT/PIsegwPgM3IYqquqEHrspEr9JV6PQOC90yAzMUFVRYXW3gcREVFr4AiLnlSWlSPq/CVM/b+n0W/yeMVxE3NzTPm/p1Gcm4c+Y0bC0s5W8ZpH/74ImD4FP699H589/TwSr4fjyqEjKvWXFB4JI2MZnHp4afutEBER6RwTFj3a8coqRAZdwLyN6/DQ+jfg4OWJcfMfgbm1HP9btAwCAgOmTgJQu6LtjOXLkBgWgX/2HcCNoH/w8aNPqzzCknozGtVVVXDx7aXLt0RERKQTvCWkR2WFRdjx0uuIPHseU597GoH3TEV1VRVObt+FpLBIhJ08i8H3TceZXT9i3ILH4Obniy2PPwNRU6N2X1Xl5UiPiUM3HyYsRETU/jBhaQMu7N2PS/sPod+kcfAeHIBjX3wLAPjn1wN4cst7mLv2dQyacTeO/u8bxKrwRFBjksMj4eLTU1thExERtRomLG1EdVUVgg8dQfBtc1IizgahMDsHg2bcjYObtuHYF9s16iMpLAIDpk6CVGaEmqpqTUMmIiJqNUxY2rCaqmr89Na7MDE3w+UDhzVuLynsBmQmJnDo7onUG1FaiJCIiKh1MGFp4679dUprbaVE3kRNTQ26+fZiwkJERO0KnxLqQCpKS5EZl4Buvr31HQoREZFamLB0MMnhkXxSiIiI2h0mLB1MUlgknHv1gNTISN+hEBERqYwJSweTeD0cJuZm6OLhpu9QiIiIVCYBIPQdhDbI5XIUFBTA2toahYWF+g6nzZLKjGAul6M4N0/foRAREan8/c0Rlg6mpqqayQoREbU7TFioHqee3li2+ys+TURERG0GExaq596XnoNrHx88/dlHcOTuzkRE1AYwYSElPYcNQs9hg/H9yreRl5qOZ/63CXbOjvoOi4iIOjgmLKQgkUhw97JnERscgou/HcRnzyyFzMQYg2bcre/QiIiog2PC0kEZm5nCa9BApWP975qAbr69cODDTwAAxbl5iLsSCrd+ffQRIhERkQITlg7Ka9BALPnqE9i7dVMcm7DwcYSfCUJscIjiWEJoGNz7MmEhIiL9UithWbRoEa5evYr8/Hzk5+fj3LlzmDJlSqP1R4wYgTNnziArKwslJSUIDw/HsmXLGq0/Z84cCCGwd+9edcKiFogLDkFleTmG3j8DANBj6CA49+qBE1/vVKoXf/UaLGysYe/uqo8wiYiIAKi5W3NSUhKWL1+OqKjanX7nzZuHffv2YcCAAQgLC6tXv7i4GFu2bEFISAiKi4sxcuRIfPbZZyguLsbnn3+uVNfNzQ3vv/8+Tp3S3u7E1LiyomIc/d83mLz4SVw68AfGznsYyeE3EHXhklK9hGu1f6/u/fyQFZ+oj1CJiIgA1K502+KSnZ0tFixYoHL9n3/+WXz77bdKx6RSqTh9+rRYsGCB+Prrr8XevXvVjkMulwshhJDL5Rq9n45UjGQy8fLeneLV33aLjaFBYuDdkxus98q+78WslS/pPV4WFhYWFsMrqn5/t3gOi1QqxZw5c2BpaYmgoCCVzvH398fw4cNx8uRJpeNvvPEGMjMz8dVXX7U0HGqB6qoq/PTWu+jq6Y68tHRc+fNYg/XiQ65x4i0REemVWreEAMDPzw9BQUEwMzNDUVERZs6cifDw8CbPSUxMRJcuXSCTybBmzRp8+eWXiteGDx+OJ598Ev7+/mrFYWJiAlNTU8XPcrlcrfOpVmxwCH5Z9z6yk5JRU1XdYJ34kOsImD4FxmamqCwrb+UI9UtqZISa6oavCxERtS61hm6MjY2Fl5eXCAgIEOvXrxcZGRnCx8enyXM8PDyEn5+fWLhwocjKyhJz584VAISVlZWIiYkRU6ZMUdRV9ZbQ6tWrRUN4S0j7xblXD7ExNEh4Duyv91has0xYOE+sOPCjsLCx1nssLCwsLIZa1JjSoVlHR44cEdu2bVO5/sqVK0VERIQAIPr37y+EEKKyslJRqqurRXV1taisrBTdu3dvtB0TExMhl8sVxdnZmQmLjorUyEisP/+XGDvvYb3H0ppl8ZdbxMbQIPH0tg+FRCrVezwsLCwshlhUTVjUviV0J4lEonRrRp36ERER8PPzU3p97dq1kMvlWLp0KRITG38qpaKiAhUVFS0LmtRSU12NpLAIuPb11XcorUYilaJbn94IO3UWvUcMxd1LF+P0rh9QnFeAqvKOdVuMiKgtUCthWbduHQ4dOoTExETI5XLMnTsXY8eOVazFsn79eri4uGDevHkAgCVLliAhIQEREREAgJEjR+Kll17Cxx9/DAAoLy/H9evXlfrIy8sDgHrHSb/SY+Lg3oEm3jp094CZpSVOfL0TsZev4u5lSzBuwaOoqa7Gj2++iwt79+s7RCKiDkWthMXBwQE7duyAk5MT8vPzERISgilTpuDo0aMAACcnJ7i5uSnqS6VSbNiwAZ6enqiqqkJ0dDSWL1+Ozz77TLvvgnQuMz4BAdMbXyTQ0Lj17YOamhokhUUi+mIwwk+fg03XLgi8ZyruW/4CYi4FIyshSd9hEhF1KHq/f6WNwnVYdFt8x4wUG0ODhHXXLnqPpTXKA6tfFS/98l294ybm5mLFgR/F8999LqRGRnqPk4WFhaW9F52vw0IdS1ZC7XyiLrftPWTI3Px8kRBS/7ZkRWkpvn/tLbj6+WD0Y3P1EBkRUcfEhIVUkp2YjJrqanTxcGu+cjtnYm4Gpx5eiA9teB5V3NVQXD38F/pPHt/KkRERdVxMWEgl1VVVyElJhb2b4W+C2M23N6RGRkgIrb8/Vp2YS1fg0rsnZGo8IUdERC3HhIVUlhWfhC7uhn9LyK1vH5SXlCI9OrbROnFXQmFkLINrn96tGBkRUcfFhIVUlhmfAHt3w78l5NbXF0lhEU0uyZ8WFYOy4mJ4+PdtxciIiDouJiyksqyERNi7ukAiNeyPTVdPd6TeiGqyTk11NRJCw5iwEBG1EsP+5iGtyoxPgszEBLaOXfUdik5Z2tmiMDun2XpxV0Lh0Z8JCxFRa2DCQirLjE8AAHRxN+yJt5a2NijOzW+2XtyVUFh1soN9B3nUm4hIn5iwkMryUtNRVVmJLgY8j8VMbgUjmQzFt7aIaEp8yDUAgId/Px1HRURETFhIZTXV1chJSjHoR5ut7GwBAMW5ec3WLSssQlpUDNz7+zVbl4iINMOEhdSSGZcAewN+tNmyLmHJa/6WEFB7W8hzAEdYiIh0jQkLqSUzIRFdPdz1HYbOWNraAgCKcnNVqp8UHomuHu6Qyox0GBURETFhIbXEXAyGvVs3+Iweoe9QdMLSzgYAUJJfoFL9rPhEGBnL0MnZSZdhERF1eExYSC3XT5xB5Nm/Meu1F2FibqbvcLTO0tYWJQUFqKlqfNG422XG124KaW/gT04REekbExZS289r34fcvhMmLVqg71C0ztJOtUea6+SnZ6CyrNygn5wiImoLmLCQ2rKTknH0f99gzOMPwc7ZUd/haJWlra1KjzTXEUIgMyHR4NemISLSNyYs1CJndv0II5kM3QMG6DsUrVJ3hAWoncfChIWISLeYsFCLlBUVIyM23uB2K1Z3hAWoncfCOSxERLrFhIVaLCksAq59fPQdhlapuiz/7bLiE2Hr6ACZqamOoiIiIiYs1GKJ1yPg0rsnpEaGswaJVSe7FoywJEAqlcLe1UU3QRERERMWarnE6+EwNjOFg5envkPRCqmRESxsrNUeYVE82mzAWxYQEekbExZqseTwG6iprjaY20IWNtYAoPYIS1FOLkoLi9DFgwkLEZGuMGGhFqsoLUV6TJzBTLy1tK1d5bY4J0/tczPjE9CFIyxERDrDhIU0khQWgW6GkrDc2viwSM0RFqB24q09R1iIiHSGCQtpJPF6BJx79YCRsbG+Q9GYYoRFzTksQO08Fo6wEBHpDhMW0kji9XDIjI3h1MNL36FozNLOFjXV1SgrLFT73Mz4RFh3sYeppYUOIiMiIiYspJGUyChUV1YZxMTb2kXj8iGEUPvcjNg4AICTd/tP3IiI2iImLKSRqvJy5KVnGMSeQpZ2NijOU/92EACk3oxGWXExPAP6azkqIiICmLCQFhTl5MLq1oTV9qwly/LXqamqRlxwKLwGDdRuUEREBIAJC2lBUU4urDrZ6TsMjVl2sm3RhNs60ReD4Tmgn0Gt/EtE1FYwYSGNGUzCYmvT4hEWAIi5GAwzS0u49O6pvaCIiAgAExbSgqKcXFh1NoSERbMRlsTr4agoLYNX4AAtRkVERAATFtICgxlhsdNshKW6qgpxV0PRnQkLEZHWMWEhjRXl5MDUwgIm5mb6DqXFZCYmMLO0bNGy/LeLvhiM7gP7QyLlrxYRkTbxX1XSWNGtL3nLdvykUI8hgQCAtOgYjdqJuRgMc2s5/MaNQo+hg2Dr0FUb4RERdXgyfQdA7V9RTi4AwKpTJ+SmpOk5GtVJAYwC4ARggH9fJF8LR3L4DY3ajA+5jvKSUjzx0TsAgKh/LmPrgmc1jpWIqKNjwkIa+zdhaT/zWGYC2ARAsfvPx58h/YvtiAWwV4N2qysrsfnRp2BmaYmewwZhwsLHITM1RVV5uaYhExF1aExYSGPFuXkAAKtOtnqNA/h31MQZQFcAmQCSAZwGUHOrzv0Afmzg3C6lZfgJwAPQLGlJuxkNACgvKcFdSxbCra8vYi4Ga9AiERExYSGNVVdVoSS/QO8jLPVGTW6TCOAFAL4AVgOQNFBHitqk5iMA+/BvgtNSaVExKCkoQPcAfyYsREQaUmvS7aJFi3D16lXk5+cjPz8f586dw5QpUxqtP2LECJw5cwZZWVkoKSlBeHg4li1bplRn4cKFOHXqFHJycpCTk4MjR45g0KBBLXozpD/6frR5JoCfAbg08roLakdV3gLQ1Dq0UgBuqB2l0ZSoqUHs5RB4BfhroTUioo5NrYQlKSkJy5cvR2BgIAIDA/HXX39h37598PX1bbB+cXExtmzZgtGjR8PHxwdr167F2rVr8dRTTynqjB07Ft9//z3GjRuHYcOGISEhAYcPH4azs7Nm74xaVWFOjt4SFimArWZmij83VkcdTpoEdJuYS1fg3r8vpDIu109EpCmhScnOzhYLFixQuf7PP/8svv3220Zfl0qlIj8/Xzz22GNqxSGXy4UQQsjlco3eD0vLyuMb14mnt32ol75nOjsJAWi1jNFSbK5+vmJjaJBw69dH739HLCwsLG2xqPr93eJ1WKRSKebMmQNLS0sEBQWpdI6/vz+GDx+OkydPNlrHwsICxsbGyMnJabItExMTyOVypUL6U3tLqFOr9yszNcUDc+/XWns1ABJQO0lXG5IjIlFeUsLbQkREGlI7YfHz80NhYSHKy8uxbds2zJw5E+Hh4U2ek5iYiLKyMly8eBGffPIJvvzyy0brvvPOO0hOTsbRo0ebbHPFihUoKChQlOTkZHXfCmmRvuaw+I4eDpmfj9bakwBYBs0n3NapqapG3JVQdA/gcv1ERJpQO2GJjIyEv78/hg4diq1bt2L79u3w8Wn6C2PUqFEIDAzEokWLsGzZMsydO7fBei+//DIeeughzJo1C+XNrFuxYcMGWFtbK4qLS2PTLak1FOXkwlIPjzX7jB6OS9bWSITmSUYVgAeh2SPNDYm5dAWeA/tDZmKi5ZaJiDoWje49HTlyRGzbtk3l+itXrhQRERH1jr/44osiNzdXBAQE6PQeGItuSr9J48TG0CBhJrfSWR9mVpZKP0skErHmxAEx/YVnxUxAVN8qQs1Sc+u8+3UUt727q9gYGiQGTr9L739PLCwsLG2t6HwOSx2JRAJTU1ON6r/00ktYtWoVpkyZgkuXLmkaEumBLle7lUilWPL1p3j1t91Kx118ekHeuRPCTp/DXtQu+NaSG4NZt879WfNQG24/PhE3/v4Hwx+cqXFbphYWWoiIiKj9USthWbduHUaOHAl3d3f4+flh7dq1GDt2LHbu3AkAWL9+PbZv366ov2TJEkyfPh3e3t7w9vbGE088gZdeegnfffedos7LL7+MtWvXYsGCBYiLi4ODgwMcHBxgaWmppbdIraEuYZHrIGFx9O4Or8ABOP71TqVdkH1HD0dpQSHiroQAqL2V4wFgLIBHAOSi6dtE1ahdRM4R2r8NdKegH/bCc2B/OPbwanEbPYcNxvrzx7ihIhF1SGqtdOvg4IAdO3bAyckJ+fn5CAkJwZQpUxQTZJ2cnODm5qaoL5VKsWHDBnh6eqKqqgrR0dFYvnw5PvvsM0WdJUuWwNTUFD//rPz/t2vWrMGbb76pyXujVlSYrbsRFq9Af1RVVODcD3shav5NQXxGj0DkufOoqapWHKsBUPcMWimAn24duz0zr2thDnQ3qnKna8dPoSAzC8Nnz8Qv695vsI7MxATTli3G8S93oDC7/lNyaVG1O0m79vVFXnqGTuMlImqL9H7/ShuFc1j0WyQSifhv8Gkx7MGZWm/78Y3rxP9tV54nZdXZTmwMDRIB90xt8tyZgEiA8pyV+FvHW/saTfm/p8XG0CDh3KtHg6879fQWa04cEG+f/VM4dPdosM6qI7+K6S88q/e/bxYWFhZtlVabw0IEAEIIFOfmwaqz9kdYugf4I/rSFaVjpQVF+HzJfxB28myT5+7Fv7eJHrr1X0/o/hZQQ4J+3IvM+ETYuze02xGQeiMK7977EKorqzBoxt0N1okPuQ63fn10GSYRUZvEhIW0Rhdrsdi7u0LeuRNi7khYqisrEXE6CKUFBc22UXebaPet/2prjRV15adn4p3psxFy+K9G65QWFODa8VPwmzCmwdcTQsPQzbc3pEZc6p+IOhYmLKQ1ukhYvAL8UVNdrZhY2xFcO3YSXdxdG5ygmxB6HaYW5nDw8tRDZERE+sOEhbQmPyMLto7afYKle8AAJEfeRHlxiVbbbctunr+E0sIi9G1glCUpLALVVVVw520hIupgmLCQ1qRFxcCphxckEonW2uwe4F/vdpChq66sRPjpc+g7vjZhkZmaos/YkTAxN0dFaRnSomLg5tfwDulERIaKCQtpTUrkTZhaWKBTt8a3SXDq6Y1ew4eo1J6towM6uTgh5uIVLUXYfoQeOwkXn57o5OIE9359sODj99DZ1RlA7TwWTrwloo5GrXVYiJqScuMmAMC5lzeyE5PqvW7V2Q7P/G8TTC0ssG7KLMVic43pHtAfABAbfFX7wbZxEaeDUFlejoWffgBTSwuU5Bcg7WbtOiwJIdcx5P57YWphgfKSjnOrjIg6No6wkNYUZeeiICsbzr161HtNIpHgobVvQAiBmppqjHm84Q0wb9c9YADSomNRnJung2jbtorSUvzx8f+QEnEDIYePY/frb0MIAQCID70OqVSKbn166zlKIqLWwxEW0qrUyJtw7uVd7/iox+ag98ih+OzppfAaNBCjHnkQJ77ZheK8/Ebb8vDv2+Hmr9zuxPZdDR7PiIlDWXExXPv4IPqfy60cFRGRfnCEhbQqJTIKzj2VR1gkEgkmLpyHc3t+wY2gCzi1YzcACUY9NqfJtrY8/gwOb/1Sh9G2T0IIpEZGwcWnp75DISJqNUxYSKtSbtxEJxcnmMmtFMe6dveApZ0tQo6eAAAU5+bh3J5fMOrh2TAxN2u0rbKiYhRmZes65HYpOfImXHozYSGijoMJC2lVSmQUACjNY+k+0B/VVVWIv3pNcezibwdhZmUJ9/59Wz1GQ5AcfgNdPNyaTPiIiAwJExbSqoy4eFRVVMC557/zWLoH9EdSWCQqSksVx9JvTab1ChygjzDbveSISEilUjj1rD9fiIjIEDFhIa2qqapGalSM8ghLgD9iLys/miyEQPSlK+ge6N/KERqGtKhYVFdW8bYQEXUYTFhI61IjoxRPCtk5O8LW0QExl6/UqxdzMRjufftAZmrayhG2f9WVlUiLjmHCQkQdBhMW0rrkiBtw6uEFW4eu6D7QHwDqjbAAQPTFYMhMTODWV3mZ+X6Tx0NmYtIaobZryRE3+KQQEXUYTFhI6y79/icKs3PwyLtvwntIAFJvRqMkv6BevdSb0SgpKFCax+I7ZiTmbVwHv/GjWzPkdik5vDYxlMqM9B0KEZHOMWEhrSstKMB3r6yGe38/BN47rcHRFQAQNTWIvXQVXgG1CYulnS1mv7kC10+cwZU/jrZmyO1ScsQNyExM4NDdQ9+hEBHpHBMW0om4KyH489MvIJVKEdNIwgLU3hZy7+8HD/9+mP/RO5BKpfhxzYZWjLT9Soms3bvJpXcvPUdCRKR7XJqfdOavL3cgJylFsWBcQ6IvBsPE3AzP7fgMmXEJ2P7iShRm57RekO1YeXEJMuMT4dK7Jy7+dlDf4RAR6RQTFtIZUVOD4ENHmqyTHHEDx7/eiaTr4bh65DhETU0rRWcYMmLjYe/WTd9hEBHpHBMW0itRU4PfP9ii7zDarby0dHgO6KfvMIiIdI5zWIjasby0dNg6Oeg7DCIinWPCQtSO5aWlw8LaGibm5voOhYhIp5iwELVjuanpAABbx656joSISLeYsBC1Y3lptQmLnZOjniMhItItJixE7Vh+RiZqamo4wkJEBo8JC1E7VlNVjcLMbNg6cuItERk2JixE7RyfFCKijoAJC1E7l5eewREWIjJ4TFiI2rnc1DTYOnAOCxEZNiYsRO1cXloGnxIiIoPHhIWonctLTYOxmSksbW30HQoRkc4wYSFq5/LSMgCAE2+JyKAxYSFq5+oWj+PEWyIyZExYiNq5opxcVFVUMGEhIoPGhIWonRNCIC89A3ZMWIjIgDFhITIAeWkZXJ6fiAwaExYiA5CXms5bQkRk0NRKWBYtWoSrV68iPz8f+fn5OHfuHKZMmdJo/REjRuDMmTPIyspCSUkJwsPDsWzZsnr1Zs2ahevXr6OsrAzXr1/Hfffdp+77IOrQuDw/ERk6tRKWpKQkLF++HIGBgQgMDMRff/2Fffv2wdfXt8H6xcXF2LJlC0aPHg0fHx+sXbsWa9euxVNPPaWoM3ToUOzZswc7duxA//79sWPHDvzwww8YPHiwZu+MqAMpzM6BlZ2dvsMgItIpoUnJzs4WCxYsULn+zz//LL799lvFz7t37xYHDx5UqnPo0CGxa9cuteKQy+VCCCHkcrlG74eFpT2WAdMmi42hQcLE3EzvsbCwsLCoU1T9/m7xHBapVIo5c+bA0tISQUFBKp3j7++P4cOH4+TJk4pjw4YNw+HDh5Xq/fnnnxg+fHiTbZmYmEAulysVoo6qODcPAGBpa6vXOIiIdEXthMXPzw+FhYUoLy/Htm3bMHPmTISHhzd5TmJiIsrKynDx4kV88skn+PLLLxWvOTo6Ij09Xal+eno6HB2b3htlxYoVKCgoUJTk5GR13wqRwSjOywMAWNrZ6DcQIiIdUTthiYyMhL+/P4YOHYqtW7di+/bt8PHxafKcUaNGITAwEIsWLcKyZcswd+5cpdeFEEo/SySSesfutGHDBlhbWyuKi4uLum+FyGAU5+YDACxsmLAQkWGSqXtCZWUloqOjAQCXLl3CoEGDsHTpUixatKjRc+Li4gAA165dg4ODA9asWYPdu3cDANLS0uqNpnTt2rXeqMudKioqUFFRoW74RAbp3xEWW73GQUSkKxqvwyKRSGBqatri+kFBQZg0aZJSncmTJ+PcuXOahkbUYVSWlaOyrJw7NhORwVJrhGXdunU4dOgQEhMTIZfLMXfuXIwdO1axFsv69evh4uKCefPmAQCWLFmChIQEREREAABGjhyJl156CR9//LGizU2bNuHUqVN45ZVXsG/fPsyYMQMTJ07EyJEjtfUeiTqE4rw8jrAQkcFSK2FxcHDAjh074OTkhPz8fISEhGDKlCk4evQoAMDJyQlubm6K+lKpFBs2bICnpyeqqqoQHR2N5cuX47PPPlPUCQoKwty5c7F27Vq8/fbbiI6Oxpw5c3DhwgUtvUWijqE4N58jLERksCSofb653ZPL5SgoKIC1tTUKCwv1HQ5Rq3vmf5tQUlCIHS+9ru9QiIhUpur3N/cSIjIQxbl5HGEhIoPFhIXIQBTn5XMOCxHB3Noa9u6ukMqM9B2KVqn9WDMRtU0cYSHqmCRSKXqPGIqAe6bAd8xImFqYAwAOb/sKf37yuZ6j0x6OsBAZCI6wEHVMQ+6/Fws/3QhH7+449sV2bP/Pawg+dARDZt0DqZHhjLJwhIXIQBTn5kFmbAxTSwuUF5foOxwiaiW2jl2Rm5qG92c9qjiWnZiMAVMnoffIYQg7eUaP0WkPR1iIDERxXu3y/BxlIepYLKytUZJXoHQsOeIGEsMiMGTWdD1FpX1MWIgMRBF3bCbqkCxtbVCSX1Dv+IVf9sNn9AjI7TvrISrtY8JCZCD+HWHhxFuijsTCxhrF+fn1jl8+eBg1VdUIvHeqHqLSPiYsRAaipC5h4QgLUYdibmPd4AhLWWERwk6dhe/oEXqISvuYsBAZiKqKCpQVF3OEhaiDsbRp+JYQAGTGJcDWyaGVI9INJixEBqQkr4AjLEQdjIWNtWKE9U65aemw6doFEmn7/7pv/++AiBRqd2zmCAtRRyGVGcHMyhIlBQ2PsOSlpcNIJoN1l/Y/8ZYJC5EBKc7Nh6UNExaijsLC2hoAGr0llJeWAQCwc3RstZh0hQvHERmQ4rw82Doaxv1qImqehU3TCUtGTBxWjbyr0dfbE46wEBmQ4tx87idE1IFY3BpRbWwOS011tUEkKwATFiKDUjuHxVbfYRBRK2luhMWQMGEhMiDFufmwsLGGRCLReV9mcis8/93n8BzYX+d9EVHDLG2ZsBBRO1SclwcjmQxmciud9zXxqSfg3t8PExY+rvO+iKhh5jbWKCsuRnVVlb5D0TkmLEQGpLhuPyEd3xbq3M0Fox6djWt/ncTu19fqtC8iapxFI6vcGiImLEQGpG4DRHknO532M/0/z6IoOwffvboaRTm5Ou2LiBpnYW2N0vzCZuuZWlq0+8Xj2nf0RKSkMCsbAGDVuZPO+nDt44N+k8bhwKatqCwr11k/RNS8xnZqvl33wAFY//cxdHZ1aaWodIMJC5EBKS0oRHVlFeQ6TFj6ThyLwuwcBB84rPI5EqkUC7d+gAfeeFVncRF1RI3t1Hy7/PRMAIBdO1+jiQvHERkQIQSKcnJ1mrD0HjEUkWfPQwih8jkTnpoHn5HDFP9wEpF2mNtYIysxuck6+em1q922900QOcJCZGAKs3Mgt9fNviFy+85w8emJiLN/q3yO58D+uGvxk4i9fBU2Dl24Ei+RFjW1U3OdqooKFGbntPvfPSYsRAamMDsb8s66mXTbe8QQ1NTU4Ma580rHTczN0T3Av159iVSKh9evRmxwCLa/uBIA4NHfTyexEXVETe3UfLu8tHTYOnRthYh0hwkLkYEpzM7R2aTbXiOGIvFaOIrv+Aey1/DBePabrbDuYq903NXPB51cnHBw0zYUZmUjKyEJ7v59dRIbUUfT3E7Nt8tNTYcdbwkRUVtSlJ2jkzksEqkUvYYPQWQDt4MSroUBANz69lE67jt6BIrz8hEfcg0AEHc1FB79mbAQaUNzOzXfLi8tHTa8JUREbUlhdi7knbU/h8Wtry8sbKwbnL+Sn56J/PRMuPdXTlh6jxqGyLN/Q9TUAADir16DS++ekJmaaj0+oo5GnX2E8jjCQkRtTWFWNkzMzWBqYaHVdjNi47Fz+WokXgtv8PX40OtKIyxy+85w9e2N8NPnFMfiroTCyFgG1z69tRobUUfU3E7Nt0uLjoWphQW6errrOiydYcJCZGAKs3MAaH/xuNKCQlw+cBg11dUNvp4Qeh2ufXorVtP0GTkMNTU1iDjz74hMWlQMyktKOPGWSAvUGWGJvXwFVRUV6DlssK7D0hkmLEQGpi5h0eVaLA1JCLkOUwsLOHp7AgB8Rg9HQsh1pX9Ma6qrkRASBg9OvCXSmDo7NVeUliH2cgh6DR+i67B0hgkLkYEpUiQsmj3abGlro1b9xOsRqKmuhlvfPjCSydBz2GCEnTpbr15cSCjcOfGWSGPq7tQcGXQeXoMGwEjWPteMZcJCZGBK8gtQXVWl0eJxDl6eWP3X7+g7cazK51SUliItOhZ+40dj/sf/hZmVJcJOnqlXL/rCZcg7d4JjD68Wx0dE6u/UfCPoAkwtLNrt0gJMWIgMjBACRdmaLc8/6Zn5MDKWYczjD6l1XkLIdfiOHgEHTw98+X8vI/VGdL06scEhqCwrR692fC+dqC1QdafmOikRN3Fx/yFUVVToMCrdaZ/jQkTUJE0Wj+vq6Y7+d03A9RNn0GfsSLj19UVyxE1UV1Y2e+7Jb79HWnQs/v7p10Z3cq6qqED0xWD0Gj4YJ7/9vkUxEpFqOzXfTgiB7197q0V9mZibo6K0tEXnagtHWIgMUGFOyxePm/TMfOSnZ2DHy68jJyUVD69fjXXnjsDO2bHZczNi43H6uz2NJit1IoPOo3vAAK7HQqQBC1ubZndq1pZZK1/E4q8+aZW+GsOEhcgAtXS12y4ebvCfMhHHvvgWlWXlOLfnF3TxcENZcTFyU9K0Ft+NcxdgbGaK7gP7aa1Noo7G0tYGxbl5rdKXc88eyIpPbJW+GsOEhcgAFWZlQ26vfsJy97IlyEvPwIW9vwMAzv/8GyrLyhF9MVir8aVFxSA/IxM9h3IeC1FLWdnZtUrCIpUZwcHLAyk3onTeV5NxqFN50aJFuHr1KvLz85Gfn49z585hypQpjdafOXMmDh8+jIyMDEX9yZMn16u3dOlSREREoKSkBAkJCfjggw9gyqFiohYrzM6FVSf1EhavQQPRd8IYHPxoq2K+Skl+AXa8/DoOb/1S6zHeCPoHPYczYSFqKUs7GxS1QsLS1dMDMhMTpLanhCUpKQnLly9HYGAgAgMD8ddff2Hfvn3w9fVtsP7o0aNx5MgRTJs2DQEBATh+/Dj2798Pf39/RZ2HH34Y77zzDt588034+PjgySefxJw5c7BhwwaN3hhRR1aYnQNTC3OYmJurVF8ilWLGy0sRdzUUwYeOKL12/cQZpEfHaj3GG0Hn4dK7Z6svcEdkCEwtLSAzMdFohEUikahUz7mXNwDofYQFAIQmJTs7WyxYsEDl+teuXROrVq1S/Pzxxx+Lo0ePKtV5//33xalTp9SKQy6XCyGEkMvlGr0fFhZDKD2GBIqNoUGiczcXleoPmz1TbAwNEm59fVstRgsba/HupZNi3PxH9H69WFjaW+nczUVsDA0SPYYEtuj8YQ/OFEu//1KlutP/839i5R+/6Oy9qPr93eI5LFKpFHPmzIGlpSWCgoJUOkcikUAulyMnJ0dx7MyZMwgICMCgQYMAAJ6enpg2bRoOHDjQZFsmJiaQy+VKhYhqFWRlA1Btef7BM+/BrNdexLkf9iIhNEzXoSmU5Bcg+NARDJ9zv2L/ISJSjaWdDQCgKDe3RefnJKfAzc8X3Xyb34jUuZc3Um7cbFE/2qT2vxJ+fn4oLCxEeXk5tm3bhpkzZyI8vOHdW+/04osvwtLSEj/88IPi2J49e7Bq1SqcOXMGFRUViImJwfHjx/Huu+822daKFStQUFCgKMnJyeq+FSKDpViev5mJt6MemY05b72GoB9/xS/r3m+N0JSc2fUjOrk4oc/Yka3eN1F7ZmlXu/VGcW7LHmu+8fc/KMjMwoCpk5qt69TTGymR+r8dpHbCEhkZCX9/fwwdOhRbt27F9u3b4ePj0+x5c+fOxZo1azBnzhxkZmYqjo8ZMwYrV67EkiVLMHDgQMycORPTp0/H66+/3mR7GzZsgLW1taK4uLio+1aIDFZJfgGqK6vQxcO90ToyU1PcvWwJzu7+Gb+sex+ipqYVI6yVFBaJuCuhGPnwg63eN1F7ZnVrhKWlc1hETQ1uBP0Dr0EDmu6nsx2s7TsjJVL/IyyAhveejhw5IrZt29ZkndmzZ4vi4mIxbdq0eq+dOnVK/Pe//1U69sgjj4ji4mIhkUi0fg+MhaWjlNlvvibWBR0Vdk6ODb7ec9ggsTE0SDh6d9drnP5TJraJOFhY2lMZ+8QjYu3Zwxq1MWTWPeK9K2eEmZVlo3V6DhtcOx/OtZvO3ovO57DUkUgkTT6CPHfuXHzzzTd4+OGHcfDgwXqvW1hYoOaO/7Orrq6GRCJReQYzEdX323ubUFpYiDlvrWzwd6nnsCHIz8hEWlSMHqL7V8jR4yjMzsHAu+/SaxxE7YmVnS2K8zRb5Tb6YjCkRkbwGND4Ao7OvXqgrLgYOUn6n3ah1l5C69atw6FDh5CYmAi5XI65c+di7NixirVY1q9fDxcXF8ybNw9AbbLy7bffYunSpfj777/h4OAAACgtLUVBQe3+B/v378d//vMfBAcH4/z58/D29sbbb7+N3377rV4iQ0SqKysqxp431mPR55sxf9O7qCgrQ0FmFn57bzMAoNfwwbgRdEHPUQI1VdWIuXQFHgPa5w6yRPpgaWfb4gm3dbISkpCfkQmvAH/cOB2EUQCcAKQCOA2gBrUTbtNuxkAIoXnQGlIrYXFwcMCOHTvg5OSE/Px8hISEYMqUKTh69CgAwMnJCW5ubor6zzzzDIyNjfHpp5/i008/VRz/5ptvMH/+fADA2rVrIYTA2rVr4eLigszMTOzfvx8rV67Uxvsj6tBu/v0Pfv9gC/pNGo+qigoMmDoJN89fQlJYBJx79cBfX32n7xAB1O7gfPfSxTCSyVBdVaXvcIjaPEs72xZPuL1dzKUruE8IbAXgetvxRADLAHgO7I+wE2c07kdb9H4vThuFc1hYWJovS77+VCzb/ZUIuGeq2BgaJKw62ek9JgDC1c+3dh2Yfn30HgsLS3soz333PzHnrZUat7N8+GBRI4GoBoS4rVTfKvs+3CA8B/TT6XtptTksRNR+/PnpF3Dt44O7lixEUlgkinJy9R0SACA5IhIVpWXw8OdtIWo/eg4bhPmb3tFL31Z2dhrfEpICeO78JUDUf2S47ufxazYgMThEo360hQkLUQcS/c9lRP1zGZ27OeNG0Hl9h6NQU1WNhGth8PTn7s3Ufox46AH4jR8Dc+vWX7i0dqdmzW4JjQLgXF2Nxh5vkQKwyi9AMoCZGvWkHUxYiDqYPz/9AgAQduqcniNRFncltMmnFYjaElMLC/QaPgQAYO/m2kxt7TKSyWBuLUexhiMsTirWswfwE/SftDBhIepgYi4G480J9yL28lV9h6IkLjgE1vad0ambs8ZtmZiboXM3LiZJuuMzejiMby3p0cWjdRMWSztbAEBRTp5G7aSqWK8uUfgI+k0amLAQdUAFGZnNV2plcVevAYBWbgtNfHo+Xt2/G4Pvm65xW0QN6TdpHBKuhaEgKxtdWnmEpW4foeK8PI3aOY3ap4FUWUBECsANtbeR9IUJCxG1CaUFBUiLitHKbSH3/n6oKC3DnLdX4q5nn9JCdET/MjE3Q++RwxBy5Dgy4xNg7966CYvVrX2Eilq4LH+dGgBLb/uzKlS9jaQLTFiIqM1ICouEc09vjdqQSCTo5tMLf335LQ5u2obJixbAk3NjSIt6jRgKUwtzhBw5gaz4JHRp5YTF0vbWCIuGCQsA7AXwAIAsFeurehtJF9RaOI6ISJcKsrI0XvG2s1s3mFlZIvF6BKLOX0R5STEyExK1FCER0GfsKKRE3kR2YhKyEhLRb9K4Vu3fspMdqioqUF5copX29gLYDyAZtRNsGxrJqAGQhNrbSPrCERYiajMKs3Ng1clOozZc+9TuHp8UFgkhBM7s+glF2W1jvRkyDHbOjki9GQ0AyIxLgLncSuPPrTqs7Gw1vh10pyoAi279+c7bQ3U/L2vgtdbEhIWI2oyinFyYWVrC2KzxDVWb0823F7KTklF6a78yIm2zsLFGSX7t5yszIQlA6z7aXLssf57W2627PXTnNodJt47v1XqP6mHCQkRtRlF2DgBo9H+r3Xx7I/F6RKOvS6RS7gRPGrGwsUZpQSEAIDuxNmFpzUebdZWwALVJiQeAsQAeuvVfT+g/WQGYsBBRG1J469aNvHOnFp1fN+E2KazhhMW6axe8e/EkHnjj1RbHSGRh/e8IS2VZOXJT01p3hMXWRuu3hG5XA+AkgN23/qvP20C3Y8JCRG3GvyMsLUtY7N1dYWZliaSwyAZfL8jIhJGxDD2HD25xjNSxyUxMYGJupkhYALT6k0JWnex0NsLSljFhIaI2ozgvHzU1NZB3btktoW6+vQGg0REWANjzxnrYOjpoNE+GOq66fYNuT1gyExJbNWGx1MGk2/aACQsRtRk11dUoyctv8QiLa5/eyEpMUswvaEjqjShIpVI4enVvaZjUgVnYWAOA0mcsMz4BnV27tUr/Eqm0duNDDZflb4+YsBBRm1KUkwurFoywyExN0WvEUCQ1MeEWANKiY1BTUwMnDReoo47Jom6EpUD5lpCphTmsu3bRef+2Dl1hJJMhJ0WfS7jpBxMWImpTCrNzIG/BU0IPrn4VnZyd8NeXO5qsV1lWjqz4RDj19GppiNSB1Y2w3H5LKD06FgDg0quHzvvv5FK7OH5OcorO+2prmLAQUZtSO8Ki3i2hsfMeRuA9U7Fn1VokR9xotn7KjSiNtwCgjkmRsNw2wpKdlIyclNRWmcxdt5t5bkqazvtqa5iwEFGbos5qtzJTU9y3/AXc89JzOPr5dlz585hK56XeiOItIWoRcxtrlBUXo6aqWul45Lnz6D1iqM777+TijPz0TFRVVOi8r7aGCQsRtSlFObkqrcNi3cUeL+z5GkPvn4G9Gz7AHx9/pnIfqTeiYGlr0ypzDsiw3L4Gy+0iz55HV0932Dk56rT/zt2cO+TtIIAJCxG1MUXZObCwtYHUyKjJeiMeegA2Xezx4ZwncGbXjxBCqNxHyo0oAIAz57GQmixsrFGaX/8ptKgLl1BTXY1eI4botP9OLs7IZsJCRKR/RTm5kN56dLMxEqkUgfdMQfAfR5EeE6d2H7kpaSgrKoZzK0ySJMNiYS1Xmr9Sp7SgEAmhYeg1XNcJixNykjveE0IAExYiamMK61a7beLRZu/BAbB1dMA/+w60uB/OY6GWMG/klhAARJ79Gz2GBDY6OigzMdGob5mpKWy6duEtISKitqAop3Y/oaYWjwu8dyoyYuOREHK9xf2k3IiCUw/eEiL1WNhYNzjCAgAR587D3FoONz/feq91cnHCW6f/wIxXlkEibdlXbyfn2vkxOUlMWIiI9K4uYWlseX5TCwv0nTAWF387pFE/J7d/j+9eXa1RG9Tx1M5haThhSbwWjuK8fPiNH13vtcGz7oFUKsXIhx/A4++vhcxU/a0h/l2DhbeEiIj0rqK0DOUlJY2OsPSbPA7GZqa4tF+zhCU7KRlpN6M1aoM6HnNrOUoa2fpB1NTg0u9/YNB9d8PI2FhxXGpkhMEzpuPCr7/jm2XL0XvkMExetEDtvju5OKO6sgp56Rktjr89Y8JCRG1OYXZOoyMsA+++C9EXLnfYf7RJfyQSSW3C0sgICwAE/bAXVp3s0G/SOMUxIQR+fPMdnN75A66fOINrf52E96CBavffycUZuWlpEDU1LYq/vWPCQkRtihSA45m/MTkpFWOg/I+UvHMneA8aiOBDh/UUHXVkZnIrSKXSJhOWjNh4RF24hGGz71McEzU1CD99DplxCQCAuKvX4OLbS+1JuB35CSGACQsRtSEzAcQBeH79Rjz/0684cevnmbde73/XBNTU1CDk6Am9xEcdm4V1/Z2aG3J2zy/o4uYK81v17xR3JQQyY2N08+2tVv+dujl32Am3ABMWImojZgL4CYDLHcddbh2fCWDAtEmIPPN3s18YRLpgXrdTcxMjLABw7dhJvD35PpQ28jRR6o1olJeUwqO/n1r9d3Zx5ggLEZE+SQFsuu3Pd74GAJuNpPD080XwoSOtFxjRbRraqbkhNdXV9fYauvP1xGthcFcjYTGzsoSFjXWHXYMFYMJCRG3AKACuaPwfJCmAbtU16HruAq6fOK21fn1Gj8C6oKMwtbDQWptkuFRNWFQRd/UaPPz7qly/k0vtLs0ddVl+gAkLEbUBTirWyzt9DhWlZVrrtygnF2ZWlrB376a1NslwmVvLUV1ZhYrSUo3bir96DdZd7GHnrNpmiZ1da2+Wcg4LEZEeqXpX/mpsvFb7zYyvfWqjq4e7Vtslw9TUKrfqirl8Bd+8sAIleaq1Z+/qgrKiYsXCih2RTN8BEBGdBpCI2gm2Df1fVA2AIseu+DE6Vqv9lhUWoTA7B13cXbXaLhkmC5vG9xFSV1lhEULVeNqts1s3ZCUkaaXv9oojLESkdzUAlt725ztfkwD4bf6jyMvM0nrfmXEJ6OLh1mQdUwsLPPbe2xg3/xFYd7HXegzUPljYWOvtCTV7127ISmTCQkSkd3sBPAAg+Y7jKVIpdix/Ab/KdDMgrErCMuT+e9F34ljcteQprDryKwbNmKaTWKhts2hmlVtd6uzqwhEWfQdARFRnLwAPAGMBLJBb4YevPsGjc2Yh65HZiL5wSSd9ZsYnoIt74wmL1MgIox6djcsHDmPN+Om4ef4ihs2epZNYqG0z1+ItIXXITExg6+iAbCYsRERtRw2AkwC+LixCcNcuGPPk4wCAqIuXddJfRlwCzKwsIbfv3ODr/SaNQydnJ5z89nuUFRbh0v4/4N6vT6P1yXBZWOsnYenk4gSpVMpbQupUXrRoEa5evYr8/Hzk5+fj3LlzmDJlSqP1Z86cicOHDyMjI0NRf/LkyfXq2djYYMuWLUhJSUFpaSnCwsIwdepU9d8NERmU2MtXYePQBak3o1GUrZunI+r2d2nsttDY+Y8g8tx5pN6IAgCEnz6Hmupq+I4ZoZN4qO2ysLZudPVaXbJ3q50UnpV45w3TjkWthCUpKQnLly9HYGAgAgMD8ddff2Hfvn3w9fVtsP7o0aNx5MgRTJs2DQEBATh+/Dj2798Pf39/RR1jY2McOXIEHh4eeOCBB9CrVy889dRTSE7u2H8xRFT76CcARP+jm9EVAMhJTkVNTQ3su925KQDgObA/XH174+T27xXHSvILEBscgj5jR+ksJmp7JBIJLO1sUKzlEZantn6Iic/Mb7JOZ1cXVJSWoVAHk87bE7Vmsf3+++9KP7/++utYvHgxhg4dirCwsHr1X3jhBaWfV65ciRkzZuCee+7BlStXAAALFixAp06dMHz4cFRVVQEAEhIS1AmLiAxU9D+XUVNdjYiz53XWR1VFBfLTMxQLc93Oc0B/lBQUIPKccv/Xj5/G1OeegYm5mVYXsqO2y9bJATITE61PfDUylsGph1eTdezduiE7KRlCCK323d60eA6LVCrFnDlzYGlpiaCgIJXOkUgkkMvlyMnJURy79957ERQUhE8++QRpaWkIDQ3FihUrIJU2HZqJiQnkcrlSISLDkpOcig13P4jwU2d12k92YjLs3eqvdtvF3RVZ8fW/oK6fOA1jM1P0GDpIp3FR21G3uGBmnHYXL8xKSEIXt6bXAbJ35RosQAsSFj8/PxQWFqK8vBzbtm3DzJkzER4ertK5L774IiwtLfHDDz8ojnXv3h0PPPAAjIyMMG3aNKxduxYvvvgiVq5c2WRbK1asQEFBgaLwFhKRYWqN3WmzEpIaHGGxd+uGrITEBuunx8Sh38RxOo+N2oYuHm6oLC9Hbmq6VtvNik9EZ7f6n73b8ZHmWmonLJGRkfD398fQoUOxdetWbN++HT4+Ps2eN3fuXKxZswZz5sxBZmbmvwFIpcjIyMDTTz+Ny5cvY8+ePVi3bh0WL17cZHsbNmyAtbW1ori4NP0XTkTUmOykZNi71h9hsXd3RWZ8/YQFAC7s/R2B907Fg6uXw9jMVOsxyTt3wujH5sJ3zEitt03q6+rpjqyEJIiaO5c21ExmQiLMLC0h79ypwdelMiN0cnZCdgefcAu0YGn+yspKREdHAwAuXbqEQYMGYenSpVi0aFGj58yePRtffvklHnzwQRw7dkzptdTUVFRWVqLmtg9BeHg4nJycYGxsjMrKygbbrKioQEVFhbrhExHVk5WQBHNrudLS66aWFrC279zgCAsAnPhmJ0ry8jHztRfh6ueDzY88hSot/JvUqZszpr/wLPzGj4aRTIbspBSEnTyjcbukmS4ebsjQ8l5WABQjJ/burijMzqn3up2jI4yMZR3+kWZAC+uwSCQSmJo2/n8Xc+fOxTfffIOHH34YBw8erPf62bNn4e3tDYlEojjWs2dPpKSkNJqsEBFpk+JL47Z5LHV/zmxgDkudC7/+jq1PPguX3j3hM2qYxnFMfPoJvLJ3F9z6+uLXdz7EzhVr0LmbM+y515HedfVwUzwCr025CUlwOX8Rj0ilGIP6X8p1n8PGEueORqha1q1bJ0aOHCnc3d2Fn5+fWLt2raiqqhITJ04UAMT69evF9u3bFfXnzp0rKioqxOLFi4WDg4OiWFtbK+p069ZNFBQUiM2bN4sePXqIadOmibS0NPHaa6+pHBcAIZfLhRBCyOVytc5jYWFhMbWwEBtDg8SAaZMVx/zvmiA2hgYJc+vm/01ZtudrMe+D9RrF0GPoILExNEhM/8//CRNzMwFAmJibiXcvnRQjH35Q79eoIxQpIMYAYu6t/0pvHTcxNxMbQ4NEwD1TtdrfTEAkAELcVhJuHa+rM3zOLPHu5VNCIpXq/froqqj6/a3WCIuDgwN27NiByMhIHDt2DEOGDMGUKVNw9OhRAICTkxPc3P5dfOmZZ56BsbExPv30U6SlpSnKpk2bFHWSkpIwefJkDBo0CCEhIdi8eTM2bdqEd955R53QiIharLykBIXZOcojLO6uKM7LV2mzu8sH/oTvmBEwk1u1OIZew4cgPyMTv3+wRfGodEVpGWIuX0XvkUNb3C6pZiaAOAAnAHx/679xt47Xbd2gzSeEZgL4CbU7lN/O5dbxmbd+9hzQD+lRsVqfO9MeqTWHZeHChU2+Pn++8uI348apNoP+77//xrBhmg+nEhG1VHZiMjrftnicvZsrshqZcHunK38cwz0vPod+E8fhwt79Leq/1/DBuBF0od7xyDN/465nn4LMxEQrc2Sovrrk4U51ycMrt6Y9ZGjplpAUwKbb/nznazUAPgJwwMQEvmNH4vhX32ml3/aOewkREaF2HsvtIyxd3F2RqeK8gYKMTERfuIyB0+pvPaIKuX1nOPfqgchz9ROWiHPnYWJuhu4B/VvUNjWtueRBCuC/V0Mx4P3NqCgs0kqfowC4NtDf7f26Adjs7AjvsEiE/nmskZodCxMWIiIA2YnKa7HUrsGi+pMZlw/8Ca/BA2HdtYvaffe8tQBdQyMsaTejkZ+eiV4jeFtIF5pLHnDrtXHbv0c6/r1VowknFes9E5eA2QuexT/xiVrpt71jwkJEBCArMQnW9p1hamEBM7kVrDrZqXxLCABCjp0AhGjR00I9hw9GYlgEinPzGnw98tx59Bo+RO12qXmqJg8A0BnK80taSt2lEO+c19JRMWEhIsK/O+F26uasWCq9sUXjGlJWWITUG9Hw6N9XrX4lEgl6DR+CGw3cDqoTefZvOPXwgty+s1ptU/PUSR7qFt/4CJp9eZ4GkIjauSqqqOtL037bu4783omIFLLr1mJxdVGse6Lu2hfxIdfg1q+PWuc49fSGvHOnehss3i7qYu1u1d6DBqrVNjWvJcmDG2pvJbVUDYClt/25tfpt75iwEBEBKM7LR1FOLqb839MIuGcKCrNzUFZUrFYbcVevwdHLE+bWqm/G6j0kABWlZYi7EtponaLsXKRFx8KLCYvW3Z48qEOdW0kN2QvgAQDqLrivab/tGRMWIqJbvnzuZeSmpcNn5LAWLcMeH3INAODm56vyOU7eXkiLikF1Myt7R/9zmSMsOrIXwHsz7kaNVPWvRG1sybkXQHcAGahdGa21+m2vmLAQEd2SEHIdXyz+D969dy6+X/mW2udnxSeiOC8f7mrcFura3R3pMXHN1ou6cAldPNxa9BSSvrn6+aJzA5tLthVSmREqX34e706ZoFhWtTE1ABJQeytJG0YA6Ip/58e0Vr/tERMWIqI7ZMTGIzclrUXnxodcg7saE28dunsiPSa22XrRF4NRUVoGRy+PFsWlL0bGxnh624dYuvNzOPX00nc4Derm2xsWNtbYlpiM+wFkN1Kvbr7JMqg+96Q5qt7ikWi53/aICQsRkRbFX70Gt36+Shu6Nsa6iz3M5VbIUGGEpTg3D68Pn4QbQf9oIcrW02fsSFjYWKM4Lx+LPv8YDl6e+g6pHq8Af5SXlCApPBJ7ATgAWIX6iUsSaued7NVi36re4nlDy/22R0xYiIi0KD7kOiysrdHFw63ZunVf3mnRcSq1XV1VpUloehF47zTEh1zHx489jYLMLDz23tv6Dqme7gEDEHclFDVV1QBqRzHWovZWzVgAD936rye0nzQ095RS3a2g9Vrutz1iwkJEpEUJoddRU1MD9/5+zdZ16O6ByvJy5CSnaNSnubU15q5dhTeO/QZjM1ON2tImq8526D1yKC7+dhAl+QU48tnXcOrhBRuHtjMPRyKVwnNAP8RculLvtRoAJwHsvvVfXdyOaeoRZ13cgmrPmLAQEWlReXEJUm9EYdgD98HE3LzJug7dPZAZn6jRTrxegQPwyr5dGDBtEmy6dlHawFHfBk67C6KmBsGHjgKonTgMAN6DA/UZlhKnHl4wt5Y3mLC0lsYecdbFLaj2jAkLEZGW/fTWu3Dw8sSTn7wPE3OzRus5eHkiPbr5CbeNkZma4qH1byArPhGbH1kIAOjczbnF7Wlb4L1Tcf3EGZQWFAAASvILkBx+Az2GBOg5sn91D/BHVUUFEkLD9BrHXgAe0P0tqPaMCQsRkZYlhIbh80UvoJtvLzzy7puN1uvq6a7ShNvGjHrkQVjb22PPG+uQHH4DlWXl6OTSNhIW78EBcOndExd+/V3p+M0LF+E9uPGEZficWXhyy/uQmZjoOkQAtQlLfOh1VFVUtEp/TWmNW1DtGRMWIiIdiLsail/WbYTfuNHo1MCoh6WtDeSdOyGtBQmLW19fPLTuDUxYOA9n9/ys2FU6JyW1wb704e5lSxB/9RoiTgcpHY86fwl2To6wd6u/LsuAqZNw/+svw3fMCNzz0nOtEmf3AH+93g4i1TFhISLSkdCjx1FeUoKB0ybXe61rdw8AaNEIi7GpKQLvnQoIgaOffa04vuPlVfjryx0tDVdr+k0aB7e+vjjw0af1Xou5dAXVVVXwHqI8j8V7cADmrluFf/YdxC/rN2LkQw+gzzjd7pzT1dMd8s6dEHPxik77Ie1gwkJEpCMVpWUIPXYSA+++q95rDl6eqK6qUmtH6DrRF4Nxcf8h/PruRyjOy1ccT70RhcKsxpY9ax1SIyNMfe4ZhJ8JQvTF4Hqvl5eUIPFaOHrcdltIamSEOW+tROylq/hhzXqc/f4nhB47iblvvw6X3j11Fqvv6BGoLC9vch8najuYsBAR6dDlA4fh0N0DLj7KX7wOnh7ITkxudg+hxnz/2lu4+NtBbYSoVb5jRqKrpzsOfrS10Tp181jqFtfrP3k8Ork4Yd97mxRroex5Yz2iLlxCfkamzmLtP2UCwk6eRUVpqc76IO1hwkJEpEM3//4Hhdk5SqMsEokEXoMGIC0qRo+R6YaLT0/kZ2QiJfJmo3XCTp6FVSc7jHjoAQDA2CceQeS580i9EaWoU1pQgO3/eQ1FObk6ibOzaze4+fniyp/HdNI+aR8TFiIiHaqprsaVP45iwNRJkBoZAQAG3n0XXHr3xKnv9ug5Ou1zVOFR7YSQ6zi5Yzfueek5jH/ycXTz7YUT3+xspQhr+U+ZgPKSEoSfOtuq/VLLMWEhItKxC3t/h1UnOzy0/g2YWlrg7mVLcOXPY4i9fFXfoWmdo3d3pKmwtsyBDz5Bclgk7l62GMnhN1p9j6QBUyfh+vHTqCwrb9V+qeWYsBAR6VhK5E1898ob6D95PF76+TtY2FrjwIef6DssrTMyNkZnVxeVFsOrrqrCty+9jozYePz56eetEN2/HLw84dTDS7ECL7UPTFiIiFpByJHj+O7V1bBx6IKT279HTrKq+/SqZ8HH72HWypd00nZzunq6wUgmQ1qUaqv35qWl49175+L6iTMq1e/s2g1+40drEiKA2km+pQWFiDx3XuO2qPXI9B0AEVFHEXL4L6y7GoqCjCyd9VFdWYku7q46a78pjl7dAQBp0bqZTNx3/GhMXrIQK4dOgBBC6TXrLvYozs1TaUfrXsOHIPLc+RY/oUX6wREWIqJWlJ+eWe/LVptyklP1tjy/g7cn8tMzUVZYpJP2U6NiYGphDjtnR6XjUiMjvPDDN5i/+d1m2zC1sICrnw9u3tqIkdoPJixERAYkOykZtk4OkEhb/593h+6eSI9p+WaOzUmLigYAOHp7KR33HNgf1vad4TNqOEY+/CCA2iTGuot9vTY8A/rDSCZD1PmLOouTdIMJCxGRAclJToHM2Bg2Xbu0et+OXp4qz19pifz0TJQWFMLRu7vScb9xo5GXnoHTO3/A9P88i/FPPo5X9+/Gyj9/QVdPd6W6PQYHIi8tXbH/ErUfTFiIiAxI3WTezq28CaLMxAT2bt10Nn+lTlp0LJx6KCcsvmNG4Prx0/j9g0+QGZ+oeFQ6Pz0D976yFEDtl90YAA9WVkL2/c/88muHOOmWiMiAZCeloLqyCg5eng3u5aMrXTzcIDUyQroOR1gAIC0qBu79+igd2/zoUzAyNkZVRQW2Pfl/MJPLkZ2YBL/xYzB/0zt4rndPvBxxA64AsHkbAGA+gKUA9uo0WtImJplERAakurISaVEx6Obbu1X7rbtNk6bDOSxA7TyWrp7uilWDAaA4Nw8Ft/YcKs7LR3Zi7e2ea3+dhNXWL7Ep4gZc7mjHBcBPAGbqNFrSJiYsREQGJiksAt18e7Vqn+79+uj0CaE6qTdjFLefmiMFMP1/Xyv+fOdrAPBRA69R28S/JyIiA5MYFgFHr+6QmZjovC8LG2s88u6bGPXIbAT/cUTn/dWtonvnxNuGjALgXFUNSSOvSwG43apHbR8TFiIiA5MUFgkjYxmcenrrvK/FX32C3iOG4rtXV2P/+x/rvL+inFwUZueolLA4qdimqvVIv5iwEBEZmNQbUaiurNL5bSGnnt5w7umN715djeCDh3Xa1+3SomJUSlhU3fxAN5skkLYxYSEiMjBVFRVIi46Bawsm3nZyccLqv/ajs2vzc0T6TRqHkoKCVl+ErSgnF/0nj4d9M1sQnAaQCKCmkddrACTcqkdtHxMWIiIDlBQW2aInhdz7+cG6iz0G3j1ZccytXx94Duxfr27fCWMQduKsSvv3aNOVP46ipKAAxbn5TdarQe2jy3V/vvM1AFjWwGvUNnEdFiIiABYWFrC3t4dE0tgUzfalIi0T3adPg1ePHqiqqFD5PN8B/jCvAUZMvQs3Dh2DRCLB0++tg7FMhotPPY/O1dXIBBDj4oTuXl64snsv3N3dm21Xmwqj4/HFw0+jq50dYGfXZN3LAJ4B8BqU56qkANhw6/XWjb5jEUIgKysLJSUlGrclAaC7XbhakVwuR0FBAaytrVFYWKjvcIionZBIJJg/fz7Gjh2r71C0SmZiDLl9ZxRmZqNKjV2JrexsYWxmCkgkKMjIgtTICHYWFjArKICkulpRr1oiQbmtDbLy8nW6maM2mQEwAlANoEzPsXQ0J06cwNdff93gZ0XV72+1RlgWLVqExYsXw8PDAwBw/fp1vPXWW/jjjz8arD9z5kwsXrwY/v7+MDU1xfXr17FmzRocPtzw5Kw5c+Zg9+7d+PXXXzFzJpfzISLdmz9/PsaMGYM9e/YgIiICVa18e0NnJBI4dPdAYVY2SvILVD7N3q0bKsvKYWplheLcPNgAsMvNbbR+IgDVW6eORiaToXfv3pg9ezYA4Kuvvmp5W+pUTkpKwvLlyxEVFQUAmDdvHvbt24cBAwYgLCysXv3Ro0fjyJEjeO2115CXl4f58+dj//79GDJkCK5cuaJU183NDe+//z5OnTrV4jdDRKQOS0tLjB07Fnv27MGBAwf0HY7WlUoEJFIpMpOSUHPb6EhTqizMkJ+eARNzc5hamMM0Nh7pzSRx8doIlgxWdHTtLtt1gxItvT2kVsLy+++/K/38+uuvY/HixRg6dGiDCcsLL7yg9PPKlSsxY8YM3HPPPUoJi1Qqxc6dO7F69WqMGjUKtra26oRFRNQinTt3BgBEREToORLdyElORRcPN3TxcENmXEKzSYvMxBgSiQSV5eWorqyErUwGaTPJigkAKwC6Xd+W2ru63zF7e3skJCS0qI0WPyUklUoxZ84cWFpaIigoSKVzJBIJ5HI5cnJylI6/8cYbyMzMVGuoyMTEBHK5XKkQEamjboKtwdwGukNVRQUy4xIgNTJq9hFgAJCZmAIAKssrUFZUDImKk3WNNYqSOoK63zFNJrWr/ZSQn58fgoKCYGZmhqKiIsycORPh4eEqnfviiy/C0tISP/zwg+LY8OHD8eSTT8Lf31+tOFasWIE1a9aodQ4RUUdTVVGB7MQkdPX0gIWNdZPzWYxNTVBTU4OaW18uWRmZsFWhD9Wn9BK1nNojLJGRkfD398fQoUOxdetWbN++HT4+Ps2eN3fuXKxZswZz5sxBZmbtrppWVlb47rvv8NRTTyE7O1utODZs2ABra2tFcXG5cy9OIiICgIrSMpQWFsK6i32T9WSmpqgqL1f87D9yJC4KASsbm8bbBm8HtTXHjx/Hhx9+qLX23N3dIYRA//711+JpTWonLJWVlYiOjsalS5fw2muv4erVq1i6dGmT58yePRtffvklZs+ejWPHjimOe3l5wdPTE/v370dlZSUqKyvx+OOP495770VlZSW6d2986eWKigoUFhYqFSKijqDu8VAhBCoqKpCWlobDhw9j/vz5jQ6552dkQWZiDEs720bbNTY1QWW56mu2ALVPCbUlsbGxzX4naUIIgRkzZuisfXWMGTMGQgjY3JFQzpo1C6tWrdJTVLqj8Uq3EokEpqamjb4+d+5cfPPNN3j44Ydx8OBBpdciIiLg5+cHf39/Rfntt99w/Phx+Pv7IzGxrf0qEBG1DYcOHYKjoyM8PDwwdepUHD9+HJs2bcLvv/8OIyOjevWrystRkl8A6y6dgUaSGpmpidIIS51Y1I6k3K4CQDSAPE3fSBshk2lvHVVtttUSubm5KCoyzHEvoWpZt26dGDlypHB3dxd+fn5i7dq1oqqqSkycOFEAEOvXrxfbt29X1J87d66oqKgQixcvFg4ODopibW3daB9ff/212Lt3r8ox1RW5XC6EEEIul6t9LgsLS8cs7u7u4ttvvxXu7u56j0Wd0ti/k+PGjRNCCPHkk08qjr3wwgsiJCREFBUViYSEBLH9+13C3slR8bqbm5v47bffRE5OjiguLhbXw8LE1KlTBQAxZswYIYQQ48ePF//8848oLi4W58+eFQN69lTqd/r06eLixYuitLRUREdHizfeeEMYGRkpXl+9erWIj48XZWVlIjk5WWzatKnR99avXz/x119/iYKCApGfny8uXrwoAgICFK8PGzZMnDx5UpSUlIiEhASxadMmYWFhIQCI48ePizs11o8QQjzzzDPi119/FUVFRWLNmjXNvpfY2FiltmNjYxXvLzg4WMyfP19ER0eL6upqAUC4urqKX3/9VRQWFor8/HyxZ88e0bVrV0UM3bt3F7/++qtIS0sThYWF4sKFC2LChAlKcZqYmIh3331XJCQkiLKyMnHjxg2xYMEC4e7uXu+9fv3114rr8OGHHwqg9ns5KCio3vu/evWq4j0DEE888YQICwsTpaWlIjw8XCxevFjp90QIIfr37y8AiJs3b4oXX3xRqb0+ffqI6upq0b17d7V/19T4/lb9l+SLL74QsbGxoqysTKSnp4sjR44okpW6X6Ljx48rfm7ow3P7RVXnF7G5woSFhYVF3WJoCQsAERwcLA4cOKD4eenSpWLs2LHCw8NDjBs3TtyIuim+/Pbf/7Hcv3+/+PPPP8XAwYPE8MkTxb33zRCjRo0SwL8JS1BQkBg9erTw8fERJ0+eFGfOnFGcP3nyZJGXlycef/xx4enpKSZOnChiYmLEG2+8IQCI+++/X+Tl5YkpU6YIV1dXMWjQILFw4cJG31toaKj49ttvRa9evYS3t7d44IEHRL9+/QQA4efnJwoKCsTSpUuFt7e3GDZsmLh06ZL46quvBABhZ2cnEhISxOuvv674H+TG+hFCiLS0NDF//nzh6ekp3Nzcmn0v9vb2Qggh5s2bJxwcHIS9vb0AahOWwsJCcejQIeHv7y/69u0rAIhLly6JU6dOiYEDB4rBgweLixcvKn1H9uvXTzz99NPCz89PeHt7i7fffluUlJQIV1dXRZ3du3eL+Ph4cd999wlPT08xfvx4MXv2bCGVSsXMmTOFEEL06NFDaTDg9oSlT58+QgihlEj4+voqzgMgFi5cKJKTk8XMmTOFh4eHmDlzpsjKyhKPP/644vfk9oRlxYoV4tq1a0rXc+PGjeLEiRMt+l3TScLSlgsTFhYWFnVLk/+I2ncWLj491S6WtjaN9tepm3OD58jtO6sVd1MJy/fffy+uX7/e6LmPPP64yM7JEUbGMgHU/p/2G2+8Iaw6dxIuPr2U6t4+wlJ3bOrUqUIIIUxNTQUAcfLkSbF8+XLlPh55RCQnJwugdoQnIiJCyGQyld5bfn6+4ovyzrJ9+3axbds2pWMjRowQVVVVinhiY2PF0qVLm+1HCCE++OADpWPNvZe682bMmKFUZ/Xq1aK8vFyRwAAQEydOFJWVlaJbt26KYz4+PkIIIQIDAxuN69q1a+LZZ58VAESPHj2EEKLeqMudfz82NsqfudsTFgDiypUr4vXXX1f8vG7dOnH+/HnFz/Hx8WLu3LlKbaxcuVKcPXtW8Xtye8Li6OgoKisrxaBBgwQAIZPJRHp6eqN/b83+rqn4/c3ND4mIGjDswftw15KFap/3w+r1OP/L/gZfm/7Cs+g/eXy9439++gUOb/1S7b4aIpFIlPZrGTt2LF577TX4+vrC2toaMpkM5ubm6OLsjLT4BGzevBlbt27F9Bn34vjJk/ju628QGhqq1GZISIjiz6mpqQCArl27IjExEQEBARg0aBBWrlypqGNkZARzc3OYm5vjxx9/xLJlyxATE4M//vgDBw8exP79+1HdyCJ2H3zwAb744gs89thjOHr0KH788UfExMQAAAICAuDt7Y1HHnlE6f0aGRnB09NT7QUAL168qPRzc++ltLS00bbi4+ORlZWl+NnHxweJiYlISkpSHAsPD0dubi58fHxw8eJFWFhYYPXq1Zg+fTqcnZ0Vfzdubm4AAH9/f1RVVeHkyZNqva877dy5EwsWLMDatWsBAA899BA++ugjALULubm5ueHLL7/E559/rjhHJpMhP7/h3bDT0tJw4MABLFiwAP/88w+mT58OMzMz/PjjjxrF2RwmLEREDQj68VdcP3Fa7fPyUtMbfe33Dz/BsS+21ztekKnesg5N8fHxQWxsLIDaLU8OHjyIbdu2YdWqVcjJycHIkSPx1VdfwaZzJ6TFJ+DLL7/E+avBuHva3RjiPwAXL17Eiy++iC1btijarLxt88S6ZEgqlSr+u3r1avzyyy/1YikrK0NSUhJ69eqFSZMmYeLEifj000/x8ssvY8yYMQ0u2Pfmm29i165duPvuuzF16lS8+eabmDt3Ln799VdIpVJ89tln2Lx5c73zWrJ6anFxsdLPzb0Xddq6M3Fs6Ph7772Hu+66Cy+99BKioqJQWlqKn376CSYmJgDQZIKkjl27duGdd97BgAEDYG5uDldXV+zevRvAv3+PTz31FM6fP690XmNJJQB88cUX2LFjB1544QXMnz8fe/bs0Vq8jWHCQkTUgMKsbBRmaS+RAICcpBSttnencePGoV+/foo1OAIDAyGTyfDiiy8qviTrNqEzMjZGJxcnVJZXIK+kBFs2b8a7eflYv349nnrqKaWEpSmXL19Gr169FPvFNKSsrAz79+/H/v378cknnyAyMhJ9+/ZFcHBwg/Vv3ryJjz76CB999BF27dqF+fPn49dff8Xly5fRp0+fJvuqqKho8Ckpbb0XVdsPCwuDm5sbunXrphhl8fHxga2trWKx1VGjRuGbb77Br7/+CqB2b6u6zYUBIDQ0FFKpFGPGjFFaEuT2WAA0G09ycjJOnTqFRx55BObm5jh69CgyMjIAABkZGUhKSkL37t2xa9euZt9XnYMHD6K4uBiLFy/G1KlTMXr0aJXPbSkmLERE7ZCpqSkcHBxgZGQEBwcHTJkyBStWrMD+/fvx7bffAqjddM7Y2BjPPfcc9u/fjxEjRmDRokUAahMyYzMzvLvhHRw5egQXTp1BT8/uGD9+vMqrlwPAW2+9hd9//x2JiYn48ccfUVNTg379+qFv375YtWoV5s2bByMjI5w/fx4lJSV47LHHUFJSgvj4+lsmmpmZ4b333sNPP/2E2NhYdOvWDYMGDcLPP/8MAHj33Xfx999/Y8uWLfj8889RXFwMHx8fTJo0Cc8//zwAIC4uDqNHj8bu3btRXl6u1qKkzb2XuvYnTJiAs2fPory8HHl5eQ22dfToUYSEhGDnzp1YtmwZZDIZPv30U5w4cQKXLl0CAERFRWHWrFnYv38/hBB4++23FSMeQO1tpu3bt+Orr77C888/j6tXr8Ld3R1du3bFjz/+iPj4eNTU1GD69Ok4ePAgSktL64301Nm5cyfWrFkDExOTevv8rVmzBps3b0ZBQQEOHToEU1NTBAYGws7OrtEF6GpqavDNN99gw4YNiIqKwt9//63yddaEWpO92mrhpFsWFhZ1S3t+SqhORUWFSE9PF4cPHxZPPPGEkEgkSnWXLVsmkpOTRXFxsTh06JB49NFHlSZqfvzxx+LmzZuitLRUpKeni+3bt4tOnToJoOFJnf379xdCCKVrNnnyZHHmzBlRXFws8vLyxN9//614EmjGjBkiKChI5OXlicLCQnHu3DmlSby3F2NjY7Fr1y7FI9BJSUli8+bNigm1AERgYKD4888/RUFBgSgsLBRXrlwRK1asULw+ZMgQceXKFVFaWiqEaPqx5jsnzzb3XoDax55v3LghKioq6j3WfGdbzT3W7O7uLo4dOyaKi4tFfHy8WLJkSb0Js6ampmLjxo0iOTlZ8VjzE088oXj99ddfFykpKaK6urrBx5rrio2NjSgtLRVFRUXC0tKyXqwPPfSQuHz5sigrKxPZ2dnixIkT4r777lPEefuk27ri6ekphBDipZde0uh3jU8JsbCwsDRT2mvCwsLSFsrw4cNFRUWFUhLWWOFTQkRERNSqTExM4Orqirfffhs//PCDYj6Mrmm8ND8RERF1HA899BAiIyNhY2ODV155pdX6ZcJCREREKtu+fTtkMhkCAwORkqLbJ99ux4SFiIiI2jwmLERERNTmMWEhog6rpqYGQO2aJkSkO3W/Y02tntscPiVERB1WamoqysrKsGjRIsXTDpr8g0pEyoyMjNC1a1fMnj0bZWVlSEtLa3FbEtQ+39zuyeVyFBQUwNraGoWFhfoOh4jaiS5duuCpp55C79699R0KkcGKiIjA559/jszMzHqvqfr9zYSFiDo8iUQCGxsbWFtbQyKR6DscIoMhhEBBQQHy8/Mb3AwSUP37m7eEiKjDE0IgLy+v0X1hiEj/OOmWiIiI2jwmLERERNTmMWEhIiKiNs/g5rDI5XJ9h0BEREQqUvV722ASlro3nJycrOdIiIiISF1yubxjPNYMAM7Ozu3qkWa5XI7k5GS4uLi0q7h1hddDGa+HMl4PZbweyng9lLW36yGXy5vdSNFgRlgAtOqukdpUWFjYLj5QrYXXQxmvhzJeD2W8Hsp4PZS1l+uhSoycdEtERERtHhMWIiIiavOYsOhReXk51qxZg/Lycn2H0ibweijj9VDG66GM10MZr4cyQ7weBjXploiIiAwTR1iIiIiozWPCQkRERG0eExYiIiJq85iwEBERUZvHhOU2ixcvRkxMDEpLS3Hx4kWMHDmy0brbtm2DEAJLly5ttt1Zs2bh+vXrKCsrw/Xr13HfffcpvR4bGwshRL2yZcuWRtvs1KkTDh06hOTkZJSVlSEhIQEff/xxvT0Z/Pz8cOLECZSUlCApKQmrVq1qNt46+roeRkZGePvttxETE4OSkhJER0dj1apVkEgkTbbr6uqK3377DUVFRcjMzMSmTZtgbGysVKejXA9D/nxYWVnhww8/RFxcHEpKSnD27FkEBgY2266hfj5acj3a6+fD19cXP/30k+LfzMbqq9N3nfb4+dDV9WiNz0dLCRaI2bNni/LycvHkk0+K3r17iw8//FAUFhYKV1fXenVnzJghgoODRVJSkli6dGmT7Q4dOlRUVlaK5cuXi169eonly5eLiooKMXjwYEUde3t74eDgoCgTJkwQQggxZsyYRtu1tbUVixYtEgEBAcLNzU2MHz9ehIeHi507dyrqyOVykZqaKnbt2iX69OkjZs6cKfLz88V//vOfNn09XnvtNZGZmSmmTZsm3N3dxf333y8KCgrE888/32i7UqlUhISEiGPHjgl/f38xYcIEkZSUJDZv3twhr4chfz52794trl27JkaNGiW8vLzE6tWrRV5ennB2du6Qn4+WXI/2+vkIDAwU//3vf8WcOXNESkpKg/XV6bu9fz50dT10/fnQoOis4XZV/v77b/Hpp58qHQsLCxPr169XOubs7CwSExOFr6+viI2NbfYDtXv3bnHw4EGlY4cOHRK7du1q9JwPP/xQ3Lx5U+338Nxzz4mEhATFz4sWLRK5ubnCxMREcezVV18VSUlJbfp67N+/X3zxxRdKdX766Sfx7bffNtrulClTRFVVlXByclIcmzNnjigtLRVyubzDXQ9D/XyYmZmJyspKMW3aNKU6wcHB4u233+5wn4+WXo/2+vm4vTRWX9W+DeHzoavroevPR0sLbwkBMDY2RkBAAA4fPqx0/PDhwxg+fLjiZ4lEgh07duC9995DWFiYSm0PGzasXrt//vmnUrt3xvLoo4/iq6++Ujq+evVqxMbGNtqPk5MTZs2ahZMnTyr1ffLkSVRUVCj17eLiAg8Pj0bb0vf1OHPmDCZMmIAePXoAAPr164eRI0fi4MGDijp3Xo9hw4bh2rVrSE1NVWrXzMwMAQEBijod5XrcyVA+HzKZDDKZDGVlZUp1SktLlYa5O8rno6XX407t5fPRHFX7NpTPR3Naej3upM3PhyaYsACwt7eHTCZDenq60vH09HQ4Ojoqfn711VdRVVWFzZs3q9y2o6Njs+3e7r777oOtrS2++eYbpeNZWVmIjo6uV3/Xrl0oLi5GSkoKCgoKsHDhwmb7rnutMfq+Hu+++y6+//57REREoKKiAsHBwfjoo4+we/duRZ07r0dD7ebl5aG8vFzRdke6HnUM7fNRVFSEc+fOYdWqVXBycoJUKsUjjzyCIUOGwMnJSXFOR/l8tPR61Glvn4/mqNq3oXw+mtPS61FHF58PTTBhuY0QQulniUSiODZw4EAsXboUTzzxhFbbvdOTTz6JQ4cOKWX6APDJJ59g4sSJ9eq/8MILGDhwIGbMmAEvLy988MEHzfbd0HF149bl9ZgzZw4effRRPPzwwxg4cCDmzZuHl156CY8//riiTkPXo6H3dGfbHel6AIb5+XjssccgkUiQkpKC8vJyPP/889i1axeqq6sVdTrS56Ol1wNon58PVTR3zQzp86GKllwPQLefj5ZgwoLa7LKqqqpeVti1a1dFxjhq1Ch07doVCQkJqKysRGVlJTw8PLBx48Ymh9LS0tKabPd2bm5umDhxIr744guVY09PT0dkZCR+++03PPPMM1iyZImiv8b6rjuvMfq+Hu+99x7eeecd7NmzB9euXcN3332HDz/8ECtWrFCrXVtbW5iYmCja7kjXo44hfj5iYmIwduxYWFpawtXVFUOGDIGxsbHa7RrK56Ml16NOe/t8NEeVvhvSXj8fzWnp9aiji8+HJpiwAKisrMSlS5cwadIkpeOTJk3CuXPnAAA7duxAv3794O/vryjJycl47733cNdddzXadlBQUL12J0+erGj3dvPnz0dGRgYOHDjQovdRl92ampoq+h49erTSo3mTJ09GcnIy4uLiGm1H39fDwsICNTU1SnWqq6shlTb+cQ0KCoKfn5/SL9DkyZNRVlaGS5cuKep0lOvREEP5fNQpKSlBWloabG1tcdddd2Hfvn1Ntmuon4+WXI+GtIfPR3NU6bsh7fXz0ZyWXo+GaOvzoSmdzehtT6Xu0a/58+eL3r17iw8++EAUFhYKNze3Rs9RZRb3sGHDRGVlpXjllVdEr169xCuvvFLvsUQAQiKRiLi4OLFhw4YG23n22WfF0aNHFT9PnTpVPPHEE6JPnz7C3d1dTJ06VYSGhorTp08r6lhbW4vU1FSxc+dO0adPH3HfffeJvLw8tR7D08f1+Prrr0ViYqLiMd777rtPZGRkiHfeeafR61H3WOKRI0eEv7+/GD9+vEhISFB6LLEjXQ9D/nxMnjxZ3HXXXcLDw0NMnDhRBAcHi7///lvIZLIO+floyfVor58PY2Nj0b9/f9G/f3+RnJws/vvf/4r+/fsLLy8vtfo2lM+Hrq6Hrj8fGhSdNdzuyuLFi0VsbKwoKysTFy9eFKNGjWqyvqqPnd1///0iPDxclJeXi7CwMDFz5sx6dSZNmiSEEKJHjx4NtrF69WoRGxur+Hns2LHi7NmzIjc3V5SUlIjIyEixYcMGYWNjo3Sen5+fOHnypCgtLRUpKSnijTfeaPPXw8rKSnz44YciLi5OlJSUiKioKPH2228LY2PjRq8HAOHq6ir2798viouLRVZWlti8ebPSI3cd6XoY8ufjwQcfFFFRUaKsrEykpKSIjz/+WFhbWzf5+2LIn4+WXI/2+vlwd3cXDTl+/LhafRvK50NX16M1Ph8tKZJbfyAiIiJqsziHhYiIiNo8JixERETU5jFhISIiojaPCQsRERG1eUxYiIiIqM1jwkJERERtHhMWIiIiavOYsBAREVGbx4SFiIiI2jwmLERERNTmMWEhIiKiNo8JCxEREbV5/w9fukcETiQnNQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11207/11207 [==============================] - 35s 3ms/step\n",
      "ModelAccuracy: 84.341%\n",
      "True Win Predictions Mean of all: 2.078%\n",
      "XXX Loss Buy Mean of all: 4.646%\n",
      "Missed good deal off all: 11.013%\n",
      "Good Zero prediction Mean: 82.263%\n",
      "good fiability\n",
      "========= Win Ratio:30.904223676383104 %====================\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Generate Data\n",
    "BAD_PERIOD_START=\"2022-08-30\"\n",
    "BAD_PERIOD_END=\"2022-11-22\"\n",
    "pair_to_test=\"GMT/USDT\"\n",
    "MAX_FORCAST_SIZE=5\n",
    "USED_MODEL=very_deep_good_model#true_win_model#model_init #model_good_x3 #very_deep_good_model 16/1.7\n",
    "\n",
    "\n",
    "loc_start=0\n",
    "loc_end=1000000\n",
    "\n",
    "\n",
    "i_start=71000\n",
    "i_end=i_start+200\n",
    "\n",
    "# loc_start=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_START))\n",
    "# loc_end=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_END))\n",
    "\n",
    "\n",
    "OnePair_DF=mini_expand5(        pair=pair_to_test,\n",
    "                                i=loc_start,j=loc_end,\n",
    "                                window=WINDOW_SIZE,\n",
    "                                metadata=MetaData,\n",
    "                                high_weight=1,\n",
    "                                BUY_PCT=0.55,\n",
    "                                SELL_PCT=SELL_PCT,\n",
    "                                buy_function=buy_fix#buy_test\n",
    "                        )\n",
    "OnePair_DT=OnePair_DF.to_numpy()\n",
    "gc.collect()\n",
    "OnePair_DT=fixdt(OnePair_DT)\n",
    "print(OnePair_DT[0,0] == OnePair_DF.iloc[0,0])\n",
    "print(OnePair_DT[5,5] == OnePair_DF.iloc[5,5])\n",
    "hp(OnePair_DF.buy.mean(),\"Buy mean pct\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 10, 500)\n",
    "dashes = [10, 5, 100, 5]  # 10 points on, 5 off, 100 on, 5 off\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "                 label='Dashes set retroactively')\n",
    "line1.set_dashes(dashes)\n",
    "plt.plot(OnePair_DF[i_start:i_end][OnePair_DF.buy[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][OnePair_DF.buy[i_start:i_end]==1].price, 'ro')\n",
    "\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "OnePair_Pred=OnePair_PredNote.round()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "Original_Traget_Data=OnePair_DT[:,-1]\n",
    "Predicted_Data=OnePair_Pred[:,0]\n",
    "gc.collect()\n",
    "TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "gc.collect()\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "gc.collect()\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "gc.collect()\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "gc.collect()\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "gc.collect()\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "x = np.linspace(0, 10, 500)\n",
    "dashes = [10, 5, 100, 5]  # 10 points on, 5 off, 100 on, 5 off\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "                 label='price')\n",
    "line1.set_dashes(dashes)\n",
    "plt.plot(OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].price, 'ro')\n",
    "\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Good_Prediction_Note' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39m# Good_Prediction_Note=very_deep_good_model.predict( XX)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Initial_Pred_Note=model_init.predict( XX)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m Predicted_Data\u001b[39m=\u001b[39mOnePair_Pred[:\u001b[39m300000\u001b[39m,\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m goodp\u001b[39m=\u001b[39m(Good_Prediction_Note\u001b[39m-\u001b[39mprecision)\u001b[39m.\u001b[39mround()\n\u001b[1;32m      9\u001b[0m \u001b[39m# badp=(Bad_Prediction_Note).round()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# initp=Initial_Pred_Note.round()\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[39m# Original_Traget_Data=YY\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[39m#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\u001b[39;00m\n\u001b[1;32m     15\u001b[0m Predicted_Data\u001b[39m=\u001b[39m(goodp)[:,\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Good_Prediction_Note' is not defined"
     ]
    }
   ],
   "source": [
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "precision=0.0\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "Predicted_Data=OnePair_Pred[:300000,0]\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(XX,YY,USEDMODEL)\n",
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
