{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single AI crypto concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDOW:15 - BUY_PCT:1 MAX_FORCAST_SIZE:10 - BUY_MODE:BUY_OPTIMAL \n",
      "WINDOW:15 - BUY_PCT:1 MAX_FORCAST_SIZE:10 - BUY_MODE:BUY_OPTIMAL \n"
     ]
    }
   ],
   "source": [
    "from xdata_config import *\n",
    "SAMPE_SIZE_PP=8000\n",
    "print(f\"WINDOW:{WINDOW_SIZE} - BUY_PCT:{BUY_PCT} MAX_FORCAST_SIZE:{MAX_FORCAST_SIZE} - BUY_MODE:{BUY_MODE} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jupyterlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jt -l\n",
    "# !jt -t monokai -T -N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 4334.26 MB\n"
     ]
    }
   ],
   "source": [
    "from functions_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_optimal(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=MAX_FORCAST_SIZE):\n",
    "    #df = df.fillna(0)\n",
    "    mino = BUY_PCT / 100.0\n",
    "    maxo = SELL_PCT / 100.0\n",
    "    \n",
    "    max_forecast_size=window#MAX_FORCAST_SIZE\n",
    "    try:\n",
    "        after_dip_val=AFTER_MARK\n",
    "    except:\n",
    "        after_dip_val=3\n",
    "    print(f\"after mark = : {after_dip_val}\")\n",
    "    try:\n",
    "        print(f\"optimalbuy buy maximum forcast size={max_forecast_size} at {BUY_PCT}% of the current price \")\n",
    "    except:\n",
    "        max_forecast_size = 3\n",
    "        print(\"optimalbuy buy default window=3\")\n",
    "        \n",
    "    rolling_max_close_diff = ((df['close'].rolling(window=window).max().shift(-window+1) / df['close']) - 1).fillna(0)\n",
    "    df['buy']=(rolling_max_close_diff >= mino).astype(int)\n",
    "    \n",
    "    # Compute rolling minimum values\n",
    "    \n",
    "    window_list=[max(7,window)]#[3, 5, 7, 10, 15, 20]\n",
    "    \n",
    "    for window_size in window_list:\n",
    "        col_name = f'ismin{window_size}'\n",
    "        rolling_min = (df['close'].shift(after_dip_val) <= df.shift(-window_size-1)['close'].rolling(2*window_size).min())\n",
    "        df = df.assign(**{col_name: rolling_min.astype(int)})\n",
    "\n",
    "    df['ismin'] = df[[f'ismin{window_size}' for window_size in window_list ]].any(axis=1).astype(int)        \n",
    "\n",
    "    # # Compute buy and sell signals\n",
    "    rolling_low_close_diff =  ((df['low'].rolling(window=int(window/2)).min().shift(-int(window/2)+1)/ df['close'] ) -1).fillna(0)\n",
    "    df['sell'] = (rolling_low_close_diff <= -maxo).astype(int)\n",
    "\n",
    "    \n",
    "    # Compute final buy signal\n",
    "    df['buy'] = ((df['buy'] == 1) & (df['sell'] == 0) & (df['ismin'] == 1)).astype(int)\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=['sell', 'ismin'] + [f'ismin{window_size}' for window_size in window_list], errors='ignore')\n",
    "    return df\n",
    "def mini_expand5(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function\n",
    "    \"\"\"\n",
    "    This function takes in several parameters to calculate technical indicators and returns a merged dataframe.\n",
    "    \n",
    "    :param pair: str, default \"GMT/USDT\"\n",
    "        The trading pair to analyze.\n",
    "        \n",
    "    :param start_index: int, default 0\n",
    "        The start index for selecting data.\n",
    "        \n",
    "    :param end_index: int, default 10000\n",
    "        The end index for selecting data.\n",
    "    \n",
    "    :param window_size: int, default 2\n",
    "        The window size to use for analyzing the data.\n",
    "    \n",
    "    :param metadata: MetaData\n",
    "        The metadata to use for analyzing the data.\n",
    "    \n",
    "    :param high_weight: int, default 1\n",
    "        The weight to use for calculating the high.\n",
    "    \n",
    "    :param BUY_PCT: float, default BUY_PCT\n",
    "        The buy pct to use for analyzing the data.\n",
    "    \n",
    "    :param SELL_PCT: float, default SELL_PCT\n",
    "        The sell pct to use for analyzing the data.\n",
    "    \n",
    "    :param buy_fn: function, default buy_min_up\n",
    "        The buy function to use for analyzing the data.\n",
    "    \n",
    "    :return: pd.DataFrame\n",
    "        A merged dataframe containing the calculated technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"mini_expand : {pair}\")\n",
    "    # Select data\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    # Calculate technical indicators\n",
    "    pair_full = full_expand(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair], window_size)\n",
    "    btc_full = full_expand(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], window_size)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    open_high_low_close_cols = merged.columns.str.contains(\"open|high|low|close\")\n",
    "    # merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"BTC_price\"] - merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")]) / merged[\"BTC_price\"]\n",
    "    # )\n",
    "    # merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"price\"] - merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")]) / merged[\"price\"]\n",
    "    # )\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  mini_expand5 {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special list if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Binance_USDT_HALAL.index(\"ROSE/USDT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datetime import date\n",
    "# TICKERS = \"../Binance-Fast-Trade-Bot/volatile_volume_\" + str(date.today()) + \".txt\"\n",
    "# VOLATILE_COINS=[line.strip() for line in open(TICKERS)]\n",
    "# PAIR_WITH=\"USDT\"\n",
    "# VOLATILE_USDT_PAIRS=[coin+\"/USDT\" for coin in VOLATILE_COINS]\n",
    "# VOLATILE_BUSD_PAIRS=[coin+\"/BUSD\" for coin in VOLATILE_COINS]\n",
    "# VOLATILE_USDT_PAIRS\n",
    "\n",
    "# coins_to_download=''\n",
    "# for coin in VOLATILE_COINS:\n",
    "#     coins_to_download=coins_to_download+\" \"+coin\n",
    "# f\"node database/ddargs.js {coins_to_download} {PAIR_WITH}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coins_to_download=''\n",
    "# for coin in VOLATILE_COINS:\n",
    "#     coins_to_download=coins_to_download+\" \"+coin\n",
    "# os.system(f\"node database/ddargs.js {coins_to_download} {PAIR_WITH}\")#node database/ddargs.js ORN BUSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_list = find_intersection(VOLATILE_USDT_PAIRS,Binance_USDT_HALAL)\n",
    "# #tf = '1m'\n",
    "# oldest_pair = \"BTC/USDT\"\n",
    "# if oldest_pair not in pair_list: pair_list.append(oldest_pair)\n",
    "# df_list1m = {}\n",
    "# df_list1d = {}\n",
    "# df_list1h = {}\n",
    "# df_list5m = {}\n",
    "# df_list15m = {}\n",
    "\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1m', path=\"./database/\")\n",
    "#     df_list1m[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1d', path=\"./database/\")\n",
    "#     df_list1d[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '1h', path=\"./database/\")\n",
    "#     df_list1h[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(ccxt.binance(), pair, '5m', path=\"./database/\")\n",
    "#     df_list5m[pair] = df.loc[:]\n",
    "\n",
    "# for pair in pair_list:\n",
    "#     df = get_historical_from_db(\n",
    "#         ccxt.binance(), pair, '15m', path=\"./database/\")\n",
    "#     df_list15m[pair] = df.loc[:]\n",
    "# del(df)\n",
    "# df_list = df_list1m\n",
    "# prerr(\"Data load 100% use df_list1d[\\\"BTC/USDT\\\"] for exemple to access\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>Pair</th>\n",
       "      <th>launch_week_stamp</th>\n",
       "      <th>launch_day_stamp</th>\n",
       "      <th>launch_minute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SNMBUSD</td>\n",
       "      <td>SNM/BUSD</td>\n",
       "      <td>1661126400000</td>\n",
       "      <td>1661472000000</td>\n",
       "      <td>2022-08-26 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>BTC/USDT</td>\n",
       "      <td>1502668800000</td>\n",
       "      <td>1502928000000</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LUNAUSDT</td>\n",
       "      <td>LUNA/USDT</td>\n",
       "      <td>1597622400000</td>\n",
       "      <td>1597968000000</td>\n",
       "      <td>2020-08-21 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ETHUSDT</td>\n",
       "      <td>ETH/USDT</td>\n",
       "      <td>1502668800000</td>\n",
       "      <td>1502928000000</td>\n",
       "      <td>2017-08-17 04:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GMTUSDT</td>\n",
       "      <td>GMT/USDT</td>\n",
       "      <td>1646611200000</td>\n",
       "      <td>1646784000000</td>\n",
       "      <td>2022-03-09 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>FIDAUSDT</td>\n",
       "      <td>FIDA/USDT</td>\n",
       "      <td>1632700800000</td>\n",
       "      <td>1632960000000</td>\n",
       "      <td>2021-09-30 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>XNOUSDT</td>\n",
       "      <td>XNO/USDT</td>\n",
       "      <td>1642982400000</td>\n",
       "      <td>1643328000000</td>\n",
       "      <td>2022-01-28 08:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>BTGUSDT</td>\n",
       "      <td>BTG/USDT</td>\n",
       "      <td>1618185600000</td>\n",
       "      <td>1618531200000</td>\n",
       "      <td>2021-04-16 07:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>GHSTUSDT</td>\n",
       "      <td>GHST/USDT</td>\n",
       "      <td>1629072000000</td>\n",
       "      <td>1629417600000</td>\n",
       "      <td>2021-08-20 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>EPSUSDT</td>\n",
       "      <td>EPS/USDT</td>\n",
       "      <td>1616976000000</td>\n",
       "      <td>1617321600000</td>\n",
       "      <td>2021-04-02 09:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0       Pair  launch_week_stamp  launch_day_stamp  \\\n",
       "0     SNMBUSD   SNM/BUSD      1661126400000     1661472000000   \n",
       "1     BTCUSDT   BTC/USDT      1502668800000     1502928000000   \n",
       "2    LUNAUSDT  LUNA/USDT      1597622400000     1597968000000   \n",
       "3     ETHUSDT   ETH/USDT      1502668800000     1502928000000   \n",
       "4     GMTUSDT   GMT/USDT      1646611200000     1646784000000   \n",
       "..        ...        ...                ...               ...   \n",
       "107  FIDAUSDT  FIDA/USDT      1632700800000     1632960000000   \n",
       "108   XNOUSDT   XNO/USDT      1642982400000     1643328000000   \n",
       "109   BTGUSDT   BTG/USDT      1618185600000     1618531200000   \n",
       "110  GHSTUSDT  GHST/USDT      1629072000000     1629417600000   \n",
       "111   EPSUSDT   EPS/USDT      1616976000000     1617321600000   \n",
       "\n",
       "           launch_minute  \n",
       "0    2022-08-26 08:00:00  \n",
       "1    2017-08-17 04:00:00  \n",
       "2    2020-08-21 10:00:00  \n",
       "3    2017-08-17 04:00:00  \n",
       "4    2022-03-09 12:00:00  \n",
       "..                   ...  \n",
       "107  2021-09-30 12:00:00  \n",
       "108  2022-01-28 08:00:00  \n",
       "109  2021-04-16 07:00:00  \n",
       "110  2021-08-20 10:00:00  \n",
       "111  2021-04-02 09:00:00  \n",
       "\n",
       "[112 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chking import\n",
    "MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/UltimeTradingBot/Data/BUY_OPTIMAL'\n",
      "Results dir: /UltimeTradingBot/Data/BUY_OPTIMAL\n"
     ]
    }
   ],
   "source": [
    "if BUY_MODE==\"BUY_ONLY\":\n",
    "    buy_function=buy_up_only\n",
    "elif BUY_MODE==\"BUY_UP\":\n",
    "    buy_function=buy_up\n",
    "elif  BUY_MODE==\"BUY_DIP\":\n",
    "    buy_function=buy_min_up\n",
    "elif  BUY_MODE==\"AFTER_DEPTH\":\n",
    "    buy_function=buy_after_depth\n",
    "elif  BUY_MODE==\"BUY_UP_CLOSE\":\n",
    "    buy_function=buy_up_close\n",
    "elif  BUY_MODE==\"AFTER_DEPTH_CLOSE\":\n",
    "    buy_function=buy_after_depth_close\n",
    "elif  BUY_MODE==\"BUY_TEST\":\n",
    "    buy_function=buy_test\n",
    "elif BUY_MODE==\"BUY_MIN_CLOSE\":\n",
    "    buy_function=buy_min_close\n",
    "elif  BUY_MODE==\"SELL_TEST\":\n",
    "    buy_function=sell_test\n",
    "elif  BUY_MODE==\"BUY_FIX\":\n",
    "    buy_function=buy_fix\n",
    "elif  BUY_MODE==\"BUY_OPTIMAL\":\n",
    "    buy_function=buy_optimal\n",
    "\n",
    "\n",
    "try:\n",
    "    os.mkdir(DATA_DIR, mode = 0o777)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(f\"Results dir: {DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: SNM/BUSD -->mini_expand : SNM/BUSD\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.857%\n",
      "######################  mini_expand5 SNM/BUSD - shape (144969, 607)  buy mean : 0.857 ############################\n",
      "df original shape (144969, 607)\n",
      "df original shape buy mean : 0.8574246907959634\n",
      "SNM/BUSD is processed -- 0/112\n",
      "working on: LUNA/USDT -->mini_expand : LUNA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.392%\n",
      "######################  mini_expand5 LUNA/USDT - shape (942285, 607)  buy mean : 0.392 ############################\n",
      "df original shape (942285, 607)\n",
      "df original shape buy mean : 0.39202576715112725\n",
      "LUNA/USDT is processed -- 1/112\n",
      "working on: GMT/USDT -->mini_expand : GMT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.375%\n",
      "######################  mini_expand5 GMT/USDT - shape (347085, 607)  buy mean : 0.375 ############################\n",
      "df original shape (347085, 607)\n",
      "df original shape buy mean : 0.3754123629658441\n",
      "GMT/USDT is processed -- 2/112\n",
      "working on: UST/USDT -->mini_expand : UST/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.015%\n",
      "######################  mini_expand5 UST/USDT - shape (168530, 607)  buy mean : 0.015 ############################\n",
      "df original shape (168530, 607)\n",
      "df original shape buy mean : 0.014834154156529996\n",
      "UST/USDT is processed -- 3/112\n",
      "working on: SOL/USDT -->mini_expand : SOL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.317%\n",
      "######################  mini_expand5 SOL/USDT - shape (968522, 607)  buy mean : 0.317 ############################\n",
      "df original shape (968522, 607)\n",
      "df original shape buy mean : 0.3170810781789159\n",
      "SOL/USDT is processed -- 4/112\n",
      "working on: APE/USDT -->mini_expand : APE/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.342%\n",
      "######################  mini_expand5 APE/USDT - shape (335567, 607)  buy mean : 0.342 ############################\n",
      "df original shape (335567, 607)\n",
      "df original shape buy mean : 0.3415115312292329\n",
      "APE/USDT is processed -- 5/112\n",
      "working on: XRP/USDT -->mini_expand : XRP/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.209%\n",
      "######################  mini_expand5 XRP/USDT - shape (968516, 607)  buy mean : 0.209 ############################\n",
      "df original shape (968516, 607)\n",
      "df original shape buy mean : 0.20866975868235527\n",
      "XRP/USDT is processed -- 6/112\n",
      "working on: IDEX/USDT -->mini_expand : IDEX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.367%\n",
      "######################  mini_expand5 IDEX/USDT - shape (443570, 607)  buy mean : 0.367 ############################\n",
      "df original shape (443570, 607)\n",
      "df original shape buy mean : 0.3665712288928467\n",
      "IDEX/USDT is processed -- 7/112\n",
      "working on: AVAX/USDT -->mini_expand : AVAX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.396%\n",
      "######################  mini_expand5 AVAX/USDT - shape (968523, 607)  buy mean : 0.396 ############################\n",
      "df original shape (968523, 607)\n",
      "df original shape buy mean : 0.39627350099068376\n",
      "AVAX/USDT is processed -- 8/112\n",
      "working on: DOT/USDT -->mini_expand : DOT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.290%\n",
      "######################  mini_expand5 DOT/USDT - shape (968524, 607)  buy mean : 0.29 ############################\n",
      "df original shape (968524, 607)\n",
      "df original shape buy mean : 0.2895127018019172\n",
      "DOT/USDT is processed -- 9/112\n",
      "working on: ADA/USDT -->mini_expand : ADA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.237%\n",
      "######################  mini_expand5 ADA/USDT - shape (968514, 607)  buy mean : 0.237 ############################\n",
      "df original shape (968514, 607)\n",
      "df original shape buy mean : 0.23716745447148932\n",
      "ADA/USDT is processed -- 10/112\n",
      "working on: JASMY/USDT -->mini_expand : JASMY/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.976%\n",
      "######################  mini_expand5 JASMY/USDT - shape (443572, 607)  buy mean : 0.976 ############################\n",
      "df original shape (443572, 607)\n",
      "df original shape buy mean : 0.9757153291912023\n",
      "JASMY/USDT is processed -- 11/112\n",
      "working on: TRX/USDT -->mini_expand : TRX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.077%\n",
      "######################  mini_expand5 TRX/USDT - shape (443573, 607)  buy mean : 0.077 ############################\n",
      "df original shape (443573, 607)\n",
      "df original shape buy mean : 0.07710117613109906\n",
      "TRX/USDT is processed -- 12/112\n",
      "working on: NEAR/USDT -->mini_expand : NEAR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.401%\n",
      "######################  mini_expand5 NEAR/USDT - shape (968528, 607)  buy mean : 0.401 ############################\n",
      "df original shape (968528, 607)\n",
      "df original shape buy mean : 0.4012274296664629\n",
      "NEAR/USDT is processed -- 13/112\n",
      "working on: AXS/USDT -->mini_expand : AXS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.287%\n",
      "######################  mini_expand5 AXS/USDT - shape (443574, 607)  buy mean : 0.287 ############################\n",
      "df original shape (443574, 607)\n",
      "df original shape buy mean : 0.28653618111070533\n",
      "AXS/USDT is processed -- 14/112\n",
      "working on: GAL/USDT -->mini_expand : GAL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.381%\n",
      "######################  mini_expand5 GAL/USDT - shape (265014, 607)  buy mean : 0.381 ############################\n",
      "df original shape (265014, 607)\n",
      "df original shape buy mean : 0.3811119412559337\n",
      "GAL/USDT is processed -- 15/112\n",
      "working on: GALA/USDT -->mini_expand : GALA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.265%\n",
      "######################  mini_expand5 GALA/USDT - shape (443576, 607)  buy mean : 0.265 ############################\n",
      "df original shape (443576, 607)\n",
      "df original shape buy mean : 0.2651180406514329\n",
      "GALA/USDT is processed -- 16/112\n",
      "working on: SHIB/USDT -->mini_expand : SHIB/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.282%\n",
      "######################  mini_expand5 SHIB/USDT - shape (443577, 607)  buy mean : 0.282 ############################\n",
      "df original shape (443577, 607)\n",
      "df original shape buy mean : 0.28202544315868494\n",
      "SHIB/USDT is processed -- 17/112\n",
      "working on: ZIL/USDT -->mini_expand : ZIL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.251%\n",
      "######################  mini_expand5 ZIL/USDT - shape (443578, 607)  buy mean : 0.251 ############################\n",
      "df original shape (443578, 607)\n",
      "df original shape buy mean : 0.25091415714936266\n",
      "ZIL/USDT is processed -- 18/112\n",
      "working on: ENS/USDT -->mini_expand : ENS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.565%\n",
      "######################  mini_expand5 ENS/USDT - shape (443578, 607)  buy mean : 0.565 ############################\n",
      "df original shape (443578, 607)\n",
      "df original shape buy mean : 0.5647259332067867\n",
      "ENS/USDT is processed -- 19/112\n",
      "working on: DOGE/USDT -->mini_expand : DOGE/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.280%\n",
      "######################  mini_expand5 DOGE/USDT - shape (968520, 607)  buy mean : 0.28 ############################\n",
      "df original shape (968520, 607)\n",
      "df original shape buy mean : 0.28032461900631894\n",
      "DOGE/USDT is processed -- 20/112\n",
      "working on: LTC/USDT -->mini_expand : LTC/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.222%\n",
      "######################  mini_expand5 LTC/USDT - shape (968519, 607)  buy mean : 0.222 ############################\n",
      "df original shape (968519, 607)\n",
      "df original shape buy mean : 0.2218851669404524\n",
      "LTC/USDT is processed -- 21/112\n",
      "working on: MANA/USDT -->mini_expand : MANA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.177%\n",
      "######################  mini_expand5 MANA/USDT - shape (438150, 607)  buy mean : 0.177 ############################\n",
      "df original shape (438150, 607)\n",
      "df original shape buy mean : 0.177108296245578\n",
      "MANA/USDT is processed -- 22/112\n",
      "working on: DAR/USDT -->mini_expand : DAR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.362%\n",
      "######################  mini_expand5 DAR/USDT - shape (443581, 607)  buy mean : 0.362 ############################\n",
      "df original shape (443581, 607)\n",
      "df original shape buy mean : 0.36160250326321464\n",
      "DAR/USDT is processed -- 23/112\n",
      "working on: WAVES/USDT -->mini_expand : WAVES/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.436%\n",
      "######################  mini_expand5 WAVES/USDT - shape (443582, 607)  buy mean : 0.436 ############################\n",
      "df original shape (443582, 607)\n",
      "df original shape buy mean : 0.4355451754128887\n",
      "WAVES/USDT is processed -- 24/112\n",
      "working on: LAZIO/USDT -->mini_expand : LAZIO/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.506%\n",
      "######################  mini_expand5 LAZIO/USDT - shape (443582, 607)  buy mean : 0.506 ############################\n",
      "df original shape (443582, 607)\n",
      "df original shape buy mean : 0.5056562259063713\n",
      "LAZIO/USDT is processed -- 25/112\n",
      "working on: ALICE/USDT -->mini_expand : ALICE/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.426%\n",
      "######################  mini_expand5 ALICE/USDT - shape (443583, 607)  buy mean : 0.426 ############################\n",
      "df original shape (443583, 607)\n",
      "df original shape buy mean : 0.42607584149978694\n",
      "ALICE/USDT is processed -- 26/112\n",
      "working on: ROSE/USDT -->mini_expand : ROSE/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.271%\n",
      "######################  mini_expand5 ROSE/USDT - shape (443583, 607)  buy mean : 0.271 ############################\n",
      "df original shape (443583, 607)\n",
      "df original shape buy mean : 0.2709752177157375\n",
      "ROSE/USDT is processed -- 27/112\n",
      "working on: ZEC/USDT -->mini_expand : ZEC/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.381%\n",
      "######################  mini_expand5 ZEC/USDT - shape (443584, 607)  buy mean : 0.381 ############################\n",
      "df original shape (443584, 607)\n",
      "df original shape buy mean : 0.38143846486798444\n",
      "ZEC/USDT is processed -- 28/112\n",
      "working on: ALGO/USDT -->mini_expand : ALGO/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.191%\n",
      "######################  mini_expand5 ALGO/USDT - shape (443585, 607)  buy mean : 0.191 ############################\n",
      "df original shape (443585, 607)\n",
      "df original shape buy mean : 0.19139511029453204\n",
      "ALGO/USDT is processed -- 29/112\n",
      "working on: GRT/USDT -->mini_expand : GRT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.355%\n",
      "######################  mini_expand5 GRT/USDT - shape (443585, 607)  buy mean : 0.355 ############################\n",
      "df original shape (443585, 607)\n",
      "df original shape buy mean : 0.3548361644329723\n",
      "GRT/USDT is processed -- 30/112\n",
      "working on: PSG/USDT -->mini_expand : PSG/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.551%\n",
      "######################  mini_expand5 PSG/USDT - shape (443586, 607)  buy mean : 0.551 ############################\n",
      "df original shape (443586, 607)\n",
      "df original shape buy mean : 0.5509641873278237\n",
      "PSG/USDT is processed -- 31/112\n",
      "working on: SLP/USDT -->mini_expand : SLP/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 2.005%\n",
      "######################  mini_expand5 SLP/USDT - shape (443586, 607)  buy mean : 2.005 ############################\n",
      "df original shape (443586, 607)\n",
      "df original shape buy mean : 2.0045718304905926\n",
      "SLP/USDT is processed -- 32/112\n",
      "working on: EOS/USDT -->mini_expand : EOS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.181%\n",
      "######################  mini_expand5 EOS/USDT - shape (443587, 607)  buy mean : 0.181 ############################\n",
      "df original shape (443587, 607)\n",
      "df original shape buy mean : 0.18124967593730204\n",
      "EOS/USDT is processed -- 33/112\n",
      "working on: PORTO/USDT -->mini_expand : PORTO/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.523%\n",
      "######################  mini_expand5 PORTO/USDT - shape (438150, 607)  buy mean : 0.523 ############################\n",
      "df original shape (438150, 607)\n",
      "df original shape buy mean : 0.5226520597968732\n",
      "PORTO/USDT is processed -- 34/112\n",
      "working on: ICP/USDT -->mini_expand : ICP/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.414%\n",
      "######################  mini_expand5 ICP/USDT - shape (443588, 607)  buy mean : 0.414 ############################\n",
      "df original shape (443588, 607)\n",
      "df original shape buy mean : 0.41389758063788923\n",
      "ICP/USDT is processed -- 35/112\n",
      "working on: EGLD/USDT -->mini_expand : EGLD/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.288%\n",
      "######################  mini_expand5 EGLD/USDT - shape (968529, 607)  buy mean : 0.288 ############################\n",
      "df original shape (968529, 607)\n",
      "df original shape buy mean : 0.28847871359556604\n",
      "EGLD/USDT is processed -- 36/112\n",
      "working on: XMR/USDT -->mini_expand : XMR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.171%\n",
      "######################  mini_expand5 XMR/USDT - shape (443589, 607)  buy mean : 0.171 ############################\n",
      "df original shape (443589, 607)\n",
      "df original shape buy mean : 0.1706534652572539\n",
      "XMR/USDT is processed -- 37/112\n",
      "working on: KDA/USDT -->mini_expand : KDA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.381%\n",
      "######################  mini_expand5 KDA/USDT - shape (344230, 607)  buy mean : 0.381 ############################\n",
      "df original shape (344230, 607)\n",
      "df original shape buy mean : 0.3808500130726549\n",
      "KDA/USDT is processed -- 38/112\n",
      "working on: ETC/USDT -->mini_expand : ETC/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.255%\n",
      "######################  mini_expand5 ETC/USDT - shape (443591, 607)  buy mean : 0.255 ############################\n",
      "df original shape (443591, 607)\n",
      "df original shape buy mean : 0.2545137299900133\n",
      "ETC/USDT is processed -- 39/112\n",
      "working on: MBOX/USDT -->mini_expand : MBOX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.383%\n",
      "######################  mini_expand5 MBOX/USDT - shape (443591, 607)  buy mean : 0.383 ############################\n",
      "df original shape (443591, 607)\n",
      "df original shape buy mean : 0.38255961009127776\n",
      "MBOX/USDT is processed -- 40/112\n",
      "working on: OGN/USDT -->mini_expand : OGN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.366%\n",
      "######################  mini_expand5 OGN/USDT - shape (443592, 607)  buy mean : 0.366 ############################\n",
      "df original shape (443592, 607)\n",
      "df original shape buy mean : 0.3656513192302837\n",
      "OGN/USDT is processed -- 41/112\n",
      "working on: AR/USDT -->mini_expand : AR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.422%\n",
      "######################  mini_expand5 AR/USDT - shape (443592, 607)  buy mean : 0.422 ############################\n",
      "df original shape (443592, 607)\n",
      "df original shape buy mean : 0.42178398167685616\n",
      "AR/USDT is processed -- 42/112\n",
      "working on: GLMR/USDT -->mini_expand : GLMR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.346%\n",
      "######################  mini_expand5 GLMR/USDT - shape (429193, 607)  buy mean : 0.346 ############################\n",
      "df original shape (429193, 607)\n",
      "df original shape buy mean : 0.3462311827080125\n",
      "GLMR/USDT is processed -- 43/112\n",
      "working on: LOKA/USDT -->mini_expand : LOKA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.477%\n",
      "######################  mini_expand5 LOKA/USDT - shape (416234, 607)  buy mean : 0.477 ############################\n",
      "df original shape (416234, 607)\n",
      "df original shape buy mean : 0.4766549585089157\n",
      "LOKA/USDT is processed -- 44/112\n",
      "working on: XLM/USDT -->mini_expand : XLM/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.127%\n",
      "######################  mini_expand5 XLM/USDT - shape (443594, 607)  buy mean : 0.127 ############################\n",
      "df original shape (443594, 607)\n",
      "df original shape buy mean : 0.12669242595706884\n",
      "XLM/USDT is processed -- 45/112\n",
      "working on: MTL/USDT -->mini_expand : MTL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.385%\n",
      "######################  mini_expand5 MTL/USDT - shape (443595, 607)  buy mean : 0.385 ############################\n",
      "df original shape (443595, 607)\n",
      "df original shape buy mean : 0.3850358998636143\n",
      "MTL/USDT is processed -- 46/112\n",
      "working on: SNX/USDT -->mini_expand : SNX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.351%\n",
      "######################  mini_expand5 SNX/USDT - shape (443596, 607)  buy mean : 0.351 ############################\n",
      "df original shape (443596, 607)\n",
      "df original shape buy mean : 0.35099504954959015\n",
      "SNX/USDT is processed -- 47/112\n",
      "working on: PYR/USDT -->mini_expand : PYR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 1.040%\n",
      "######################  mini_expand5 PYR/USDT - shape (443596, 607)  buy mean : 1.04 ############################\n",
      "df original shape (443596, 607)\n",
      "df original shape buy mean : 1.0403610492430049\n",
      "PYR/USDT is processed -- 48/112\n",
      "working on: DASH/USDT -->mini_expand : DASH/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.239%\n",
      "######################  mini_expand5 DASH/USDT - shape (443597, 607)  buy mean : 0.239 ############################\n",
      "df original shape (443597, 607)\n",
      "df original shape buy mean : 0.23918105848326296\n",
      "DASH/USDT is processed -- 49/112\n",
      "working on: CITY/USDT -->mini_expand : CITY/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.623%\n",
      "######################  mini_expand5 CITY/USDT - shape (421650, 607)  buy mean : 0.623 ############################\n",
      "df original shape (421650, 607)\n",
      "df original shape buy mean : 0.6230285782046721\n",
      "CITY/USDT is processed -- 50/112\n",
      "working on: ASTR/USDT -->mini_expand : ASTR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.780%\n",
      "######################  mini_expand5 ASTR/USDT - shape (360078, 607)  buy mean : 0.78 ############################\n",
      "df original shape (360078, 607)\n",
      "df original shape buy mean : 0.7801087542143647\n",
      "ASTR/USDT is processed -- 51/112\n",
      "working on: IOTA/USDT -->mini_expand : IOTA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.151%\n",
      "######################  mini_expand5 IOTA/USDT - shape (443599, 607)  buy mean : 0.151 ############################\n",
      "df original shape (443599, 607)\n",
      "df original shape buy mean : 0.15126273954630196\n",
      "IOTA/USDT is processed -- 52/112\n",
      "working on: ACM/USDT -->mini_expand : ACM/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.556%\n",
      "######################  mini_expand5 ACM/USDT - shape (433650, 607)  buy mean : 0.556 ############################\n",
      "df original shape (433650, 607)\n",
      "df original shape buy mean : 0.555978323532803\n",
      "ACM/USDT is processed -- 53/112\n",
      "working on: BAR/USDT -->mini_expand : BAR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.946%\n",
      "######################  mini_expand5 BAR/USDT - shape (443600, 607)  buy mean : 0.946 ############################\n",
      "df original shape (443600, 607)\n",
      "df original shape buy mean : 0.9461226330027053\n",
      "BAR/USDT is processed -- 54/112\n",
      "working on: JUV/USDT -->mini_expand : JUV/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.993%\n",
      "######################  mini_expand5 JUV/USDT - shape (443601, 607)  buy mean : 0.993 ############################\n",
      "df original shape (443601, 607)\n",
      "df original shape buy mean : 0.9934603393590185\n",
      "JUV/USDT is processed -- 55/112\n",
      "working on: SYS/USDT -->mini_expand : SYS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.431%\n",
      "######################  mini_expand5 SYS/USDT - shape (443601, 607)  buy mean : 0.431 ############################\n",
      "df original shape (443601, 607)\n",
      "df original shape buy mean : 0.43079253653621163\n",
      "SYS/USDT is processed -- 56/112\n",
      "working on: RVN/USDT -->mini_expand : RVN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.257%\n",
      "######################  mini_expand5 RVN/USDT - shape (443602, 607)  buy mean : 0.257 ############################\n",
      "df original shape (443602, 607)\n",
      "df original shape buy mean : 0.2569871190842241\n",
      "RVN/USDT is processed -- 57/112\n",
      "working on: MBL/USDT -->mini_expand : MBL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.359%\n",
      "######################  mini_expand5 MBL/USDT - shape (443603, 607)  buy mean : 0.359 ############################\n",
      "df original shape (443603, 607)\n",
      "df original shape buy mean : 0.3591048753051715\n",
      "MBL/USDT is processed -- 58/112\n",
      "working on: REN/USDT -->mini_expand : REN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.425%\n",
      "######################  mini_expand5 REN/USDT - shape (443603, 607)  buy mean : 0.425 ############################\n",
      "df original shape (443603, 607)\n",
      "df original shape buy mean : 0.42515492456092496\n",
      "REN/USDT is processed -- 59/112\n",
      "working on: JST/USDT -->mini_expand : JST/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.167%\n",
      "######################  mini_expand5 JST/USDT - shape (443604, 607)  buy mean : 0.167 ############################\n",
      "df original shape (443604, 607)\n",
      "df original shape buy mean : 0.1665900217310935\n",
      "JST/USDT is processed -- 60/112\n",
      "working on: OMG/USDT -->mini_expand : OMG/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.211%\n",
      "######################  mini_expand5 OMG/USDT - shape (443604, 607)  buy mean : 0.211 ############################\n",
      "df original shape (443604, 607)\n",
      "df original shape buy mean : 0.2112244253884095\n",
      "OMG/USDT is processed -- 61/112\n",
      "working on: ATM/USDT -->mini_expand : ATM/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 1.055%\n",
      "######################  mini_expand5 ATM/USDT - shape (443605, 607)  buy mean : 1.055 ############################\n",
      "df original shape (443605, 607)\n",
      "df original shape buy mean : 1.054767191533008\n",
      "ATM/USDT is processed -- 62/112\n",
      "working on: XEC/USDT -->mini_expand : XEC/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.206%\n",
      "######################  mini_expand5 XEC/USDT - shape (443606, 607)  buy mean : 0.206 ############################\n",
      "df original shape (443606, 607)\n",
      "df original shape buy mean : 0.20558784146291978\n",
      "XEC/USDT is processed -- 63/112\n",
      "working on: STORJ/USDT -->mini_expand : STORJ/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.292%\n",
      "######################  mini_expand5 STORJ/USDT - shape (443606, 607)  buy mean : 0.292 ############################\n",
      "df original shape (443606, 607)\n",
      "df original shape buy mean : 0.29192571786675564\n",
      "STORJ/USDT is processed -- 64/112\n",
      "working on: ZRX/USDT -->mini_expand : ZRX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.259%\n",
      "######################  mini_expand5 ZRX/USDT - shape (443607, 607)  buy mean : 0.259 ############################\n",
      "df original shape (443607, 607)\n",
      "df original shape buy mean : 0.2587876205740667\n",
      "ZRX/USDT is processed -- 65/112\n",
      "working on: SRM/USDT -->mini_expand : SRM/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.265%\n",
      "######################  mini_expand5 SRM/USDT - shape (443607, 607)  buy mean : 0.265 ############################\n",
      "df original shape (443607, 607)\n",
      "df original shape buy mean : 0.2646486642456048\n",
      "SRM/USDT is processed -- 66/112\n",
      "working on: ICX/USDT -->mini_expand : ICX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.643%\n",
      "######################  mini_expand5 ICX/USDT - shape (443608, 607)  buy mean : 0.643 ############################\n",
      "df original shape (443608, 607)\n",
      "df original shape buy mean : 0.6433608050350761\n",
      "ICX/USDT is processed -- 67/112\n",
      "working on: API3/USDT -->mini_expand : API3/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.505%\n",
      "######################  mini_expand5 API3/USDT - shape (414809, 607)  buy mean : 0.505 ############################\n",
      "df original shape (414809, 607)\n",
      "df original shape buy mean : 0.5045695729841928\n",
      "API3/USDT is processed -- 68/112\n",
      "working on: ONT/USDT -->mini_expand : ONT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.154%\n",
      "######################  mini_expand5 ONT/USDT - shape (443609, 607)  buy mean : 0.154 ############################\n",
      "df original shape (443609, 607)\n",
      "df original shape buy mean : 0.15441526208891165\n",
      "ONT/USDT is processed -- 69/112\n",
      "working on: SKL/USDT -->mini_expand : SKL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.500%\n",
      "######################  mini_expand5 SKL/USDT - shape (443610, 607)  buy mean : 0.5 ############################\n",
      "df original shape (443610, 607)\n",
      "df original shape buy mean : 0.5004395753026307\n",
      "SKL/USDT is processed -- 70/112\n",
      "working on: MULTI/USDT -->mini_expand : MULTI/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.653%\n",
      "######################  mini_expand5 MULTI/USDT - shape (306810, 607)  buy mean : 0.653 ############################\n",
      "df original shape (306810, 607)\n",
      "df original shape buy mean : 0.6528470388839999\n",
      "MULTI/USDT is processed -- 71/112\n",
      "working on: QTUM/USDT -->mini_expand : QTUM/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.169%\n",
      "######################  mini_expand5 QTUM/USDT - shape (443611, 607)  buy mean : 0.169 ############################\n",
      "df original shape (443611, 607)\n",
      "df original shape buy mean : 0.16906704297233388\n",
      "QTUM/USDT is processed -- 72/112\n",
      "working on: COCOS/USDT -->mini_expand : COCOS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.319%\n",
      "######################  mini_expand5 COCOS/USDT - shape (443612, 607)  buy mean : 0.319 ############################\n",
      "df original shape (443612, 607)\n",
      "df original shape buy mean : 0.31897243537145076\n",
      "COCOS/USDT is processed -- 73/112\n",
      "working on: VOXEL/USDT -->mini_expand : VOXEL/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.454%\n",
      "######################  mini_expand5 VOXEL/USDT - shape (432900, 607)  buy mean : 0.454 ############################\n",
      "df original shape (432900, 607)\n",
      "df original shape buy mean : 0.45414645414645416\n",
      "VOXEL/USDT is processed -- 74/112\n",
      "working on: HIVE/USDT -->mini_expand : HIVE/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.282%\n",
      "######################  mini_expand5 HIVE/USDT - shape (443613, 607)  buy mean : 0.282 ############################\n",
      "df original shape (443613, 607)\n",
      "df original shape buy mean : 0.28155171286684566\n",
      "HIVE/USDT is processed -- 75/112\n",
      "working on: KP3R/USDT -->mini_expand : KP3R/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.415%\n",
      "######################  mini_expand5 KP3R/USDT - shape (443614, 607)  buy mean : 0.415 ############################\n",
      "df original shape (443614, 607)\n",
      "df original shape buy mean : 0.41522584949979036\n",
      "KP3R/USDT is processed -- 76/112\n",
      "working on: ATA/USDT -->mini_expand : ATA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.345%\n",
      "######################  mini_expand5 ATA/USDT - shape (443614, 607)  buy mean : 0.345 ############################\n",
      "df original shape (443614, 607)\n",
      "df original shape buy mean : 0.3451198564517801\n",
      "ATA/USDT is processed -- 77/112\n",
      "working on: STMX/USDT -->mini_expand : STMX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.446%\n",
      "######################  mini_expand5 STMX/USDT - shape (443615, 607)  buy mean : 0.446 ############################\n",
      "df original shape (443615, 607)\n",
      "df original shape buy mean : 0.4463329689032156\n",
      "STMX/USDT is processed -- 78/112\n",
      "working on: ADX/USDT -->mini_expand : ADX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.317%\n",
      "######################  mini_expand5 ADX/USDT - shape (443616, 607)  buy mean : 0.317 ############################\n",
      "df original shape (443616, 607)\n",
      "df original shape buy mean : 0.31716619779268557\n",
      "ADX/USDT is processed -- 79/112\n",
      "working on: HIGH/USDT -->mini_expand : HIGH/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 1.467%\n",
      "######################  mini_expand5 HIGH/USDT - shape (443616, 607)  buy mean : 1.467 ############################\n",
      "df original shape (443616, 607)\n",
      "df original shape buy mean : 1.4668091322224626\n",
      "HIGH/USDT is processed -- 80/112\n",
      "working on: NULS/USDT -->mini_expand : NULS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.510%\n",
      "######################  mini_expand5 NULS/USDT - shape (443617, 607)  buy mean : 0.51 ############################\n",
      "df original shape (443617, 607)\n",
      "df original shape buy mean : 0.5096738853560616\n",
      "NULS/USDT is processed -- 81/112\n",
      "working on: MLN/USDT -->mini_expand : MLN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 1.149%\n",
      "######################  mini_expand5 MLN/USDT - shape (443617, 607)  buy mean : 1.149 ############################\n",
      "df original shape (443617, 607)\n",
      "df original shape buy mean : 1.1491895035582496\n",
      "MLN/USDT is processed -- 82/112\n",
      "working on: YGG/USDT -->mini_expand : YGG/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.587%\n",
      "######################  mini_expand5 YGG/USDT - shape (443618, 607)  buy mean : 0.587 ############################\n",
      "df original shape (443618, 607)\n",
      "df original shape buy mean : 0.586766091547232\n",
      "YGG/USDT is processed -- 83/112\n",
      "working on: SC/USDT -->mini_expand : SC/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.462%\n",
      "######################  mini_expand5 SC/USDT - shape (443619, 607)  buy mean : 0.462 ############################\n",
      "df original shape (443619, 607)\n",
      "df original shape buy mean : 0.46210825054833093\n",
      "SC/USDT is processed -- 84/112\n",
      "working on: CKB/USDT -->mini_expand : CKB/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.402%\n",
      "######################  mini_expand5 CKB/USDT - shape (443619, 607)  buy mean : 0.402 ############################\n",
      "df original shape (443619, 607)\n",
      "df original shape buy mean : 0.402372305965254\n",
      "CKB/USDT is processed -- 85/112\n",
      "working on: TOMO/USDT -->mini_expand : TOMO/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.476%\n",
      "######################  mini_expand5 TOMO/USDT - shape (443620, 607)  buy mean : 0.476 ############################\n",
      "df original shape (443620, 607)\n",
      "df original shape buy mean : 0.47630855236463643\n",
      "TOMO/USDT is processed -- 86/112\n",
      "working on: STX/USDT -->mini_expand : STX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.661%\n",
      "######################  mini_expand5 STX/USDT - shape (443620, 607)  buy mean : 0.661 ############################\n",
      "df original shape (443620, 607)\n",
      "df original shape buy mean : 0.6611514359136198\n",
      "STX/USDT is processed -- 87/112\n",
      "working on: FLUX/USDT -->mini_expand : FLUX/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.718%\n",
      "######################  mini_expand5 FLUX/USDT - shape (443621, 607)  buy mean : 0.718 ############################\n",
      "df original shape (443621, 607)\n",
      "df original shape buy mean : 0.7184060267660909\n",
      "FLUX/USDT is processed -- 88/112\n",
      "working on: DNT/USDT -->mini_expand : DNT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.961%\n",
      "######################  mini_expand5 DNT/USDT - shape (405180, 607)  buy mean : 0.961 ############################\n",
      "df original shape (405180, 607)\n",
      "df original shape buy mean : 0.960807542326867\n",
      "DNT/USDT is processed -- 89/112\n",
      "working on: ORN/USDT -->mini_expand : ORN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.411%\n",
      "######################  mini_expand5 ORN/USDT - shape (443622, 607)  buy mean : 0.411 ############################\n",
      "df original shape (443622, 607)\n",
      "df original shape buy mean : 0.41138627029317754\n",
      "ORN/USDT is processed -- 90/112\n",
      "working on: PLA/USDT -->mini_expand : PLA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.944%\n",
      "######################  mini_expand5 PLA/USDT - shape (443623, 607)  buy mean : 0.944 ############################\n",
      "df original shape (443623, 607)\n",
      "df original shape buy mean : 0.9444956641111936\n",
      "PLA/USDT is processed -- 91/112\n",
      "working on: BADGER/USDT -->mini_expand : BADGER/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.821%\n",
      "######################  mini_expand5 BADGER/USDT - shape (443623, 607)  buy mean : 0.821 ############################\n",
      "df original shape (443623, 607)\n",
      "df original shape buy mean : 0.820516519657457\n",
      "BADGER/USDT is processed -- 92/112\n",
      "working on: DF/USDT -->mini_expand : DF/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 1.291%\n",
      "######################  mini_expand5 DF/USDT - shape (443624, 607)  buy mean : 1.291 ############################\n",
      "df original shape (443624, 607)\n",
      "df original shape buy mean : 1.290732692550448\n",
      "DF/USDT is processed -- 93/112\n",
      "working on: MOB/USDT -->mini_expand : MOB/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.366%\n",
      "######################  mini_expand5 MOB/USDT - shape (273705, 607)  buy mean : 0.366 ############################\n",
      "df original shape (273705, 607)\n",
      "df original shape buy mean : 0.36608757603989694\n",
      "MOB/USDT is processed -- 94/112\n",
      "working on: LPT/USDT -->mini_expand : LPT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.319%\n",
      "######################  mini_expand5 LPT/USDT - shape (443625, 607)  buy mean : 0.319 ############################\n",
      "df original shape (443625, 607)\n",
      "df original shape buy mean : 0.31896308819385744\n",
      "LPT/USDT is processed -- 95/112\n",
      "working on: SCRT/USDT -->mini_expand : SCRT/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.292%\n",
      "######################  mini_expand5 SCRT/USDT - shape (414826, 607)  buy mean : 0.292 ############################\n",
      "df original shape (414826, 607)\n",
      "df original shape buy mean : 0.2919296283260933\n",
      "SCRT/USDT is processed -- 96/112\n",
      "working on: RAD/USDT -->mini_expand : RAD/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.567%\n",
      "######################  mini_expand5 RAD/USDT - shape (610059, 607)  buy mean : 0.567 ############################\n",
      "df original shape (610059, 607)\n",
      "df original shape buy mean : 0.5668304213199051\n",
      "RAD/USDT is processed -- 97/112\n",
      "working on: NMR/USDT -->mini_expand : NMR/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.584%\n",
      "######################  mini_expand5 NMR/USDT - shape (443627, 607)  buy mean : 0.584 ############################\n",
      "df original shape (443627, 607)\n",
      "df original shape buy mean : 0.5842746271079082\n",
      "NMR/USDT is processed -- 98/112\n",
      "working on: ELF/USDT -->mini_expand : ELF/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.332%\n",
      "######################  mini_expand5 ELF/USDT - shape (443628, 607)  buy mean : 0.332 ############################\n",
      "df original shape (443628, 607)\n",
      "df original shape buy mean : 0.3320349481998431\n",
      "ELF/USDT is processed -- 99/112\n",
      "working on: TORN/USDT -->mini_expand : TORN/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.752%\n",
      "######################  mini_expand5 TORN/USDT - shape (432900, 607)  buy mean : 0.752 ############################\n",
      "df original shape (432900, 607)\n",
      "df original shape buy mean : 0.7523677523677523\n",
      "TORN/USDT is processed -- 100/112\n",
      "working on: T/USDT -->mini_expand : T/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.728%\n",
      "######################  mini_expand5 T/USDT - shape (364429, 607)  buy mean : 0.728 ############################\n",
      "df original shape (364429, 607)\n",
      "df original shape buy mean : 0.727988167791257\n",
      "T/USDT is processed -- 101/112\n",
      "working on: QUICK/USDT -->mini_expand : QUICK/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.729%\n",
      "######################  mini_expand5 QUICK/USDT - shape (443630, 607)  buy mean : 0.729 ############################\n",
      "df original shape (443630, 607)\n",
      "df original shape buy mean : 0.729436692739445\n",
      "QUICK/USDT is processed -- 102/112\n",
      "working on: LSK/USDT -->mini_expand : LSK/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.551%\n",
      "######################  mini_expand5 LSK/USDT - shape (443630, 607)  buy mean : 0.551 ############################\n",
      "df original shape (443630, 607)\n",
      "df original shape buy mean : 0.5513603678741293\n",
      "LSK/USDT is processed -- 103/112\n",
      "working on: FIDA/USDT -->mini_expand : FIDA/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.513%\n",
      "######################  mini_expand5 FIDA/USDT - shape (443631, 607)  buy mean : 0.513 ############################\n",
      "df original shape (443631, 607)\n",
      "df original shape buy mean : 0.5125881644880542\n",
      "FIDA/USDT is processed -- 104/112\n",
      "working on: XNO/USDT -->mini_expand : XNO/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.429%\n",
      "######################  mini_expand5 XNO/USDT - shape (404752, 607)  buy mean : 0.429 ############################\n",
      "df original shape (404752, 607)\n",
      "df original shape buy mean : 0.42939874293394475\n",
      "XNO/USDT is processed -- 105/112\n",
      "working on: BTG/USDT -->mini_expand : BTG/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.640%\n",
      "######################  mini_expand5 BTG/USDT - shape (405180, 607)  buy mean : 0.64 ############################\n",
      "df original shape (405180, 607)\n",
      "df original shape buy mean : 0.639715681919147\n",
      "BTG/USDT is processed -- 106/112\n",
      "working on: GHST/USDT -->mini_expand : GHST/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.087%\n",
      "######################  mini_expand5 GHST/USDT - shape (443633, 607)  buy mean : 0.087 ############################\n",
      "df original shape (443633, 607)\n",
      "df original shape buy mean : 0.0870088564196081\n",
      "GHST/USDT is processed -- 107/112\n",
      "working on: EPS/USDT -->mini_expand : EPS/USDT\n",
      "after mark = : 3\n",
      "optimalbuy buy maximum forcast size=10 at 1% of the current price \n",
      "Precent Mean: 0.500%\n",
      "######################  mini_expand5 EPS/USDT - shape (162960, 607)  buy mean : 0.5 ############################\n",
      "df original shape (162960, 607)\n",
      "df original shape buy mean : 0.5001227295041728\n",
      "EPS/USDT is processed -- 108/112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf=pd.DataFrame()\n",
    "count=0\n",
    "row_numbers=SAMPE_SIZE_PP\n",
    "for pair in pair_list:\n",
    "    if pair != \"BTC/USDT\" and pair != \"EUR/USDT\" and pair != \"ETH/USDT\" :\n",
    "        print(\"working on: \"+pair ,end=\" -->\")\n",
    "        try:\n",
    "            \n",
    "            df=mini_expand5(pair=pair,i=0,j=len(df_list1m[pair]),window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function)\n",
    "            print(\"df original shape \"+str(df.shape))\n",
    "            print(f\"df original shape buy mean : {df.buy.mean()*100}\")\n",
    "            df=df.reset_index()\n",
    "            try:df.pop(\"num_index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"date\")\n",
    "            except: pass\n",
    "            df=data_shufler(df)            \n",
    "            #df=data_chooser(df,weight=50,row_numbers=df.buy.sum()*2)\n",
    "            df=data_chooser50(df,row_numbers=row_numbers)\n",
    "            gc.collect()\n",
    "            df=data_cleanup(df)\n",
    "            df=df.dropna()\n",
    "            print(pair+f\" is processed -- {count}/{len(pair_list)}\")\n",
    "            # print(df.iloc[0:1])\n",
    "        except Exception as e:\n",
    "            print(f\"error while processing {pair} {count}/{len(pair_list)}\")\n",
    "            print(e)\n",
    "        xdf=pd.concat([xdf,df],axis=0)\n",
    "        count+=1\n",
    "        del(df)\n",
    "        gc.collect()\n",
    "df=xdf\n",
    "del xdf\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-14_5min</th>\n",
       "      <th>BTC_high-15_5min</th>\n",
       "      <th>BTC_low-15_5min</th>\n",
       "      <th>BTC_close-15_5min</th>\n",
       "      <th>BTC_volume-15_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>124.69049</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>133.82389</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>-51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137650</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>76980.00</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>38934.000</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>...</td>\n",
       "      <td>1093.82764</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>-0.010518</td>\n",
       "      <td>1260.19146</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>-351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.427500</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>116.70</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>5661.100</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>...</td>\n",
       "      <td>47.73208</td>\n",
       "      <td>-0.002308</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>62.24329</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>-616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2456.00</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2215.000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>...</td>\n",
       "      <td>438.84442</td>\n",
       "      <td>-0.007394</td>\n",
       "      <td>-0.005291</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>82.35837</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.96432</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>1282.82839</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871995</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>288.58946</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>317.43590</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>-551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871996</th>\n",
       "      <td>0.062602</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>34287.70</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>22344.100</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>...</td>\n",
       "      <td>281.76808</td>\n",
       "      <td>-0.002617</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>385.63223</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>-223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871997</th>\n",
       "      <td>1.194575</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>15462.00</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>55672.000</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>...</td>\n",
       "      <td>187.61722</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.012989</td>\n",
       "      <td>-0.013998</td>\n",
       "      <td>146.11996</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>-218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871998</th>\n",
       "      <td>0.061308</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>65270.00</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>123291.000</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>...</td>\n",
       "      <td>330.94158</td>\n",
       "      <td>-0.009130</td>\n",
       "      <td>-0.004610</td>\n",
       "      <td>-0.006787</td>\n",
       "      <td>542.04080</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871999</th>\n",
       "      <td>26.792500</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>4353.17</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>1087.910</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>...</td>\n",
       "      <td>171.74612</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.007079</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>148.16977</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>-308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872000 rows × 607 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price    high-1     low-1   close-1  volume-1    high-2     low-2  \\\n",
       "0        0.005725  0.000000  0.000000  0.000000      0.00  0.000000  0.000000   \n",
       "1        0.137650 -0.000363  0.001090 -0.000363  76980.00 -0.000363  0.000363   \n",
       "2        0.427500 -0.000468 -0.000468 -0.000468    116.70 -0.003509  0.002573   \n",
       "3        0.510300  0.000000  0.000588  0.000000   2456.00 -0.000196  0.000784   \n",
       "4        0.486200  0.000000  0.000000  0.000000      0.00  0.000000  0.000000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "871995  31.000000  0.000000  0.000000  0.000000      0.00  0.000000  0.003226   \n",
       "871996   0.062602 -0.000439  0.001318  0.000200  34287.70  0.001318  0.002117   \n",
       "871997   1.194575 -0.001277  0.001904 -0.000691  15462.00  0.000565  0.006927   \n",
       "871998   0.061308 -0.001509 -0.000204 -0.000204  65270.00 -0.002161  0.000775   \n",
       "871999  26.792500  0.000467  0.004199  0.001213   4353.17  0.003826  0.006065   \n",
       "\n",
       "         close-2    volume-2    high-3  ...  BTC_volume-14_5min  \\\n",
       "0       0.000000       0.000  0.000000  ...           124.69049   \n",
       "1       0.000363   38934.000  0.000363  ...          1093.82764   \n",
       "2      -0.003509    5661.100  0.002105  ...            47.73208   \n",
       "3       0.000000    2215.000  0.000196  ...           438.84442   \n",
       "4       0.000000       0.000  0.000000  ...          1797.96432   \n",
       "...          ...         ...       ...  ...                 ...   \n",
       "871995  0.000000      17.036  0.000000  ...           288.58946   \n",
       "871996  0.001318   22344.100  0.001478  ...           281.76808   \n",
       "871997  0.001486   55672.000  0.003076  ...           187.61722   \n",
       "871998 -0.001509  123291.000 -0.000693  ...           330.94158   \n",
       "871999  0.004572    1087.910  0.002706  ...           171.74612   \n",
       "\n",
       "        BTC_high-15_5min  BTC_low-15_5min  BTC_close-15_5min  \\\n",
       "0               0.003185         0.005859           0.004666   \n",
       "1              -0.016236        -0.008104          -0.010518   \n",
       "2              -0.002308        -0.000026          -0.000299   \n",
       "3              -0.007394        -0.005291          -0.005718   \n",
       "4               0.001813         0.004589           0.002865   \n",
       "...                  ...              ...                ...   \n",
       "871995          0.000519         0.005972           0.001657   \n",
       "871996         -0.002617         0.000289          -0.001885   \n",
       "871997         -0.015309        -0.012989          -0.013998   \n",
       "871998         -0.009130        -0.004610          -0.006787   \n",
       "871999          0.005346         0.007079           0.005519   \n",
       "\n",
       "        BTC_volume-15_5min  day  hour  minute  lunch_day  buy  \n",
       "0                133.82389    6    14      18        -51    0  \n",
       "1               1260.19146    4     9      39       -351    1  \n",
       "2                 62.24329    7     1      26       -616    1  \n",
       "3                 82.35837    6    22      30        572    0  \n",
       "4               1282.82839    3    21      22       -117    0  \n",
       "...                    ...  ...   ...     ...        ...  ...  \n",
       "871995           317.43590    1    13      20       -551    0  \n",
       "871996           385.63223    7     3      57       -223    1  \n",
       "871997           146.11996    1    12      23       -218    1  \n",
       "871998           542.04080    5    23      23       -621    1  \n",
       "871999           148.16977    2     2      59       -308    1  \n",
       "\n",
       "[872000 rows x 607 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index().drop(columns=\"num_index\")\n",
    "gc.collect()\n",
    "for i in range(1):\n",
    "    df = df.reindex(np.random.permutation(df.index)).reset_index().drop(columns=\"index\")\n",
    "    gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-14_5min</th>\n",
       "      <th>BTC_high-15_5min</th>\n",
       "      <th>BTC_low-15_5min</th>\n",
       "      <th>BTC_close-15_5min</th>\n",
       "      <th>BTC_volume-15_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.005725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>124.69049</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>133.82389</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>-51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137650</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>76980.00</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>38934.000</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>...</td>\n",
       "      <td>1093.82764</td>\n",
       "      <td>-0.016236</td>\n",
       "      <td>-0.008104</td>\n",
       "      <td>-0.010518</td>\n",
       "      <td>1260.19146</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>-351</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.427500</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>-0.000468</td>\n",
       "      <td>116.70</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>-0.003509</td>\n",
       "      <td>5661.100</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>...</td>\n",
       "      <td>47.73208</td>\n",
       "      <td>-0.002308</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000299</td>\n",
       "      <td>62.24329</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>-616</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.510300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2456.00</td>\n",
       "      <td>-0.000196</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2215.000</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>...</td>\n",
       "      <td>438.84442</td>\n",
       "      <td>-0.007394</td>\n",
       "      <td>-0.005291</td>\n",
       "      <td>-0.005718</td>\n",
       "      <td>82.35837</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>572</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.486200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1797.96432</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.004589</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>1282.82839</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>-117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871995</th>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>288.58946</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.005972</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>317.43590</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>-551</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871996</th>\n",
       "      <td>0.062602</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>34287.70</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>22344.100</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>...</td>\n",
       "      <td>281.76808</td>\n",
       "      <td>-0.002617</td>\n",
       "      <td>0.000289</td>\n",
       "      <td>-0.001885</td>\n",
       "      <td>385.63223</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>-223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871997</th>\n",
       "      <td>1.194575</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.001904</td>\n",
       "      <td>-0.000691</td>\n",
       "      <td>15462.00</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.006927</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>55672.000</td>\n",
       "      <td>0.003076</td>\n",
       "      <td>...</td>\n",
       "      <td>187.61722</td>\n",
       "      <td>-0.015309</td>\n",
       "      <td>-0.012989</td>\n",
       "      <td>-0.013998</td>\n",
       "      <td>146.11996</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>-218</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871998</th>\n",
       "      <td>0.061308</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>-0.000204</td>\n",
       "      <td>65270.00</td>\n",
       "      <td>-0.002161</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>-0.001509</td>\n",
       "      <td>123291.000</td>\n",
       "      <td>-0.000693</td>\n",
       "      <td>...</td>\n",
       "      <td>330.94158</td>\n",
       "      <td>-0.009130</td>\n",
       "      <td>-0.004610</td>\n",
       "      <td>-0.006787</td>\n",
       "      <td>542.04080</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>-621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871999</th>\n",
       "      <td>26.792500</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.004199</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>4353.17</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.006065</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>1087.910</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>...</td>\n",
       "      <td>171.74612</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.007079</td>\n",
       "      <td>0.005519</td>\n",
       "      <td>148.16977</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>-308</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>872000 rows × 607 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            price    high-1     low-1   close-1  volume-1    high-2     low-2  \\\n",
       "0        0.005725  0.000000  0.000000  0.000000      0.00  0.000000  0.000000   \n",
       "1        0.137650 -0.000363  0.001090 -0.000363  76980.00 -0.000363  0.000363   \n",
       "2        0.427500 -0.000468 -0.000468 -0.000468    116.70 -0.003509  0.002573   \n",
       "3        0.510300  0.000000  0.000588  0.000000   2456.00 -0.000196  0.000784   \n",
       "4        0.486200  0.000000  0.000000  0.000000      0.00  0.000000  0.000000   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "871995  31.000000  0.000000  0.000000  0.000000      0.00  0.000000  0.003226   \n",
       "871996   0.062602 -0.000439  0.001318  0.000200  34287.70  0.001318  0.002117   \n",
       "871997   1.194575 -0.001277  0.001904 -0.000691  15462.00  0.000565  0.006927   \n",
       "871998   0.061308 -0.001509 -0.000204 -0.000204  65270.00 -0.002161  0.000775   \n",
       "871999  26.792500  0.000467  0.004199  0.001213   4353.17  0.003826  0.006065   \n",
       "\n",
       "         close-2    volume-2    high-3  ...  BTC_volume-14_5min  \\\n",
       "0       0.000000       0.000  0.000000  ...           124.69049   \n",
       "1       0.000363   38934.000  0.000363  ...          1093.82764   \n",
       "2      -0.003509    5661.100  0.002105  ...            47.73208   \n",
       "3       0.000000    2215.000  0.000196  ...           438.84442   \n",
       "4       0.000000       0.000  0.000000  ...          1797.96432   \n",
       "...          ...         ...       ...  ...                 ...   \n",
       "871995  0.000000      17.036  0.000000  ...           288.58946   \n",
       "871996  0.001318   22344.100  0.001478  ...           281.76808   \n",
       "871997  0.001486   55672.000  0.003076  ...           187.61722   \n",
       "871998 -0.001509  123291.000 -0.000693  ...           330.94158   \n",
       "871999  0.004572    1087.910  0.002706  ...           171.74612   \n",
       "\n",
       "        BTC_high-15_5min  BTC_low-15_5min  BTC_close-15_5min  \\\n",
       "0               0.003185         0.005859           0.004666   \n",
       "1              -0.016236        -0.008104          -0.010518   \n",
       "2              -0.002308        -0.000026          -0.000299   \n",
       "3              -0.007394        -0.005291          -0.005718   \n",
       "4               0.001813         0.004589           0.002865   \n",
       "...                  ...              ...                ...   \n",
       "871995          0.000519         0.005972           0.001657   \n",
       "871996         -0.002617         0.000289          -0.001885   \n",
       "871997         -0.015309        -0.012989          -0.013998   \n",
       "871998         -0.009130        -0.004610          -0.006787   \n",
       "871999          0.005346         0.007079           0.005519   \n",
       "\n",
       "        BTC_volume-15_5min  day  hour  minute  lunch_day  buy  \n",
       "0                133.82389    6    14      18        -51    0  \n",
       "1               1260.19146    4     9      39       -351    1  \n",
       "2                 62.24329    7     1      26       -616    1  \n",
       "3                 82.35837    6    22      30        572    0  \n",
       "4               1282.82839    3    21      22       -117    0  \n",
       "...                    ...  ...   ...     ...        ...  ...  \n",
       "871995           317.43590    1    13      20       -551    0  \n",
       "871996           385.63223    7     3      57       -223    1  \n",
       "871997           146.11996    1    12      23       -218    1  \n",
       "871998           542.04080    5    23      23       -621    1  \n",
       "871999           148.16977    2     2      59       -308    1  \n",
       "\n",
       "[872000 rows x 607 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f\"/UltimeTradingBot/Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df choosen data shape(872000, 607)\n",
      "pair: True\n",
      "174400\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"df choosen data shape\"+str(df.shape))\n",
    "print(f\"pair: {(df.shape[0]/2)==df.buy.sum()}\")\n",
    "dt=df.to_numpy(dtype=np.float32)\n",
    "#dt=df.to_numpy()\n",
    "dt=np.nan_to_num(dt,nan=0)\n",
    "#dt=dt.astype(np.float32)\n",
    "dt=np.nan_to_num(dt, neginf=0) \n",
    "dt=np.nan_to_num(dt, posinf=0) \n",
    "\n",
    "index_20pct= int(0.2*len(dt[:,0]))\n",
    "print(index_20pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feather loading\n",
    "df=pd.read_feather(f\"/gdrive/UltimeTradingBot/Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n",
    "# dt=df.to_numpy(dtype=np.float32)\n",
    "# dt=fixdt(dt)\n",
    "# index_20pct= int(0.2*len(dt[:,0]))\n",
    "# gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Normalized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Normalzed Model\n",
    "IN_DIM=dt.shape[1]-1\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(250),activation='relu')) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20),activation='relu')) \n",
    "model.add(Dense(int(50),activation='relu')) \n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "callbacks_a = ModelCheckpoint(filepath =Model_FileName,monitor ='val_accuracy',save_best_only = True, save_weights = True)\n",
    "callbacks_b = EarlyStopping(monitor ='val_accuracy',mode='auto',patience=15,verbose=1)\n",
    "print(\"saving file in: \"+Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, 0:-1],\n",
    "                dt[index_20pct:,-1],\n",
    "                validation_data=(dt[:index_20pct, :-1],dt[:index_20pct,-1]),\n",
    "                epochs=6000,\n",
    "                batch_size=256*10,\n",
    "                callbacks=[callbacks_a,callbacks_b])\n",
    "\n",
    "print('##########################################################################')\n",
    "print(f\"------val_accuracy-----> {'{0:.4g}'.format(max(history.history['val_accuracy'])*100)} | {'{0:.4g}'.format(max(history.history['accuracy'])*100)} <----------accuracy----------\")\n",
    "print(Normalization_File)\n",
    "print(Model_FileName)\n",
    "model_init_file=Model_FileName.replace(f\"_v{VERSION}\", \"_vInit\")\n",
    "print(f\"save to: {model_init_file}\")\n",
    "model.save(model_init_file)\n",
    "model_init=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_init_file=Model_FileName.replace(f\"_v{VERSION}\", \"_vInit\")\n",
    "# print(f\"save to: {model_init_file}\")\n",
    "# model.save(model_init_file)\n",
    "# model_init=model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Model Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Binary Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.utils import to_categorical\n",
    "# from keras.utils.vis_utils import plot_model\n",
    "# from tensorflow.keras.layers.experimental.preprocessing import BinaryQuantization\n",
    "\n",
    "# # Define the class weights\n",
    "# class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# SizeTunner = 1\n",
    "# IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "# model = Sequential()\n",
    "\n",
    "# model.add(BinaryQuantization(input_shape=(IN_DIM,)))\n",
    "\n",
    "# model.add(Dense(int(300 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(200 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(80 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(int(20 * SizeTunner)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(BinaryQuantization())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "# ]\n",
    "\n",
    "# print(\"saving file in: \" + Model_FileName)\n",
    "# history = model.fit(dt[index_20pct:, :-1],\n",
    "#                     dt[index_20pct:, -1],\n",
    "#                     validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "#                     epochs=500,\n",
    "#                     batch_size=256*10,\n",
    "#                     callbacks=callbacks,\n",
    "#                     class_weight=class_weights)\n",
    "# # Save the model\n",
    "# binary_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_BINARY.h5\"\n",
    "# model.save(binary_model_file)\n",
    "# binary_model_file=tf.keras.models.load_model(binary_model_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini_range_start=0\n",
    "# mini_range_stop=200000\n",
    "# model.evaluate(dt[mini_range_start:mini_range_stop,:-1],dt[mini_range_start:mini_range_stop,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-  Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USED_MODEL=very_deep_model\n",
    "#model_init=model\n",
    "#USED_MODEL=model_init#load_model(\"/UltimeTradingBot/Data/BUY_UP_CLOSE/tp60_w6_max3min_Model_GoodVeryDeep.h5\")\n",
    "Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "prediction2=Prediction_Note.round()\n",
    "hp(prediction2[:,0].mean())\n",
    "PesemisticPrediction=(Prediction_Note[:,0]-0.49).round()\n",
    "hp(PesemisticPrediction.mean())\n",
    "Y=dt[:,-1].copy()\n",
    "Pred01=prediction2[:,-1]\n",
    "Original_Traget_Data=Y\n",
    "Predicted_Data=Pred01\n",
    "\n",
    "TruePred=((Original_Traget_Data==Predicted_Data)).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN v1\n",
    "import gc\n",
    "from keras.layers import Conv1D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(int(300 * SizeTunner), kernel_size=3, activation='elu', padding='same', input_shape=(IN_DIM, 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(int(200 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(int(80 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(int(80 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "\n",
    "# Reshape the input data to have a single channel\n",
    "X_train = dt[index_20pct:, :-1].reshape(-1, IN_DIM, 1)\n",
    "y_train = dt[index_20pct:, -1]\n",
    "X_val = dt[:index_20pct, :-1].reshape(-1, IN_DIM, 1)\n",
    "y_val = dt[:index_20pct, -1]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "cnn1_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_cnn1.h5\"\n",
    "model.save(cnn1_model_file)\n",
    "print(cnn1_model_file)\n",
    "cnn1_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_cnn1.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN v2:\n",
    "import gc\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 0.5\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM, 1)))\n",
    "model.add(Conv1D(int(150 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(int(100 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(int(40 * SizeTunner), kernel_size=3, activation='elu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling1D(pool_size=2))\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(int(40 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(40 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(40 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(10 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = RMSprop(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "\n",
    "X_train = dt[25000:, :-1].reshape(-1, IN_DIM, 1)\n",
    "y_train = dt[25000:, -1]\n",
    "X_val = dt[:25000, :-1].reshape(-1, IN_DIM, 1)\n",
    "y_val = dt[:25000, -1]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=128*100,\n",
    "                    callbacks=callbacks,\n",
    "                    )\n",
    "\n",
    "cnn2_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_cnn2.h5\"\n",
    "model.save(cnn2_model_file)\n",
    "print(cnn2_model_file)\n",
    "cnn2_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_cnn2.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINI FFNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "\n",
    "X_train = dt[index_20pct:, :-1]\n",
    "y_train = dt[index_20pct:, -1]\n",
    "X_val = dt[:index_20pct, :-1]\n",
    "y_val = dt[:index_20pct, -1]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=100,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "ffnn_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_ffnn.h5\"\n",
    "model.save(ffnn_model_file)\n",
    "print(ffnn_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(dt[:,:-1],dt[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests and Gradient Boosting Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBOOST:\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare the data\n",
    "X = dt[:, :-1]\n",
    "y = dt[:, -1]\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1000, random_state=42)\n",
    "\n",
    "# Create the XGBoost classifier\n",
    "model = xgb.XGBClassifier(n_estimators=500,\n",
    "                          max_depth=5,\n",
    "                          learning_rate=0.1,\n",
    "                          subsample=0.8,\n",
    "                          colsample_bytree=0.8,\n",
    "                          gamma=0.1,\n",
    "                          random_state=42,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, early_stopping_rounds=10, eval_set=[(X_val, y_val)], verbose=True)\n",
    "\n",
    "# Predict the validation set\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "# Save the model\n",
    "xgb_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_xgb.pkl\"\n",
    "model.save_model(xgb_model_file)\n",
    "print(xgb_model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LightGBM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANTI Retrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_dt=dt[TruePred]\n",
    "# good_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bad_dt=dt[ np.logical_not(TruePred)]\n",
    "bad_dt=dt[Predicted_Data==1 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anti prediction\n",
    "\n",
    "BadONE=bad_dt[bad_dt[:,-1]==0]\n",
    "TrueOne=bad_dt[bad_dt[:,-1]==1][:BadONE.shape[0]]\n",
    "AntiPrediction_DT=np.concatenate((BadONE,TrueOne),axis=0)\n",
    "np.random.shuffle(AntiPrediction_DT)\n",
    "\n",
    "retrain_dt=AntiPrediction_DT\n",
    "print(f\"Dataset Size is : {retrain_dt.shape[0]}\")\n",
    "class_1_weight=hp(retrain_dt[:,-1].mean())/100\n",
    "\n",
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "index_20pct=int(retrain_dt.shape[1]*0.2)\n",
    "\n",
    "class_weights = {0: 1-class_1_weight, 1: class_1_weight}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='loss', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='loss', mode='auto', patience=20, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                    retrain_dt[index_20pct:, -1],\n",
    "                    validation_data=(retrain_dt[:index_20pct, :-1], retrain_dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*5,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "justgood_good_model=model\n",
    "justgood_good_model_wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Anti-Model_v2.h5'\n",
    "justgood_good_model.save(justgood_good_model_wheretosave)\n",
    "print(justgood_good_model_wheretosave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True PredONly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change retaindt\n",
    "for rrr in range(1,7):\n",
    "    retrain_dt=dt\n",
    "    class_1_weight=TrueWinPred.mean()\n",
    "\n",
    "    import gc\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import Nadam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    # Define the class weights\n",
    "    index_20pct=int(retrain_dt.shape[1]*0.2)\n",
    "    class_weights = {0: 1-class_1_weight, 1: class_1_weight}\n",
    "    gc.collect()\n",
    "\n",
    "    SizeTunner = 1\n",
    "    IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    # model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    # model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"saving file in: \" + Model_FileName)\n",
    "    history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                        TrueWinPred[index_20pct:],\n",
    "                        validation_data=(retrain_dt[:index_20pct, :-1], TrueWinPred[:index_20pct]),\n",
    "                        epochs=500,\n",
    "                        batch_size=256*5,\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weights)\n",
    "\n",
    "    #868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "    #Results after 380 min\n",
    "    # Epoch 133/500\n",
    "    # 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "    # Epoch 134/500\n",
    "    # 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "    # Epoch 134: early stopping\n",
    "\n",
    "    true_win_model=model\n",
    "    wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re{rrr}.h5\"\n",
    "    true_win_model.save(wheretosave)\n",
    "    print(wheretosave)\n",
    "    USED_MODEL=true_win_model\n",
    "    bad_Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "    Pred02=bad_Prediction_Note.round()\n",
    "    Original_Traget_Data=Y\n",
    "    Predicted_Data=Pred02[:,0]\n",
    "\n",
    "    BadTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    BadModelAccuracy=hp(BadTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "    BadTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    BadTrueWinPred_Mean=hp(BadTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "    BadLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    BadLossPred_Mean=hp(BadLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "    BadMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    BadMissedDeal_Mean=hp(BadMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "    BadGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    BadGoodZero_Mean=hp(BadGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "    fiability=BadTrueWinPred_Mean + BadLossPred_Mean + BadMissedDeal_Mean + BadGoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=BadTrueWinPred_Mean/(BadLossPred_Mean+BadTrueWinPred_Mean)\n",
    "    print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "    ## for retraining again\n",
    "    TrueWinPred=BadTrueWinPred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win-Loss Double Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep neural network to predict a binary outcome (win or loss) and applying class weights to the loss function. To implement cost-sensitive learning or ensemble methods, you can make the following modifications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# # Custom cost-sensitive loss function\n",
    "# def cost_sensitive_loss(y_true, y_pred):\n",
    "#     cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    \n",
    "#     y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "#     y_pred_1_probs = 1 - y_pred_probs\n",
    "    \n",
    "#     y_true_int = tf.cast(y_true, tf.int32)\n",
    "#     cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "#     loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# # Function to create the model\n",
    "# def create_model(input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "#     model.add(Dense(200, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Prepare the data\n",
    "# retrain_dt = dt\n",
    "# index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "# X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "# y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "# # Define the optimizer and callbacks\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model for class 1 (win)\n",
    "# model_win = create_model(input_dim=IN_DIM)\n",
    "# model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "# model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Train the model for class 0 (loss)\n",
    "# model_loss = create_model(input_dim=IN_DIM)\n",
    "# model_loss.compile(optimizer=optimizer, loss=cost_sensitive_loss,metrics=['accuracy'])\n",
    "# model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Combine the predictions from both models\n",
    "# win_preds = model_win.predict(dt[:, 0:-1])\n",
    "# loss_preds = model_loss.predict(dt[:, 0:-1])\n",
    "\n",
    "# # Use a strategy such as averaging, voting, or another combination method\n",
    "# combined_preds = (win_preds + loss_preds) / 2\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Additional metrics\n",
    "# true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "# false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "# true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "# false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "# print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "# print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "# print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "# print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate\n",
    "# from keras.models import Model\n",
    "\n",
    "# # 1. Freeze the weights of model_win and model_loss\n",
    "# for layer in model_win.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# for layer in model_loss.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # 2. Add an additional layer to each model to obtain the intermediate features\n",
    "# model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "# model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# # 3. Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "# combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "# combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# # 4. Train the new model to make predictions using the intermediate features from both models\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the combined model\n",
    "# combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Custom cost-sensitive loss function\n",
    "def cost_sensitive_loss(y_true, y_pred):\n",
    "    cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    y_pred_1_probs = 1 - y_pred_probs\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "    loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "    model.add(Dense(200, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, loss_function):\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Prepare the data\n",
    "retrain_dt = dt\n",
    "index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "\n",
    "# Train the model for class 1 (win)\n",
    "model_win = create_model(input_dim=IN_DIM)\n",
    "model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Train the model for class 0 (loss)\n",
    "model_loss = create_model(input_dim=IN_DIM)\n",
    "model_loss.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# Freeze the weights of model_win and model_loss\n",
    "for layer in model_win.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model_loss.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add an additional layer to each model\n",
    "# Add an additional layer to each model to obtain the intermediate features\n",
    "model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# Train the new model to make predictions using the intermediate features from both models\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the combined model\n",
    "combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Make predictions using the combined model\n",
    "combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "# Create a new input layer for the single input\n",
    "single_input = Input(shape=(IN_DIM,))\n",
    "\n",
    "# Duplicate the input for the two original models\n",
    "input_duplication = Lambda(lambda x: tf.tile(tf.expand_dims(x, axis=1), [1, 2, 1]))(single_input)\n",
    "input_for_model_win = Lambda(lambda x: x[:, 0])(input_duplication)\n",
    "input_for_model_loss = Lambda(lambda x: x[:, 1])(input_duplication)\n",
    "\n",
    "# Feed the duplicated input into the original models\n",
    "model_win_output = model_win(input_for_model_win)\n",
    "model_loss_output = model_loss(input_for_model_loss)\n",
    "\n",
    "# Combine the outputs of the original models\n",
    "combined_input = concatenate([model_win_output, model_loss_output])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# Create the new model\n",
    "single_input_combined_model = Model(inputs=single_input, outputs=combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_preds = single_input_combined_model.predict(dt[:, 0:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very_deep_good_model=model\n",
    "wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_CombinedLOssWin.h5\"\n",
    "combined_model.save(wheretosave)\n",
    "print(wheretosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "very_deep_bad_model=model\n",
    "very_deep_bad_model.save(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_BadVeryDeep_v2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_expand5(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function\n",
    "    \"\"\"\n",
    "    This function takes in several parameters to calculate technical indicators and returns a merged dataframe.\n",
    "    \n",
    "    :param pair: str, default \"GMT/USDT\"\n",
    "        The trading pair to analyze.\n",
    "        \n",
    "    :param start_index: int, default 0\n",
    "        The start index for selecting data.\n",
    "        \n",
    "    :param end_index: int, default 10000\n",
    "        The end index for selecting data.\n",
    "    \n",
    "    :param window_size: int, default 2\n",
    "        The window size to use for analyzing the data.\n",
    "    \n",
    "    :param metadata: MetaData\n",
    "        The metadata to use for analyzing the data.\n",
    "    \n",
    "    :param high_weight: int, default 1\n",
    "        The weight to use for calculating the high.\n",
    "    \n",
    "    :param BUY_PCT: float, default BUY_PCT\n",
    "        The buy pct to use for analyzing the data.\n",
    "    \n",
    "    :param SELL_PCT: float, default SELL_PCT\n",
    "        The sell pct to use for analyzing the data.\n",
    "    \n",
    "    :param buy_fn: function, default buy_min_up\n",
    "        The buy function to use for analyzing the data.\n",
    "    \n",
    "    :return: pd.DataFrame\n",
    "        A merged dataframe containing the calculated technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"mini_expand : {pair}\")\n",
    "    # Select data\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    # Calculate technical indicators\n",
    "    pair_full = full_expand(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair], window_size)\n",
    "    btc_full = full_expand(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], window_size)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    open_high_low_close_cols = merged.columns.str.contains(\"open|high|low|close\")\n",
    "    # merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"BTC_price\"] - merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")]) / merged[\"BTC_price\"]\n",
    "    # )\n",
    "    # merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"price\"] - merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")]) / merged[\"price\"]\n",
    "    # )\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  mini_expand5 {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test On special coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Data\n",
    "BAD_PERIOD_START=\"2022-08-30\"\n",
    "BAD_PERIOD_END=\"2022-11-22\"\n",
    "pair_to_test=\"GMT/USDT\"\n",
    "MAX_FORCAST_SIZE=120\n",
    "USED_MODEL=very_deep_good_model#true_win_model#model_init #model_good_x3 #very_deep_good_model 16/1.7\n",
    "\n",
    "BUY_PCT_TEST=0.45\n",
    "loc_start=0\n",
    "loc_end=1000000\n",
    "\n",
    "\n",
    "i_start=71000\n",
    "i_end=i_start+200\n",
    "\n",
    "# loc_start=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_START))\n",
    "# loc_end=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_END))\n",
    "\n",
    "\n",
    "OnePair_DF=mini_expand5(        pair=pair_to_test,\n",
    "                                i=loc_start,j=loc_end,\n",
    "                                window=WINDOW_SIZE,\n",
    "                                metadata=MetaData,\n",
    "                                high_weight=1,\n",
    "                                BUY_PCT=BUY_PCT_TEST,\n",
    "                                SELL_PCT=SELL_PCT,\n",
    "                                buy_function=buy_test#buy_test\n",
    "                        )\n",
    "OnePair_DT=OnePair_DF.to_numpy()\n",
    "gc.collect()\n",
    "OnePair_DT=fixdt(OnePair_DT)\n",
    "print(OnePair_DT[0,0] == OnePair_DF.iloc[0,0])\n",
    "print(OnePair_DT[5,5] == OnePair_DF.iloc[5,5])\n",
    "hp(OnePair_DF.buy.mean(),\"Buy mean pct\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(0, 10, 500)\n",
    "dashes = [10, 5, 100, 5]  # 10 points on, 5 off, 100 on, 5 off\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "                 label='Dashes set retroactively')\n",
    "line1.set_dashes(dashes)\n",
    "plt.plot(OnePair_DF[i_start:i_end][OnePair_DF.buy[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][OnePair_DF.buy[i_start:i_end]==1].price, 'ro')\n",
    "\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "OnePair_Pred=OnePair_PredNote.round()\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "Original_Traget_Data=OnePair_DT[:,-1]\n",
    "Predicted_Data=OnePair_Pred[:,0]\n",
    "gc.collect()\n",
    "TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "gc.collect()\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "gc.collect()\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "gc.collect()\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "gc.collect()\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "gc.collect()\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "x = np.linspace(0, 10, 500)\n",
    "dashes = [10, 5, 100, 5]  # 10 points on, 5 off, 100 on, 5 off\n",
    "fig, ax = plt.subplots()\n",
    "line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "                 label='price')\n",
    "line1.set_dashes(dashes)\n",
    "plt.plot(OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].price, 'ro')\n",
    "\n",
    "\n",
    "ax.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "precision=0.0\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "Predicted_Data=OnePair_Pred[:300000,0]\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(XX,YY,USEDMODEL)\n",
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
