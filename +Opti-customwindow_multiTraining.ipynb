{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single AI crypto concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xdata_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_optimal_5m(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=MAX_FORCAST_SIZE):\n",
    "    #df = df.fillna(0)\n",
    "    mino = BUY_PCT / 100.0\n",
    "    maxo = SELL_PCT / 100.0\n",
    "    window=max(7,int(window/5))\n",
    "    max_forecast_size=window#MAX_FORCAST_SIZE\n",
    "    try:\n",
    "        after_dip_val=AFTER_MARK\n",
    "    except:\n",
    "        after_dip_val=3\n",
    "    print(f\"after mark = : {after_dip_val}\")\n",
    "    try:\n",
    "        print(f\"optimalbuy buy maximum forcast size={max_forecast_size} at {BUY_PCT}% of the current price \")\n",
    "    except:\n",
    "        max_forecast_size = 3\n",
    "        print(\"optimalbuy buy default window=3\")\n",
    "        \n",
    "    rolling_max_close_diff = ((df['close-1_5min'].rolling(window=window).max().shift(-window+1) / df['close']) - 1).fillna(0)\n",
    "    df['buy']=(rolling_max_close_diff >= mino).astype(int)\n",
    "    \n",
    "    # Compute rolling minimum values\n",
    "    window_list=[window]#[3, 5, 7, 10, 15, 20]\n",
    "    \n",
    "    for window_size in window_list:\n",
    "        col_name = f'ismin{window_size}'\n",
    "        rolling_min = (df['close'].shift(after_dip_val) <= df.shift(-window_size-1)['close-1_5min'].rolling(2*window_size).min())\n",
    "        df = df.assign(**{col_name: rolling_min.astype(int)})\n",
    "\n",
    "    df['ismin'] = df[[f'ismin{window_size}' for window_size in window_list ]].any(axis=1).astype(int)        \n",
    "\n",
    "    # # Compute buy and sell signals\n",
    "    rolling_low_close_diff =  ((df['low-1_5min'].rolling(window=int(window/2)).min().shift(-int(window/2)+1)/ df['close'] ) -1).fillna(0)\n",
    "    df['sell'] = (rolling_low_close_diff <= -maxo).astype(int)\n",
    "\n",
    "    \n",
    "    # Compute final buy signal\n",
    "    df['buy'] = ((df['buy'] == 1) & (df['sell'] == 0) & (df['ismin'] == 1)).astype(int)\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=['sell', 'ismin'] + [f'ismin{window_size}' for window_size in window_list], errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def full_expand_costum(df1m,df5m,df15m,df1h,df1d,w1m=10,w5m=30,w15m=30,w1h=3,w1d=7):\n",
    "    d1min=df1m.copy()\n",
    "    d1min=expand_previous(d1min,window=w1m).drop(columns=[\"volume\"])\n",
    "    d1min=rapid1d_expand(d1min,df1d,w1d)\n",
    "    d1min=rapid1h_expand(d1min,df1h,w1h)\n",
    "    d1min=rapid15m_expand(d1min,df15m,w15m)\n",
    "    d1min=rapid5m_expand(d1min,df5m,w5m)\n",
    "    return d1min\n",
    "\n",
    "def maxi_expand(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase,w1m=6,w5m=30,w15m=30,w1h=3,w1d=7,btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=30,btc_w1d=30):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function\n",
    "    \"\"\"\n",
    "    This function takes in several parameters to calculate technical indicators and returns a merged dataframe.\n",
    "    \n",
    "    :param pair: str, default \"GMT/USDT\"\n",
    "        The trading pair to analyze.\n",
    "        \n",
    "    :param start_index: int, default 0\n",
    "        The start index for selecting data.\n",
    "        \n",
    "    :param end_index: int, default 10000\n",
    "        The end index for selecting data.\n",
    "    \n",
    "    :param window_size: int, default 2\n",
    "        The window size to use for analyzing the data.\n",
    "    \n",
    "    :param metadata: MetaData\n",
    "        The metadata to use for analyzing the data.\n",
    "    \n",
    "    :param high_weight: int, default 1\n",
    "        The weight to use for calculating the high.\n",
    "    \n",
    "    :param BUY_PCT: float, default BUY_PCT\n",
    "        The buy pct to use for analyzing the data.\n",
    "    \n",
    "    :param SELL_PCT: float, default SELL_PCT\n",
    "        The sell pct to use for analyzing the data.\n",
    "    \n",
    "    :param buy_fn: function, default buy_min_up\n",
    "        The buy function to use for analyzing the data.\n",
    "    \n",
    "    :return: pd.DataFrame\n",
    "        A merged dataframe containing the calculated technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"maxi custum expend : {pair} with those parameters: w1m={w1m},w5m={w5m},w15m={w15m},w1h={w1h},w1d={w1d} btc_w1m={btc_w1m},btc_w5m={btc_w5m},btc_w15m={btc_w15m},btc_w1h={btc_w1h},btc_w1d={btc_w1d}\")\n",
    "    # Select data\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    # Calculate technical indicators\n",
    "    pair_full = full_expand_costum(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair],w1m=w1m,w5m=w5m,w15m=w15m,w1h=w1h,w1d=w1d)\n",
    "    btc_full = full_expand_costum(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], w1m=btc_w1m,w5m=btc_w5m,w15m=btc_w15m,w1h=btc_w1h,w1d=btc_w1d)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    #print(merged.columns)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    open_high_low_close_cols = merged.columns.str.contains(\"open|high|low|close\")\n",
    "    # merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"BTC_price\"] - merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")]) / merged[\"BTC_price\"]\n",
    "    # )\n",
    "    # merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"price\"] - merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")]) / merged[\"price\"]\n",
    "    # )\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  max expend {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_optimal(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=3):\n",
    "    #df = df.fillna(0)\n",
    "    mino = BUY_PCT / 100.0\n",
    "    maxo = SELL_PCT / 100.0\n",
    "    window=15\n",
    "    max_forecast_size=15#MAX_FORCAST_SIZE\n",
    "    try:\n",
    "        after_dip_val=1\n",
    "    except:\n",
    "        after_dip_val=1\n",
    "    print(f\"after mark = : {after_dip_val}\")\n",
    "    try:\n",
    "        print(f\"optimalbuy buy maximum forcast size={max_forecast_size} at {BUY_PCT}% of the current price \")\n",
    "    except:\n",
    "        max_forecast_size = 3\n",
    "        print(\"optimalbuy buy default window=3\")\n",
    "        \n",
    "    rolling_max_close_diff = ((df['close'].rolling(window=window).max().shift(-window+1) / df['close']) - 1).fillna(0)\n",
    "    df['buy']=(rolling_max_close_diff >= mino).astype(int)\n",
    "    \n",
    "    # Compute rolling minimum values\n",
    "    \n",
    "    window_list=[5,window]#[3, 5, 7, 10, 15, 20]\n",
    "    \n",
    "    for window_size in window_list:\n",
    "        col_name = f'ismin{window_size}'\n",
    "        rolling_min = (df['close'].shift(after_dip_val) <= df.shift(-window_size-1)['close'].rolling(2*window_size).min())\n",
    "        df = df.assign(**{col_name: rolling_min.astype(int)})\n",
    "\n",
    "    df['ismin'] = df[[f'ismin{window_size}' for window_size in window_list ]].any(axis=1).astype(int)        \n",
    "\n",
    "    # # Compute buy and sell signals\n",
    "    rolling_low_close_diff =  ((df['low'].rolling(window=int(window/2)).min().shift(-int(window/2)+1)/ df['close'] ) -1).fillna(0)\n",
    "    df['sell'] = (rolling_low_close_diff <= -maxo).astype(int)\n",
    "\n",
    "    \n",
    "    # Compute final buy signal\n",
    "    df['buy'] = ((df['buy'] == 1) & (df['sell'] == 0) & (df['ismin'] == 1)).astype(int)\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=['sell', 'ismin'] + [f'ismin{window_size}' for window_size in window_list], errors='ignore')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_local_min(df, BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_near_min_v1(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5, num_values=3):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum\n",
    "    or near the local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "    num_values (int): The number of values near the local minimum to include.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum and its neighbors.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum or near the local minimum\n",
    "    close_diff = np.abs(df['close'] - rolling_min_close)\n",
    "    threshold = close_diff.nsmallest(num_values+1).iloc[-1]\n",
    "    local_min = (close_diff <= threshold).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def is_near_min(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5, num_values=3):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum\n",
    "    or within the specified range before or after the local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "    num_values (int): The number of values before and after the local minimum to include.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum and its neighbors.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    # Include values before and after the local minimum\n",
    "    for i in range(1, num_values+1):\n",
    "        df['buy'] = df['buy'] | df['buy'].shift(-i) | df['buy'].shift(i)\n",
    "\n",
    "    # Fill any NaN values introduced by shifting with 0\n",
    "    df['buy'].fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_max_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=7, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close)\n",
    "\n",
    "    # Compute maximum price change over next `window` rows\n",
    "    # max_price = df['close'].rolling(window=window, min_periods=1).max()\n",
    "    # max_price_shifted = max_price.shift(-window)\n",
    "    # max_price_change = ((max_price_shifted - max_price)/max_price).fillna(0)\n",
    "    max_price = df['close'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (local_min & win).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_close_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "\n",
    "    max_price = df['close'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (win).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_high_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "    max_price = df['high'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (win).astype(int)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=maxi_expand(pair=pair,i=0,j=len(df_list1m[pair]),window=2,metadata=MetaData,BUY_PCT=1.7,SELL_PCT=0.3,buy_function=is_local_min)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special list if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binance_USDT_HALAL.index(\"ROSE/USDT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chking import\n",
    "MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUY_MODE==\"BUY_ONLY\":\n",
    "    buy_function=buy_up_only\n",
    "elif BUY_MODE==\"BUY_UP\":\n",
    "    buy_function=buy_up\n",
    "elif  BUY_MODE==\"BUY_DIP\":\n",
    "    buy_function=buy_min_up\n",
    "elif  BUY_MODE==\"AFTER_DEPTH\":\n",
    "    buy_function=buy_after_depth\n",
    "elif  BUY_MODE==\"BUY_UP_CLOSE\":\n",
    "    buy_function=buy_up_close\n",
    "elif  BUY_MODE==\"AFTER_DEPTH_CLOSE\":\n",
    "    buy_function=buy_after_depth_close\n",
    "elif  BUY_MODE==\"BUY_TEST\":\n",
    "    buy_function=buy_test\n",
    "elif BUY_MODE==\"BUY_MIN_CLOSE\":\n",
    "    buy_function=buy_min_close\n",
    "elif  BUY_MODE==\"SELL_TEST\":\n",
    "    buy_function=sell_test\n",
    "elif  BUY_MODE==\"BUY_FIX\":\n",
    "    buy_function=buy_fix\n",
    "elif  BUY_MODE==\"BUY_OPTIMAL\":\n",
    "    buy_function=buy_optimal\n",
    "elif  BUY_MODE==\"IS_MIN\":\n",
    "    buy_function=is_local_min\n",
    "elif  BUY_MODE==\"IS_HIGH\":\n",
    "    buy_function=is_high_win\n",
    "elif  BUY_MODE==\"IS_CLOSE\":\n",
    "    buy_function=is_close_win\n",
    "try:\n",
    "    os.mkdir(DATA_DIR, mode = 0o777)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(f\"Results dir: {DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precent Mean: 1.542%\n",
      "######################  max expend SC/USDT - shape (455139, 499)  buy mean : 1.542 ############################\n",
      "df original shape (455139, 499)\n",
      "df original shape buy mean : 1.5421662393246898\n",
      "SC/USDT is processed -- 84/112\n",
      "working on: CKB/USDT -->maxi custum expend : CKB/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.401%\n",
      "######################  max expend CKB/USDT - shape (455139, 499)  buy mean : 1.401 ############################\n",
      "df original shape (455139, 499)\n",
      "df original shape buy mean : 1.4011104300005053\n",
      "CKB/USDT is processed -- 85/112\n",
      "working on: TOMO/USDT -->maxi custum expend : TOMO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.930%\n",
      "######################  max expend TOMO/USDT - shape (455140, 499)  buy mean : 1.93 ############################\n",
      "df original shape (455140, 499)\n",
      "df original shape buy mean : 1.9303950432833852\n",
      "TOMO/USDT is processed -- 86/112\n",
      "working on: STX/USDT -->maxi custum expend : STX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.082%\n",
      "######################  max expend STX/USDT - shape (455140, 499)  buy mean : 2.082 ############################\n",
      "df original shape (455140, 499)\n",
      "df original shape buy mean : 2.081996748253285\n",
      "STX/USDT is processed -- 87/112\n",
      "working on: FLUX/USDT -->maxi custum expend : FLUX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.354%\n",
      "######################  max expend FLUX/USDT - shape (455141, 499)  buy mean : 4.354 ############################\n",
      "df original shape (455141, 499)\n",
      "df original shape buy mean : 4.354255055026904\n",
      "FLUX/USDT is processed -- 88/112\n",
      "working on: DNT/USDT -->maxi custum expend : DNT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.886%\n",
      "######################  max expend DNT/USDT - shape (416700, 499)  buy mean : 3.886 ############################\n",
      "df original shape (416700, 499)\n",
      "df original shape buy mean : 3.886009119270458\n",
      "DNT/USDT is processed -- 89/112\n",
      "working on: ORN/USDT -->maxi custum expend : ORN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.387%\n",
      "######################  max expend ORN/USDT - shape (455142, 499)  buy mean : 2.387 ############################\n",
      "df original shape (455142, 499)\n",
      "df original shape buy mean : 2.386727658620826\n",
      "ORN/USDT is processed -- 90/112\n",
      "working on: PLA/USDT -->maxi custum expend : PLA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.473%\n",
      "######################  max expend PLA/USDT - shape (455143, 499)  buy mean : 2.473 ############################\n",
      "df original shape (455143, 499)\n",
      "df original shape buy mean : 2.472849192451603\n",
      "PLA/USDT is processed -- 91/112\n",
      "working on: BADGER/USDT -->maxi custum expend : BADGER/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.511%\n",
      "######################  max expend BADGER/USDT - shape (455143, 499)  buy mean : 2.511 ############################\n",
      "df original shape (455143, 499)\n",
      "df original shape buy mean : 2.511078935631219\n",
      "BADGER/USDT is processed -- 92/112\n",
      "working on: DF/USDT -->maxi custum expend : DF/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.240%\n",
      "######################  max expend DF/USDT - shape (455144, 499)  buy mean : 4.24 ############################\n",
      "df original shape (455144, 499)\n",
      "df original shape buy mean : 4.239537377181727\n",
      "DF/USDT is processed -- 93/112\n",
      "working on: MOB/USDT -->maxi custum expend : MOB/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.741%\n",
      "######################  max expend MOB/USDT - shape (285225, 499)  buy mean : 1.741 ############################\n",
      "df original shape (285225, 499)\n",
      "df original shape buy mean : 1.740731001840652\n",
      "MOB/USDT is processed -- 94/112\n",
      "working on: LPT/USDT -->maxi custum expend : LPT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.944%\n",
      "######################  max expend LPT/USDT - shape (455145, 499)  buy mean : 1.944 ############################\n",
      "df original shape (455145, 499)\n",
      "df original shape buy mean : 1.9444352898526844\n",
      "LPT/USDT is processed -- 95/112\n",
      "working on: SCRT/USDT -->maxi custum expend : SCRT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.749%\n",
      "######################  max expend SCRT/USDT - shape (426346, 499)  buy mean : 1.749 ############################\n",
      "df original shape (426346, 499)\n",
      "df original shape buy mean : 1.749283445839764\n",
      "SCRT/USDT is processed -- 96/112\n",
      "working on: RAD/USDT -->maxi custum expend : RAD/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.625%\n",
      "######################  max expend RAD/USDT - shape (621579, 499)  buy mean : 4.625 ############################\n",
      "df original shape (621579, 499)\n",
      "df original shape buy mean : 4.624512732894773\n",
      "RAD/USDT is processed -- 97/112\n",
      "working on: NMR/USDT -->maxi custum expend : NMR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.385%\n",
      "######################  max expend NMR/USDT - shape (455147, 499)  buy mean : 3.385 ############################\n",
      "df original shape (455147, 499)\n",
      "df original shape buy mean : 3.3850602113163437\n",
      "NMR/USDT is processed -- 98/112\n",
      "working on: ELF/USDT -->maxi custum expend : ELF/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.166%\n",
      "######################  max expend ELF/USDT - shape (455148, 499)  buy mean : 2.166 ############################\n",
      "df original shape (455148, 499)\n",
      "df original shape buy mean : 2.1656691889231636\n",
      "ELF/USDT is processed -- 99/112\n",
      "working on: TORN/USDT -->maxi custum expend : TORN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.142%\n",
      "######################  max expend TORN/USDT - shape (444420, 499)  buy mean : 5.142 ############################\n",
      "df original shape (444420, 499)\n",
      "df original shape buy mean : 5.142207821430179\n",
      "TORN/USDT is processed -- 100/112\n",
      "working on: T/USDT -->maxi custum expend : T/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.914%\n",
      "######################  max expend T/USDT - shape (375949, 499)  buy mean : 1.914 ############################\n",
      "df original shape (375949, 499)\n",
      "df original shape buy mean : 1.9143554045894524\n",
      "T/USDT is processed -- 101/112\n",
      "working on: QUICK/USDT -->maxi custum expend : QUICK/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.137%\n",
      "######################  max expend QUICK/USDT - shape (455150, 499)  buy mean : 3.137 ############################\n",
      "df original shape (455150, 499)\n",
      "df original shape buy mean : 3.1369878062177303\n",
      "QUICK/USDT is processed -- 102/112\n",
      "working on: LSK/USDT -->maxi custum expend : LSK/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.149%\n",
      "######################  max expend LSK/USDT - shape (455150, 499)  buy mean : 2.149 ############################\n",
      "df original shape (455150, 499)\n",
      "df original shape buy mean : 2.1491815884873122\n",
      "LSK/USDT is processed -- 103/112\n",
      "working on: FIDA/USDT -->maxi custum expend : FIDA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.590%\n",
      "######################  max expend FIDA/USDT - shape (455151, 499)  buy mean : 3.59 ############################\n",
      "df original shape (455151, 499)\n",
      "df original shape buy mean : 3.590237086153826\n",
      "FIDA/USDT is processed -- 104/112\n",
      "working on: XNO/USDT -->maxi custum expend : XNO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.092%\n",
      "######################  max expend XNO/USDT - shape (416272, 499)  buy mean : 2.092 ############################\n",
      "df original shape (416272, 499)\n",
      "df original shape buy mean : 2.0919014490525427\n",
      "XNO/USDT is processed -- 105/112\n",
      "working on: BTG/USDT -->maxi custum expend : BTG/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.645%\n",
      "######################  max expend BTG/USDT - shape (416700, 499)  buy mean : 2.645 ############################\n",
      "df original shape (416700, 499)\n",
      "df original shape buy mean : 2.6450683945284377\n",
      "BTG/USDT is processed -- 106/112\n",
      "working on: GHST/USDT -->maxi custum expend : GHST/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 0.119%\n",
      "######################  max expend GHST/USDT - shape (455153, 499)  buy mean : 0.119 ############################\n",
      "df original shape (455153, 499)\n",
      "df original shape buy mean : 0.11930054289436738\n",
      "GHST/USDT is processed -- 107/112\n",
      "working on: EPS/USDT -->maxi custum expend : EPS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.522%\n",
      "######################  max expend EPS/USDT - shape (174480, 499)  buy mean : 4.522 ############################\n",
      "df original shape (174480, 499)\n",
      "df original shape buy mean : 4.522008253094911\n",
      "EPS/USDT is processed -- 108/112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf=pd.DataFrame()\n",
    "count=0\n",
    "row_numbers=10000\n",
    "for pair in pair_list[:]:\n",
    "    if pair != \"BTC/USDT\" and pair != \"EUR/USDT\" and pair != \"ETH/USDT\" :\n",
    "        print(\"working on: \"+pair ,end=\" -->\")\n",
    "        try:\n",
    "            \n",
    "            df=maxi_expand(pair=pair,i=0,j=len(df_list1m[pair]),window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function,\n",
    "                           w1m=6,w5m=10,w15m=50,w1h=8,w1d=7,\n",
    "                           btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15)\n",
    "            print(\"df original shape \"+str(df.shape))\n",
    "            print(f\"df original shape buy mean : {df.buy.mean()*100}\")\n",
    "            df=df.reset_index()\n",
    "            try:df.pop(\"num_index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"date\")\n",
    "            except: pass\n",
    "            df=data_shufler(df)            \n",
    "            #df=data_chooser(df,weight=50,row_numbers=df.buy.sum()*2)\n",
    "            df=data_chooser50(df,row_numbers=row_numbers)\n",
    "            gc.collect()\n",
    "            df=data_cleanup(df)\n",
    "            df=df.dropna()\n",
    "            print(pair+f\" is processed -- {count}/{len(pair_list)}\")\n",
    "            # print(df.iloc[0:1])\n",
    "        except Exception as e:\n",
    "            print(f\"error while processing {pair} {count}/{len(pair_list)}\")\n",
    "            print(e)\n",
    "        xdf=pd.concat([xdf,df],axis=0)\n",
    "        count+=1\n",
    "        del(df)\n",
    "        gc.collect()\n",
    "df=xdf\n",
    "del xdf\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-3_5min</th>\n",
       "      <th>BTC_high-4_5min</th>\n",
       "      <th>BTC_low-4_5min</th>\n",
       "      <th>BTC_close-4_5min</th>\n",
       "      <th>BTC_volume-4_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.361925</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>5661.000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>1097.00</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>...</td>\n",
       "      <td>363.67607</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>339.21955</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456675</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002026</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>3969.40</td>\n",
       "      <td>-0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>571.37969</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>-0.000966</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>698.47740</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>-750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.532500</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>5369.550</td>\n",
       "      <td>-0.007537</td>\n",
       "      <td>-0.002452</td>\n",
       "      <td>-0.003178</td>\n",
       "      <td>8853.35</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>...</td>\n",
       "      <td>373.95757</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>202.10288</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.740000</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>...</td>\n",
       "      <td>956.45047</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>2368.87006</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>-551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.910</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>...</td>\n",
       "      <td>274.63024</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>99.90263</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089995</th>\n",
       "      <td>1.458000</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>13.700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>137.46733</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>128.18419</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089996</th>\n",
       "      <td>0.518500</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>789.000</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>7363.00</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>...</td>\n",
       "      <td>85.34676</td>\n",
       "      <td>-0.000913</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>179.62893</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>-632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089997</th>\n",
       "      <td>3.580000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>443.171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>595.71385</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>394.32062</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>-695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089998</th>\n",
       "      <td>0.087800</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>9349.00</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>...</td>\n",
       "      <td>554.73769</td>\n",
       "      <td>-0.004571</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>353.68536</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>-632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089999</th>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>496.300</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>2274.30</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>...</td>\n",
       "      <td>104.05384</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>233.55521</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1090000 rows Ã— 499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             price    high-1     low-1   close-1  volume-1    high-2  \\\n",
       "0         0.361925  0.000345  0.001174  0.000345  5661.000  0.000898   \n",
       "1         0.456675  0.000602  0.000602  0.000602     0.000 -0.002026   \n",
       "2        27.532500 -0.004631 -0.000636 -0.000999  5369.550 -0.007537   \n",
       "3        25.740000 -0.000389 -0.000389 -0.000389     0.000 -0.000389   \n",
       "4         3.130000  0.000000  0.000000  0.000000    49.910 -0.006390   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1089995   1.458000 -0.000686 -0.000686 -0.000686    13.700  0.000000   \n",
       "1089996   0.518500 -0.001929 -0.001157 -0.001929   789.000 -0.003857   \n",
       "1089997   3.580000  0.000000  0.000000  0.000000   443.171  0.000000   \n",
       "1089998   0.087800 -0.003417 -0.003417 -0.003417     0.000 -0.003417   \n",
       "1089999   0.347000  0.000000  0.005764  0.002882   496.300  0.002882   \n",
       "\n",
       "            low-2   close-2  volume-2    high-3  ...  BTC_volume-3_5min  \\\n",
       "0        0.001174  0.001174   1097.00  0.000898  ...          363.67607   \n",
       "1        0.001259  0.000602   3969.40 -0.003996  ...          571.37969   \n",
       "2       -0.002452 -0.003178   8853.35 -0.008626  ...          373.95757   \n",
       "3       -0.000389 -0.000389      1.00 -0.000389  ...          956.45047   \n",
       "4       -0.006390 -0.006390      0.00 -0.006390  ...          274.63024   \n",
       "...           ...       ...       ...       ...  ...                ...   \n",
       "1089995  0.000000  0.000000      0.00  0.000000  ...          137.46733   \n",
       "1089996 -0.001157 -0.001157   7363.00 -0.005786  ...           85.34676   \n",
       "1089997  0.000000  0.000000      0.00  0.000000  ...          595.71385   \n",
       "1089998 -0.002278 -0.003417   9349.00  0.004556  ...          554.73769   \n",
       "1089999  0.005764  0.005764   2274.30  0.002882  ...          104.05384   \n",
       "\n",
       "         BTC_high-4_5min  BTC_low-4_5min  BTC_close-4_5min  BTC_volume-4_5min  \\\n",
       "0              -0.001092        0.000308         -0.000394          339.21955   \n",
       "1              -0.002375       -0.000966         -0.001691          698.47740   \n",
       "2               0.002393        0.005051          0.004128          202.10288   \n",
       "3              -0.002065        0.001875          0.001213         2368.87006   \n",
       "4               0.001559        0.002950          0.002462           99.90263   \n",
       "...                  ...             ...               ...                ...   \n",
       "1089995         0.000881        0.002906          0.000881          128.18419   \n",
       "1089996        -0.000913        0.002724         -0.000176          179.62893   \n",
       "1089997         0.000002        0.001065          0.000311          394.32062   \n",
       "1089998        -0.004571        0.003279          0.003078          353.68536   \n",
       "1089999         0.007901        0.010933          0.010310          233.55521   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          5     4      52        193    0  \n",
       "1          5     2      56       -750    0  \n",
       "2          4    15      13        348    1  \n",
       "3          3     5      32       -551    1  \n",
       "4          3     4       6       -426    1  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "1089995    6     6      57        131    0  \n",
       "1089996    3    19      33       -632    0  \n",
       "1089997    5    20      37       -695    0  \n",
       "1089998    1    14      17       -632    1  \n",
       "1089999    7    18      55         68    1  \n",
       "\n",
       "[1090000 rows x 499 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index().drop(columns=\"num_index\")\n",
    "gc.collect()\n",
    "for i in range(1):\n",
    "    df = df.reindex(np.random.permutation(df.index)).reset_index().drop(columns=\"index\")\n",
    "    gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-3_5min</th>\n",
       "      <th>BTC_high-4_5min</th>\n",
       "      <th>BTC_low-4_5min</th>\n",
       "      <th>BTC_close-4_5min</th>\n",
       "      <th>BTC_volume-4_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.361925</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000345</td>\n",
       "      <td>5661.000</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>1097.00</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>...</td>\n",
       "      <td>363.67607</td>\n",
       "      <td>-0.001092</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>339.21955</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.456675</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.002026</td>\n",
       "      <td>0.001259</td>\n",
       "      <td>0.000602</td>\n",
       "      <td>3969.40</td>\n",
       "      <td>-0.003996</td>\n",
       "      <td>...</td>\n",
       "      <td>571.37969</td>\n",
       "      <td>-0.002375</td>\n",
       "      <td>-0.000966</td>\n",
       "      <td>-0.001691</td>\n",
       "      <td>698.47740</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>-750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27.532500</td>\n",
       "      <td>-0.004631</td>\n",
       "      <td>-0.000636</td>\n",
       "      <td>-0.000999</td>\n",
       "      <td>5369.550</td>\n",
       "      <td>-0.007537</td>\n",
       "      <td>-0.002452</td>\n",
       "      <td>-0.003178</td>\n",
       "      <td>8853.35</td>\n",
       "      <td>-0.008626</td>\n",
       "      <td>...</td>\n",
       "      <td>373.95757</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>202.10288</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>348</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25.740000</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.000389</td>\n",
       "      <td>...</td>\n",
       "      <td>956.45047</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>2368.87006</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>-551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.130000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.910</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.006390</td>\n",
       "      <td>...</td>\n",
       "      <td>274.63024</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>99.90263</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>-426</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089995</th>\n",
       "      <td>1.458000</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>-0.000686</td>\n",
       "      <td>13.700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>137.46733</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.002906</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>128.18419</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089996</th>\n",
       "      <td>0.518500</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>-0.001929</td>\n",
       "      <td>789.000</td>\n",
       "      <td>-0.003857</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>7363.00</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>...</td>\n",
       "      <td>85.34676</td>\n",
       "      <td>-0.000913</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>-0.000176</td>\n",
       "      <td>179.62893</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>33</td>\n",
       "      <td>-632</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089997</th>\n",
       "      <td>3.580000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>443.171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>595.71385</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>394.32062</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>-695</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089998</th>\n",
       "      <td>0.087800</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>-0.003417</td>\n",
       "      <td>9349.00</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>...</td>\n",
       "      <td>554.73769</td>\n",
       "      <td>-0.004571</td>\n",
       "      <td>0.003279</td>\n",
       "      <td>0.003078</td>\n",
       "      <td>353.68536</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>-632</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089999</th>\n",
       "      <td>0.347000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>496.300</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>0.005764</td>\n",
       "      <td>2274.30</td>\n",
       "      <td>0.002882</td>\n",
       "      <td>...</td>\n",
       "      <td>104.05384</td>\n",
       "      <td>0.007901</td>\n",
       "      <td>0.010933</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>233.55521</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1090000 rows Ã— 499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             price    high-1     low-1   close-1  volume-1    high-2  \\\n",
       "0         0.361925  0.000345  0.001174  0.000345  5661.000  0.000898   \n",
       "1         0.456675  0.000602  0.000602  0.000602     0.000 -0.002026   \n",
       "2        27.532500 -0.004631 -0.000636 -0.000999  5369.550 -0.007537   \n",
       "3        25.740000 -0.000389 -0.000389 -0.000389     0.000 -0.000389   \n",
       "4         3.130000  0.000000  0.000000  0.000000    49.910 -0.006390   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1089995   1.458000 -0.000686 -0.000686 -0.000686    13.700  0.000000   \n",
       "1089996   0.518500 -0.001929 -0.001157 -0.001929   789.000 -0.003857   \n",
       "1089997   3.580000  0.000000  0.000000  0.000000   443.171  0.000000   \n",
       "1089998   0.087800 -0.003417 -0.003417 -0.003417     0.000 -0.003417   \n",
       "1089999   0.347000  0.000000  0.005764  0.002882   496.300  0.002882   \n",
       "\n",
       "            low-2   close-2  volume-2    high-3  ...  BTC_volume-3_5min  \\\n",
       "0        0.001174  0.001174   1097.00  0.000898  ...          363.67607   \n",
       "1        0.001259  0.000602   3969.40 -0.003996  ...          571.37969   \n",
       "2       -0.002452 -0.003178   8853.35 -0.008626  ...          373.95757   \n",
       "3       -0.000389 -0.000389      1.00 -0.000389  ...          956.45047   \n",
       "4       -0.006390 -0.006390      0.00 -0.006390  ...          274.63024   \n",
       "...           ...       ...       ...       ...  ...                ...   \n",
       "1089995  0.000000  0.000000      0.00  0.000000  ...          137.46733   \n",
       "1089996 -0.001157 -0.001157   7363.00 -0.005786  ...           85.34676   \n",
       "1089997  0.000000  0.000000      0.00  0.000000  ...          595.71385   \n",
       "1089998 -0.002278 -0.003417   9349.00  0.004556  ...          554.73769   \n",
       "1089999  0.005764  0.005764   2274.30  0.002882  ...          104.05384   \n",
       "\n",
       "         BTC_high-4_5min  BTC_low-4_5min  BTC_close-4_5min  BTC_volume-4_5min  \\\n",
       "0              -0.001092        0.000308         -0.000394          339.21955   \n",
       "1              -0.002375       -0.000966         -0.001691          698.47740   \n",
       "2               0.002393        0.005051          0.004128          202.10288   \n",
       "3              -0.002065        0.001875          0.001213         2368.87006   \n",
       "4               0.001559        0.002950          0.002462           99.90263   \n",
       "...                  ...             ...               ...                ...   \n",
       "1089995         0.000881        0.002906          0.000881          128.18419   \n",
       "1089996        -0.000913        0.002724         -0.000176          179.62893   \n",
       "1089997         0.000002        0.001065          0.000311          394.32062   \n",
       "1089998        -0.004571        0.003279          0.003078          353.68536   \n",
       "1089999         0.007901        0.010933          0.010310          233.55521   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          5     4      52        193    0  \n",
       "1          5     2      56       -750    0  \n",
       "2          4    15      13        348    1  \n",
       "3          3     5      32       -551    1  \n",
       "4          3     4       6       -426    1  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "1089995    6     6      57        131    0  \n",
       "1089996    3    19      33       -632    0  \n",
       "1089997    5    20      37       -695    0  \n",
       "1089998    1    14      17       -632    1  \n",
       "1089999    7    18      55         68    1  \n",
       "\n",
       "[1090000 rows x 499 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f\"../BigFiles/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df choosen data shape(1090000, 499)\n",
      "pair: True\n",
      "218000\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"df choosen data shape\"+str(df.shape))\n",
    "print(f\"pair: {(df.shape[0]/2)==df.buy.sum()}\")\n",
    "dt=df.to_numpy(dtype=np.float32)\n",
    "#dt=df.to_numpy()\n",
    "dt=np.nan_to_num(dt,nan=0)\n",
    "#dt=dt.astype(np.float32)\n",
    "dt=np.nan_to_num(dt, neginf=0) \n",
    "dt=np.nan_to_num(dt, posinf=0) \n",
    "\n",
    "index_20pct= int(0.2*len(dt[:,0]))\n",
    "print(index_20pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feather loading\n",
    "# df=pd.read_feather(f\"../Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n",
    "# dt=df.to_numpy(dtype=np.float32)\n",
    "# dt=fixdt(dt)\n",
    "# index_20pct= int(0.2*len(dt[:,0]))\n",
    "# gc.collect()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Normalized Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Model Plus - Very deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 23:41:02.934645: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-04-18 23:41:02.935295: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-04-18 23:41:02.935614: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: abj-K93SV\n",
      "2023-04-18 23:41:02.935669: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: abj-K93SV\n",
      "2023-04-18 23:41:02.936558: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-04-18 23:41:02.937264: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.154.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 498)              1992      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                1620      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259,213\n",
      "Trainable params: 256,377\n",
      "Non-trainable params: 2,836\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-18 23:41:14.403464: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1737024000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "341/341 [==============================] - 112s 289ms/step - loss: 0.4771 - accuracy: 0.7798 - val_loss: 0.6428 - val_accuracy: 0.5867\n",
      "Epoch 2/500\n",
      "341/341 [==============================] - 94s 275ms/step - loss: 0.4587 - accuracy: 0.7909 - val_loss: 0.4832 - val_accuracy: 0.7813\n",
      "Epoch 3/500\n",
      "341/341 [==============================] - 94s 276ms/step - loss: 0.4515 - accuracy: 0.7949 - val_loss: 0.4679 - val_accuracy: 0.7888\n",
      "Epoch 4/500\n",
      "341/341 [==============================] - 93s 273ms/step - loss: 0.4435 - accuracy: 0.7997 - val_loss: 0.4554 - val_accuracy: 0.7917\n",
      "Epoch 5/500\n",
      "341/341 [==============================] - 94s 275ms/step - loss: 0.4357 - accuracy: 0.8042 - val_loss: 0.4494 - val_accuracy: 0.7941\n",
      "Epoch 6/500\n",
      "341/341 [==============================] - 92s 269ms/step - loss: 0.4287 - accuracy: 0.8075 - val_loss: 0.4493 - val_accuracy: 0.8003\n",
      "Epoch 7/500\n",
      "341/341 [==============================] - 93s 271ms/step - loss: 0.4232 - accuracy: 0.8111 - val_loss: 0.4365 - val_accuracy: 0.8041\n",
      "Epoch 8/500\n",
      "341/341 [==============================] - 92s 271ms/step - loss: 0.4178 - accuracy: 0.8139 - val_loss: 0.4413 - val_accuracy: 0.8024\n",
      "Epoch 9/500\n",
      "341/341 [==============================] - 94s 275ms/step - loss: 0.4133 - accuracy: 0.8163 - val_loss: 0.4306 - val_accuracy: 0.8083\n",
      "Epoch 10/500\n",
      "341/341 [==============================] - 93s 273ms/step - loss: 0.4094 - accuracy: 0.8182 - val_loss: 0.4399 - val_accuracy: 0.8006\n",
      "Epoch 11/500\n",
      "341/341 [==============================] - 94s 275ms/step - loss: 0.4054 - accuracy: 0.8202 - val_loss: 0.4220 - val_accuracy: 0.8111\n",
      "Epoch 12/500\n",
      "341/341 [==============================] - 93s 273ms/step - loss: 0.4020 - accuracy: 0.8220 - val_loss: 0.4283 - val_accuracy: 0.8104\n",
      "Epoch 13/500\n",
      "341/341 [==============================] - 96s 283ms/step - loss: 0.3988 - accuracy: 0.8237 - val_loss: 0.4288 - val_accuracy: 0.8148\n",
      "Epoch 14/500\n",
      "341/341 [==============================] - 106s 310ms/step - loss: 0.3962 - accuracy: 0.8249 - val_loss: 0.4160 - val_accuracy: 0.8161\n",
      "Epoch 15/500\n",
      "341/341 [==============================] - 103s 301ms/step - loss: 0.3936 - accuracy: 0.8263 - val_loss: 0.4266 - val_accuracy: 0.8037\n",
      "Epoch 16/500\n",
      "341/341 [==============================] - 100s 293ms/step - loss: 0.3916 - accuracy: 0.8274 - val_loss: 0.4112 - val_accuracy: 0.8176\n",
      "Epoch 17/500\n",
      "341/341 [==============================] - 107s 313ms/step - loss: 0.3893 - accuracy: 0.8286 - val_loss: 0.4090 - val_accuracy: 0.8194\n",
      "Epoch 18/500\n",
      "341/341 [==============================] - 107s 313ms/step - loss: 0.3872 - accuracy: 0.8295 - val_loss: 0.4088 - val_accuracy: 0.8180\n",
      "Epoch 19/500\n",
      "341/341 [==============================] - 98s 289ms/step - loss: 0.3851 - accuracy: 0.8306 - val_loss: 0.4159 - val_accuracy: 0.8137\n",
      "Epoch 20/500\n",
      "341/341 [==============================] - 104s 305ms/step - loss: 0.3836 - accuracy: 0.8312 - val_loss: 0.4139 - val_accuracy: 0.8210\n",
      "Epoch 21/500\n",
      "341/341 [==============================] - 104s 305ms/step - loss: 0.3811 - accuracy: 0.8327 - val_loss: 0.4146 - val_accuracy: 0.8185\n",
      "Epoch 22/500\n",
      "341/341 [==============================] - 98s 289ms/step - loss: 0.3795 - accuracy: 0.8332 - val_loss: 0.4075 - val_accuracy: 0.8217\n",
      "Epoch 23/500\n",
      "341/341 [==============================] - 105s 308ms/step - loss: 0.3778 - accuracy: 0.8342 - val_loss: 0.4038 - val_accuracy: 0.8211\n",
      "Epoch 24/500\n",
      "341/341 [==============================] - 104s 305ms/step - loss: 0.3768 - accuracy: 0.8342 - val_loss: 0.4000 - val_accuracy: 0.8228\n",
      "Epoch 25/500\n",
      "341/341 [==============================] - 99s 291ms/step - loss: 0.3751 - accuracy: 0.8353 - val_loss: 0.3935 - val_accuracy: 0.8244\n",
      "Epoch 26/500\n",
      "341/341 [==============================] - 101s 297ms/step - loss: 0.3734 - accuracy: 0.8360 - val_loss: 0.4091 - val_accuracy: 0.8148\n",
      "Epoch 27/500\n",
      "341/341 [==============================] - 106s 312ms/step - loss: 0.3723 - accuracy: 0.8366 - val_loss: 0.3979 - val_accuracy: 0.8231\n",
      "Epoch 28/500\n",
      "341/341 [==============================] - 100s 294ms/step - loss: 0.3706 - accuracy: 0.8376 - val_loss: 0.3958 - val_accuracy: 0.8236\n",
      "Epoch 29/500\n",
      "341/341 [==============================] - 104s 305ms/step - loss: 0.3694 - accuracy: 0.8380 - val_loss: 0.4063 - val_accuracy: 0.8134\n",
      "Epoch 30/500\n",
      "341/341 [==============================] - 103s 302ms/step - loss: 0.3682 - accuracy: 0.8389 - val_loss: 0.3957 - val_accuracy: 0.8235\n",
      "Epoch 31/500\n",
      "341/341 [==============================] - 98s 287ms/step - loss: 0.3667 - accuracy: 0.8392 - val_loss: 0.3969 - val_accuracy: 0.8236\n",
      "Epoch 31: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_VeryDeep.h5\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-  Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 00:32:55.572501: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 233s 7ms/step\n",
      "Precent Mean: 45.242%\n",
      "Precent Mean: 2.328%\n",
      "ModelAccuracy: 83.049%\n",
      "True Win Predictions Mean of all: 39.145%\n",
      "XXX Loss Buy Mean of all: 6.096%\n",
      "Missed good deal off all: 10.855%\n",
      "Good Zero prediction Mean: 43.904%\n",
      "good fiability\n",
      "========= Win Ratio:86.52549678389072 ====================\n"
     ]
    }
   ],
   "source": [
    "USED_MODEL=very_deep_model\n",
    "#model_init=model\n",
    "#USED_MODEL=model_init#load_model(\"/UltimeTradingBot/Data/BUY_UP_CLOSE/tp60_w6_max3min_Model_GoodVeryDeep.h5\")\n",
    "Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "prediction2=Prediction_Note.round()\n",
    "hp(prediction2[:,0].mean())\n",
    "PesemisticPrediction=(Prediction_Note[:,0]-0.49).round()\n",
    "hp(PesemisticPrediction.mean())\n",
    "Y=dt[:,-1].copy()\n",
    "Pred01=prediction2[:,-1]\n",
    "Original_Traget_Data=Y\n",
    "Predicted_Data=Pred01\n",
    "\n",
    "TruePred=((Original_Traget_Data==Predicted_Data)).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True PredONly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 00:37:55.247226: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 251s 7ms/step\n",
      "Training model...\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 00:43:01.464997: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171082792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 82s 178ms/step - loss: 0.1653 - accuracy: 0.8486 - val_loss: 0.4079 - val_accuracy: 0.8182\n",
      "Epoch 2/500\n",
      "426/426 [==============================] - 78s 183ms/step - loss: 0.1490 - accuracy: 0.8668 - val_loss: 0.2649 - val_accuracy: 0.8687\n",
      "Epoch 3/500\n",
      "426/426 [==============================] - 71s 167ms/step - loss: 0.1393 - accuracy: 0.8762 - val_loss: 0.2093 - val_accuracy: 0.9091\n",
      "Epoch 4/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.1329 - accuracy: 0.8827 - val_loss: 0.1822 - val_accuracy: 0.9394\n",
      "Epoch 5/500\n",
      "426/426 [==============================] - 72s 169ms/step - loss: 0.1281 - accuracy: 0.8877 - val_loss: 0.1899 - val_accuracy: 0.9192\n",
      "Epoch 6/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.1246 - accuracy: 0.8915 - val_loss: 0.1864 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.1218 - accuracy: 0.8940 - val_loss: 0.1787 - val_accuracy: 0.9293\n",
      "Epoch 8/500\n",
      "426/426 [==============================] - 76s 179ms/step - loss: 0.1196 - accuracy: 0.8962 - val_loss: 0.1685 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "426/426 [==============================] - 74s 175ms/step - loss: 0.1180 - accuracy: 0.8978 - val_loss: 0.1494 - val_accuracy: 0.9495\n",
      "Epoch 10/500\n",
      "426/426 [==============================] - 74s 175ms/step - loss: 0.1164 - accuracy: 0.8991 - val_loss: 0.1587 - val_accuracy: 0.9293\n",
      "Epoch 11/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.1148 - accuracy: 0.9010 - val_loss: 0.1687 - val_accuracy: 0.9293\n",
      "Epoch 12/500\n",
      "426/426 [==============================] - 77s 181ms/step - loss: 0.1138 - accuracy: 0.9018 - val_loss: 0.1379 - val_accuracy: 0.9394\n",
      "Epoch 13/500\n",
      "426/426 [==============================] - 77s 181ms/step - loss: 0.1128 - accuracy: 0.9028 - val_loss: 0.1603 - val_accuracy: 0.9192\n",
      "Epoch 14/500\n",
      "426/426 [==============================] - 76s 179ms/step - loss: 0.1119 - accuracy: 0.9035 - val_loss: 0.1388 - val_accuracy: 0.9697\n",
      "Epoch 15/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.1111 - accuracy: 0.9043 - val_loss: 0.1693 - val_accuracy: 0.9293\n",
      "Epoch 16/500\n",
      "426/426 [==============================] - 72s 169ms/step - loss: 0.1105 - accuracy: 0.9053 - val_loss: 0.1987 - val_accuracy: 0.9293\n",
      "Epoch 17/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.1098 - accuracy: 0.9058 - val_loss: 0.1677 - val_accuracy: 0.9293\n",
      "Epoch 18/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.1090 - accuracy: 0.9067 - val_loss: 0.1677 - val_accuracy: 0.9394\n",
      "Epoch 19/500\n",
      "426/426 [==============================] - 76s 179ms/step - loss: 0.1088 - accuracy: 0.9068 - val_loss: 0.1746 - val_accuracy: 0.9293\n",
      "Epoch 20/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.1081 - accuracy: 0.9071 - val_loss: 0.1707 - val_accuracy: 0.9394\n",
      "Epoch 21/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.1076 - accuracy: 0.9081 - val_loss: 0.1519 - val_accuracy: 0.9495\n",
      "Epoch 22/500\n",
      "426/426 [==============================] - 75s 177ms/step - loss: 0.1070 - accuracy: 0.9084 - val_loss: 0.1614 - val_accuracy: 0.9697\n",
      "Epoch 23/500\n",
      "426/426 [==============================] - 72s 169ms/step - loss: 0.1066 - accuracy: 0.9089 - val_loss: 0.1584 - val_accuracy: 0.9394\n",
      "Epoch 24/500\n",
      "426/426 [==============================] - 77s 180ms/step - loss: 0.1063 - accuracy: 0.9092 - val_loss: 0.1602 - val_accuracy: 0.9293\n",
      "Epoch 25/500\n",
      "426/426 [==============================] - 73s 173ms/step - loss: 0.1060 - accuracy: 0.9096 - val_loss: 0.1526 - val_accuracy: 0.9394\n",
      "Epoch 26/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.1058 - accuracy: 0.9096 - val_loss: 0.1717 - val_accuracy: 0.9293\n",
      "Epoch 27/500\n",
      "426/426 [==============================] - 73s 170ms/step - loss: 0.1052 - accuracy: 0.9105 - val_loss: 0.1583 - val_accuracy: 0.9293\n",
      "Epoch 28/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.1051 - accuracy: 0.9103 - val_loss: 0.1433 - val_accuracy: 0.9495\n",
      "Epoch 29/500\n",
      "426/426 [==============================] - 72s 169ms/step - loss: 0.1050 - accuracy: 0.9106 - val_loss: 0.1677 - val_accuracy: 0.9495\n",
      "Epoch 30/500\n",
      "426/426 [==============================] - 80s 187ms/step - loss: 0.1045 - accuracy: 0.9109 - val_loss: 0.1650 - val_accuracy: 0.9394\n",
      "Epoch 30: early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 01:20:48.805544: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 193s 6ms/step\n",
      "Check the fiability: 1.0\n",
      "---------------------------------------------------------------\n",
      "accuracy : 79.92247706422019%\n",
      "True win prediction: 33.75807339449541%\n",
      "Win ratio: 89.79722820795028%\n",
      "---------------------------------------------------------------\n",
      "Training model...\n",
      "Epoch 1/500\n",
      "426/426 [==============================] - 87s 177ms/step - loss: 0.1395 - accuracy: 0.8746 - val_loss: 0.4976 - val_accuracy: 0.8081\n",
      "Epoch 2/500\n",
      "426/426 [==============================] - 74s 175ms/step - loss: 0.1161 - accuracy: 0.8993 - val_loss: 0.1887 - val_accuracy: 0.9192\n",
      "Epoch 3/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.1047 - accuracy: 0.9113 - val_loss: 0.1315 - val_accuracy: 0.9394\n",
      "Epoch 4/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0977 - accuracy: 0.9186 - val_loss: 0.1518 - val_accuracy: 0.9394\n",
      "Epoch 5/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0935 - accuracy: 0.9227 - val_loss: 0.1344 - val_accuracy: 0.9596\n",
      "Epoch 6/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0904 - accuracy: 0.9256 - val_loss: 0.1495 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "426/426 [==============================] - 72s 170ms/step - loss: 0.0883 - accuracy: 0.9278 - val_loss: 0.1306 - val_accuracy: 0.9495\n",
      "Epoch 8/500\n",
      "426/426 [==============================] - 73s 173ms/step - loss: 0.0866 - accuracy: 0.9295 - val_loss: 0.1722 - val_accuracy: 0.9495\n",
      "Epoch 9/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0851 - accuracy: 0.9309 - val_loss: 0.1332 - val_accuracy: 0.9394\n",
      "Epoch 10/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0838 - accuracy: 0.9321 - val_loss: 0.1367 - val_accuracy: 0.9596\n",
      "Epoch 11/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0825 - accuracy: 0.9335 - val_loss: 0.1271 - val_accuracy: 0.9596\n",
      "Epoch 12/500\n",
      "426/426 [==============================] - 77s 181ms/step - loss: 0.0820 - accuracy: 0.9339 - val_loss: 0.1315 - val_accuracy: 0.9394\n",
      "Epoch 13/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0810 - accuracy: 0.9348 - val_loss: 0.1317 - val_accuracy: 0.9394\n",
      "Epoch 14/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0804 - accuracy: 0.9354 - val_loss: 0.1352 - val_accuracy: 0.9293\n",
      "Epoch 15/500\n",
      "426/426 [==============================] - 75s 175ms/step - loss: 0.0797 - accuracy: 0.9361 - val_loss: 0.1387 - val_accuracy: 0.9394\n",
      "Epoch 16/500\n",
      "426/426 [==============================] - 72s 170ms/step - loss: 0.0791 - accuracy: 0.9363 - val_loss: 0.1618 - val_accuracy: 0.9495\n",
      "Epoch 17/500\n",
      "426/426 [==============================] - 78s 183ms/step - loss: 0.0787 - accuracy: 0.9369 - val_loss: 0.1143 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0783 - accuracy: 0.9372 - val_loss: 0.1238 - val_accuracy: 0.9495\n",
      "Epoch 19/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0776 - accuracy: 0.9380 - val_loss: 0.1357 - val_accuracy: 0.9596\n",
      "Epoch 20/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0775 - accuracy: 0.9382 - val_loss: 0.1078 - val_accuracy: 0.9596\n",
      "Epoch 21/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0771 - accuracy: 0.9386 - val_loss: 0.1082 - val_accuracy: 0.9596\n",
      "Epoch 21: early stopping\n",
      "34063/34063 [==============================] - 184s 5ms/step\n",
      "Check the fiability: 1.0\n",
      "---------------------------------------------------------------\n",
      "accuracy : 77.21293577981652%\n",
      "True win prediction: 30.318623853211008%\n",
      "Win ratio: 90.70829616413916%\n",
      "---------------------------------------------------------------\n",
      "Training model...\n",
      "Epoch 1/500\n",
      "426/426 [==============================] - 85s 179ms/step - loss: 0.1243 - accuracy: 0.8893 - val_loss: 0.6444 - val_accuracy: 0.7778\n",
      "Epoch 2/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0971 - accuracy: 0.9176 - val_loss: 0.2111 - val_accuracy: 0.9192\n",
      "Epoch 3/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0849 - accuracy: 0.9302 - val_loss: 0.1634 - val_accuracy: 0.9394\n",
      "Epoch 4/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.0787 - accuracy: 0.9368 - val_loss: 0.1471 - val_accuracy: 0.9596\n",
      "Epoch 5/500\n",
      "426/426 [==============================] - 67s 157ms/step - loss: 0.0751 - accuracy: 0.9402 - val_loss: 0.1649 - val_accuracy: 0.9293\n",
      "Epoch 6/500\n",
      "426/426 [==============================] - 64s 149ms/step - loss: 0.0729 - accuracy: 0.9424 - val_loss: 0.1381 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "426/426 [==============================] - 61s 143ms/step - loss: 0.0711 - accuracy: 0.9442 - val_loss: 0.1491 - val_accuracy: 0.9293\n",
      "Epoch 8/500\n",
      "426/426 [==============================] - 61s 143ms/step - loss: 0.0697 - accuracy: 0.9456 - val_loss: 0.1756 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "426/426 [==============================] - 63s 148ms/step - loss: 0.0688 - accuracy: 0.9463 - val_loss: 0.1541 - val_accuracy: 0.9495\n",
      "Epoch 10/500\n",
      "426/426 [==============================] - 64s 150ms/step - loss: 0.0679 - accuracy: 0.9471 - val_loss: 0.1356 - val_accuracy: 0.9495\n",
      "Epoch 11/500\n",
      "426/426 [==============================] - 67s 156ms/step - loss: 0.0672 - accuracy: 0.9480 - val_loss: 0.1406 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "426/426 [==============================] - 78s 184ms/step - loss: 0.0665 - accuracy: 0.9485 - val_loss: 0.1562 - val_accuracy: 0.9192\n",
      "Epoch 13/500\n",
      "426/426 [==============================] - 86s 202ms/step - loss: 0.0658 - accuracy: 0.9492 - val_loss: 0.1296 - val_accuracy: 0.9596\n",
      "Epoch 14/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0653 - accuracy: 0.9496 - val_loss: 0.1422 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0649 - accuracy: 0.9499 - val_loss: 0.1566 - val_accuracy: 0.9394\n",
      "Epoch 16/500\n",
      "426/426 [==============================] - 73s 173ms/step - loss: 0.0645 - accuracy: 0.9503 - val_loss: 0.1779 - val_accuracy: 0.9394\n",
      "Epoch 17/500\n",
      "426/426 [==============================] - 76s 179ms/step - loss: 0.0642 - accuracy: 0.9506 - val_loss: 0.1528 - val_accuracy: 0.9394\n",
      "Epoch 18/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0637 - accuracy: 0.9510 - val_loss: 0.1593 - val_accuracy: 0.9697\n",
      "Epoch 19/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.0635 - accuracy: 0.9511 - val_loss: 0.1230 - val_accuracy: 0.9697\n",
      "Epoch 20/500\n",
      "426/426 [==============================] - 75s 177ms/step - loss: 0.0633 - accuracy: 0.9516 - val_loss: 0.1722 - val_accuracy: 0.9293\n",
      "Epoch 21/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0630 - accuracy: 0.9518 - val_loss: 0.1784 - val_accuracy: 0.9394\n",
      "Epoch 22/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0625 - accuracy: 0.9521 - val_loss: 0.2106 - val_accuracy: 0.9394\n",
      "Epoch 23/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0624 - accuracy: 0.9523 - val_loss: 0.1146 - val_accuracy: 0.9394\n",
      "Epoch 24/500\n",
      "426/426 [==============================] - 82s 192ms/step - loss: 0.0623 - accuracy: 0.9524 - val_loss: 0.1350 - val_accuracy: 0.9596\n",
      "Epoch 25/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0620 - accuracy: 0.9527 - val_loss: 0.1404 - val_accuracy: 0.9697\n",
      "Epoch 26/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.0618 - accuracy: 0.9526 - val_loss: 0.1649 - val_accuracy: 0.9495\n",
      "Epoch 27/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0616 - accuracy: 0.9531 - val_loss: 0.1108 - val_accuracy: 0.9495\n",
      "Epoch 27: early stopping\n",
      "34063/34063 [==============================] - 165s 5ms/step\n",
      "Check the fiability: 1.0\n",
      "---------------------------------------------------------------\n",
      "accuracy : 75.91990825688073%\n",
      "True win prediction: 28.449816513761466%\n",
      "Win ratio: 91.8336645157087%\n",
      "---------------------------------------------------------------\n",
      "Training model...\n",
      "Epoch 1/500\n",
      "426/426 [==============================] - 79s 160ms/step - loss: 0.1167 - accuracy: 0.8963 - val_loss: 0.6157 - val_accuracy: 0.8182\n",
      "Epoch 2/500\n",
      "426/426 [==============================] - 66s 155ms/step - loss: 0.0877 - accuracy: 0.9266 - val_loss: 0.2022 - val_accuracy: 0.9394\n",
      "Epoch 3/500\n",
      "426/426 [==============================] - 67s 157ms/step - loss: 0.0759 - accuracy: 0.9386 - val_loss: 0.1212 - val_accuracy: 0.9697\n",
      "Epoch 4/500\n",
      "426/426 [==============================] - 65s 154ms/step - loss: 0.0702 - accuracy: 0.9443 - val_loss: 0.1368 - val_accuracy: 0.9596\n",
      "Epoch 5/500\n",
      "426/426 [==============================] - 67s 156ms/step - loss: 0.0669 - accuracy: 0.9478 - val_loss: 0.1595 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "426/426 [==============================] - 66s 156ms/step - loss: 0.0644 - accuracy: 0.9500 - val_loss: 0.1147 - val_accuracy: 0.9596\n",
      "Epoch 7/500\n",
      "426/426 [==============================] - 67s 157ms/step - loss: 0.0627 - accuracy: 0.9516 - val_loss: 0.1132 - val_accuracy: 0.9596\n",
      "Epoch 8/500\n",
      "426/426 [==============================] - 66s 156ms/step - loss: 0.0615 - accuracy: 0.9531 - val_loss: 0.0995 - val_accuracy: 0.9697\n",
      "Epoch 9/500\n",
      "426/426 [==============================] - 67s 156ms/step - loss: 0.0604 - accuracy: 0.9540 - val_loss: 0.1037 - val_accuracy: 0.9495\n",
      "Epoch 10/500\n",
      "426/426 [==============================] - 65s 153ms/step - loss: 0.0597 - accuracy: 0.9547 - val_loss: 0.0881 - val_accuracy: 0.9596\n",
      "Epoch 11/500\n",
      "426/426 [==============================] - 66s 155ms/step - loss: 0.0589 - accuracy: 0.9554 - val_loss: 0.1257 - val_accuracy: 0.9495\n",
      "Epoch 12/500\n",
      "426/426 [==============================] - 67s 157ms/step - loss: 0.0582 - accuracy: 0.9560 - val_loss: 0.1118 - val_accuracy: 0.9596\n",
      "Epoch 13/500\n",
      "426/426 [==============================] - 68s 160ms/step - loss: 0.0577 - accuracy: 0.9565 - val_loss: 0.0914 - val_accuracy: 0.9697\n",
      "Epoch 14/500\n",
      "426/426 [==============================] - 65s 153ms/step - loss: 0.0574 - accuracy: 0.9568 - val_loss: 0.0844 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "426/426 [==============================] - 65s 154ms/step - loss: 0.0569 - accuracy: 0.9572 - val_loss: 0.0997 - val_accuracy: 0.9495\n",
      "Epoch 16/500\n",
      "426/426 [==============================] - 67s 157ms/step - loss: 0.0565 - accuracy: 0.9574 - val_loss: 0.1182 - val_accuracy: 0.9495\n",
      "Epoch 17/500\n",
      "426/426 [==============================] - 65s 153ms/step - loss: 0.0561 - accuracy: 0.9578 - val_loss: 0.1100 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "426/426 [==============================] - 65s 152ms/step - loss: 0.0558 - accuracy: 0.9581 - val_loss: 0.1153 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "426/426 [==============================] - 65s 153ms/step - loss: 0.0555 - accuracy: 0.9587 - val_loss: 0.1017 - val_accuracy: 0.9495\n",
      "Epoch 19: early stopping\n",
      "34063/34063 [==============================] - 166s 5ms/step\n",
      "Check the fiability: 1.0\n",
      "---------------------------------------------------------------\n",
      "accuracy : 75.49678899082569%\n",
      "True win prediction: 28.35330275229358%\n",
      "Win ratio: 90.84738687839335%\n",
      "---------------------------------------------------------------\n",
      "Training model...\n",
      "Epoch 1/500\n",
      "426/426 [==============================] - 82s 176ms/step - loss: 0.1098 - accuracy: 0.9037 - val_loss: 0.5828 - val_accuracy: 0.8182\n",
      "Epoch 2/500\n",
      "426/426 [==============================] - 75s 177ms/step - loss: 0.0799 - accuracy: 0.9347 - val_loss: 0.1004 - val_accuracy: 0.9697\n",
      "Epoch 3/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0701 - accuracy: 0.9448 - val_loss: 0.0949 - val_accuracy: 0.9697\n",
      "Epoch 4/500\n",
      "426/426 [==============================] - 72s 169ms/step - loss: 0.0658 - accuracy: 0.9493 - val_loss: 0.0709 - val_accuracy: 0.9899\n",
      "Epoch 5/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0632 - accuracy: 0.9518 - val_loss: 0.0826 - val_accuracy: 0.9798\n",
      "Epoch 6/500\n",
      "426/426 [==============================] - 74s 173ms/step - loss: 0.0617 - accuracy: 0.9534 - val_loss: 0.0740 - val_accuracy: 0.9899\n",
      "Epoch 7/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0601 - accuracy: 0.9547 - val_loss: 0.0670 - val_accuracy: 0.9899\n",
      "Epoch 8/500\n",
      "426/426 [==============================] - 78s 184ms/step - loss: 0.0593 - accuracy: 0.9556 - val_loss: 0.0574 - val_accuracy: 1.0000\n",
      "Epoch 9/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0584 - accuracy: 0.9564 - val_loss: 0.0578 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "426/426 [==============================] - 72s 168ms/step - loss: 0.0579 - accuracy: 0.9568 - val_loss: 0.0559 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "426/426 [==============================] - 75s 176ms/step - loss: 0.0572 - accuracy: 0.9575 - val_loss: 0.0693 - val_accuracy: 0.9899\n",
      "Epoch 12/500\n",
      "426/426 [==============================] - 73s 172ms/step - loss: 0.0565 - accuracy: 0.9579 - val_loss: 0.0534 - val_accuracy: 0.9899\n",
      "Epoch 13/500\n",
      "426/426 [==============================] - 73s 171ms/step - loss: 0.0563 - accuracy: 0.9583 - val_loss: 0.0539 - val_accuracy: 1.0000\n",
      "Epoch 14/500\n",
      "426/426 [==============================] - 77s 181ms/step - loss: 0.0559 - accuracy: 0.9585 - val_loss: 0.0680 - val_accuracy: 0.9899\n",
      "Epoch 15/500\n",
      "426/426 [==============================] - 77s 180ms/step - loss: 0.0555 - accuracy: 0.9589 - val_loss: 0.0562 - val_accuracy: 1.0000\n",
      "Epoch 16/500\n",
      "426/426 [==============================] - 73s 170ms/step - loss: 0.0551 - accuracy: 0.9593 - val_loss: 0.0483 - val_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      "426/426 [==============================] - 73s 170ms/step - loss: 0.0550 - accuracy: 0.9592 - val_loss: 0.0560 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.0546 - accuracy: 0.9598 - val_loss: 0.0572 - val_accuracy: 0.9899\n",
      "Epoch 19/500\n",
      "426/426 [==============================] - 74s 174ms/step - loss: 0.0542 - accuracy: 0.9601 - val_loss: 0.0620 - val_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      "426/426 [==============================] - 80s 189ms/step - loss: 0.0542 - accuracy: 0.9601 - val_loss: 0.0553 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "426/426 [==============================] - 77s 180ms/step - loss: 0.0538 - accuracy: 0.9604 - val_loss: 0.0588 - val_accuracy: 0.9899\n",
      "Epoch 22/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0537 - accuracy: 0.9605 - val_loss: 0.0505 - val_accuracy: 1.0000\n",
      "Epoch 23/500\n",
      "426/426 [==============================] - 76s 178ms/step - loss: 0.0534 - accuracy: 0.9607 - val_loss: 0.0495 - val_accuracy: 0.9899\n",
      "Epoch 24/500\n",
      "426/426 [==============================] - 75s 175ms/step - loss: 0.0532 - accuracy: 0.9608 - val_loss: 0.0593 - val_accuracy: 1.0000\n",
      "Epoch 24: early stopping\n",
      "34063/34063 [==============================] - 192s 6ms/step\n",
      "Check the fiability: 1.0\n",
      "---------------------------------------------------------------\n",
      "accuracy : 74.58422018348624%\n",
      "True win prediction: 26.863027522935777%\n",
      "Win ratio: 92.18028874911064%\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define constants\n",
    "RETRAIN_ITERATIONS = 5\n",
    "INDEX_20PCT = int(dt.shape[1] * 0.2)\n",
    "SIZE_TUNER = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "CLASS_1_WEIGHT = TrueWinPred.mean()\n",
    "CLASS_WEIGHTS = {0: 1 - CLASS_1_WEIGHT, 1: CLASS_1_WEIGHT}\n",
    "BATCH_SIZE = 256 * 10\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.002\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "PATIENCE = 16\n",
    "TAKE_PROFIT=BUY_PCT\n",
    "tp100=TAKE_PROFIT*100\n",
    "MODEL_FILE_SUFFIX=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'\n",
    "\n",
    "### Init\n",
    "Y=dt[:, -1]\n",
    "\n",
    "init_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "predictions = init_model.predict(dt[:, :-1])\n",
    "predictions_rounded = predictions.round()\n",
    "original_target_data = Y\n",
    "predicted_data = predictions_rounded[:, 0]\n",
    "TrueWinPred=((predicted_data == 1) & (original_target_data == 1))\n",
    "\n",
    "for rrr in range(RETRAIN_ITERATIONS):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='relu'))\n",
    "    model.add(Dense(int(20 * SIZE_TUNER), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Nadam(lr=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5', monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=PATIENCE, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(dt[INDEX_20PCT:, :-1], TrueWinPred[INDEX_20PCT:],\n",
    "                        validation_data=(dt[:INDEX_20PCT, :-1], TrueWinPred[:INDEX_20PCT]),\n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = Sequential()\n",
    "    best_model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    best_model.add(Dense(int(200 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dropout(0.2))\n",
    "\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dropout(0.2))\n",
    "\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='relu'))\n",
    "    best_model.add(Dense(int(20 * SIZE_TUNER), activation='relu'))\n",
    "    best_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    best_model.load_weights(MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5')\n",
    "\n",
    "    # Evaluate the model on the hold-out dataset\n",
    "    predictions = best_model.predict(dt[:, :-1])\n",
    "    predictions_rounded = predictions.round()\n",
    "    original_target_data = Y\n",
    "    predicted_data = predictions_rounded[:, 0]\n",
    "\n",
    "    # Compute various metrics\n",
    "    accuracy = (original_target_data == predicted_data).mean()\n",
    "    true_win_predictions = ((predicted_data == 1) & (original_target_data == 1)).mean()\n",
    "    loss_buy_mean = ((predicted_data == 1) & (original_target_data == 0)).mean()\n",
    "    missed_good_deal_mean = ((predicted_data == 0) & (original_target_data == 1)).mean()\n",
    "    good_zero_prediction_mean = ((predicted_data == 0) & (original_target_data == 0)).mean()\n",
    "\n",
    "    fiability = true_win_predictions + loss_buy_mean + missed_good_deal_mean + good_zero_prediction_mean\n",
    "    if fiability == 100:\n",
    "        print(\"Good fiability\")\n",
    "    else:\n",
    "        print(f\"Check the fiability: {fiability}\")\n",
    "\n",
    "    win_ratio = true_win_predictions / (loss_buy_mean + true_win_predictions)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5')\n",
    "    print(f\"accuracy : {accuracy*100}%\")\n",
    "    print(f\"True win prediction: {true_win_predictions*100}%\")\n",
    "    print(f\"Win ratio: {win_ratio*100}%\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    # Use the true win predictions for retraining\n",
    "    TrueWinPred = ((predicted_data == 1) & (original_target_data == 1)).copy()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 258s 8ms/step\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_49 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 149s 166ms/step - loss: 0.1299 - accuracy: 0.8871 - val_loss: 0.2448 - val_accuracy: 0.8586\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 141s 165ms/step - loss: 0.1050 - accuracy: 0.9098 - val_loss: 0.1397 - val_accuracy: 0.9394\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 137s 160ms/step - loss: 0.0902 - accuracy: 0.9231 - val_loss: 0.1457 - val_accuracy: 0.9293\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0807 - accuracy: 0.9315 - val_loss: 0.1432 - val_accuracy: 0.9394\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 137s 161ms/step - loss: 0.0748 - accuracy: 0.9367 - val_loss: 0.1124 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0703 - accuracy: 0.9403 - val_loss: 0.1567 - val_accuracy: 0.9192\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0666 - accuracy: 0.9437 - val_loss: 0.1003 - val_accuracy: 0.9596\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 139s 164ms/step - loss: 0.0642 - accuracy: 0.9460 - val_loss: 0.1292 - val_accuracy: 0.9192\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 140s 164ms/step - loss: 0.0621 - accuracy: 0.9477 - val_loss: 0.1397 - val_accuracy: 0.9394\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 137s 160ms/step - loss: 0.0602 - accuracy: 0.9492 - val_loss: 0.1072 - val_accuracy: 0.9394\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 138s 162ms/step - loss: 0.0589 - accuracy: 0.9506 - val_loss: 0.1462 - val_accuracy: 0.9394\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 140s 164ms/step - loss: 0.0577 - accuracy: 0.9513 - val_loss: 0.1117 - val_accuracy: 0.9293\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0567 - accuracy: 0.9522 - val_loss: 0.1053 - val_accuracy: 0.9293\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 141s 165ms/step - loss: 0.0552 - accuracy: 0.9536 - val_loss: 0.0973 - val_accuracy: 0.9394\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 137s 161ms/step - loss: 0.0547 - accuracy: 0.9541 - val_loss: 0.1119 - val_accuracy: 0.9394\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0543 - accuracy: 0.9544 - val_loss: 0.1267 - val_accuracy: 0.9394\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0533 - accuracy: 0.9549 - val_loss: 0.0885 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 122s 144ms/step - loss: 0.0527 - accuracy: 0.9557 - val_loss: 0.1029 - val_accuracy: 0.9293\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 122s 144ms/step - loss: 0.0522 - accuracy: 0.9562 - val_loss: 0.1153 - val_accuracy: 0.9293\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 121s 142ms/step - loss: 0.0518 - accuracy: 0.9564 - val_loss: 0.1142 - val_accuracy: 0.9293\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 123s 144ms/step - loss: 0.0512 - accuracy: 0.9570 - val_loss: 0.1272 - val_accuracy: 0.9394\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 120s 140ms/step - loss: 0.0507 - accuracy: 0.9573 - val_loss: 0.0729 - val_accuracy: 0.9596\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 116s 136ms/step - loss: 0.0501 - accuracy: 0.9581 - val_loss: 0.1439 - val_accuracy: 0.9192\n",
      "Epoch 23: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re1.h5\n",
      "34063/34063 [==============================] - 139s 4ms/step\n",
      "ModelAccuracy: 81.787%\n",
      "True Win Predictions Mean of all: 37.513%\n",
      "XXX Loss Buy Mean of all: 5.726%\n",
      "Missed good deal off all: 12.487%\n",
      "Good Zero prediction Mean: 44.274%\n",
      "good fiability\n",
      "========= Win Ratio:86.75732556257083 ====================\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_54 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 113s 126ms/step - loss: 0.1506 - accuracy: 0.8618 - val_loss: 0.2359 - val_accuracy: 0.8990\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 108s 127ms/step - loss: 0.1304 - accuracy: 0.8842 - val_loss: 0.1962 - val_accuracy: 0.9192\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 105s 123ms/step - loss: 0.1190 - accuracy: 0.8957 - val_loss: 0.1769 - val_accuracy: 0.9394\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 105s 123ms/step - loss: 0.1118 - accuracy: 0.9029 - val_loss: 0.1497 - val_accuracy: 0.9495\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 107s 126ms/step - loss: 0.1069 - accuracy: 0.9079 - val_loss: 0.1751 - val_accuracy: 0.9596\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 105s 124ms/step - loss: 0.1038 - accuracy: 0.9108 - val_loss: 0.1574 - val_accuracy: 0.9596\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 101s 119ms/step - loss: 0.1012 - accuracy: 0.9135 - val_loss: 0.1337 - val_accuracy: 0.9798\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0993 - accuracy: 0.9156 - val_loss: 0.1426 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0977 - accuracy: 0.9171 - val_loss: 0.1475 - val_accuracy: 0.9495\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 107s 126ms/step - loss: 0.0964 - accuracy: 0.9185 - val_loss: 0.1230 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 102s 120ms/step - loss: 0.0951 - accuracy: 0.9194 - val_loss: 0.1668 - val_accuracy: 0.9293\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0944 - accuracy: 0.9201 - val_loss: 0.1259 - val_accuracy: 0.9697\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 108s 126ms/step - loss: 0.0936 - accuracy: 0.9210 - val_loss: 0.1730 - val_accuracy: 0.8990\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0928 - accuracy: 0.9216 - val_loss: 0.1634 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 103s 121ms/step - loss: 0.0921 - accuracy: 0.9225 - val_loss: 0.1239 - val_accuracy: 0.9495\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 108s 127ms/step - loss: 0.0914 - accuracy: 0.9230 - val_loss: 0.1486 - val_accuracy: 0.9596\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 108s 127ms/step - loss: 0.0909 - accuracy: 0.9235 - val_loss: 0.1538 - val_accuracy: 0.9394\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 103s 121ms/step - loss: 0.0904 - accuracy: 0.9240 - val_loss: 0.1678 - val_accuracy: 0.9394\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 103s 121ms/step - loss: 0.0898 - accuracy: 0.9245 - val_loss: 0.1377 - val_accuracy: 0.9596\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 106s 125ms/step - loss: 0.0896 - accuracy: 0.9248 - val_loss: 0.1155 - val_accuracy: 0.9596\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 112s 131ms/step - loss: 0.0893 - accuracy: 0.9253 - val_loss: 0.1592 - val_accuracy: 0.9394\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 125s 147ms/step - loss: 0.0889 - accuracy: 0.9252 - val_loss: 0.1184 - val_accuracy: 0.9697\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 124s 145ms/step - loss: 0.0884 - accuracy: 0.9258 - val_loss: 0.1405 - val_accuracy: 0.9697\n",
      "Epoch 23: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re2.h5\n",
      "34063/34063 [==============================] - 236s 7ms/step\n",
      "ModelAccuracy: 80.034%\n",
      "True Win Predictions Mean of all: 33.462%\n",
      "XXX Loss Buy Mean of all: 3.428%\n",
      "Missed good deal off all: 16.538%\n",
      "Good Zero prediction Mean: 46.572%\n",
      "good fiability\n",
      "========= Win Ratio:90.70750880997561 ====================\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_59 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 153s 167ms/step - loss: 0.1329 - accuracy: 0.8702 - val_loss: 0.2277 - val_accuracy: 0.8990\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 138s 161ms/step - loss: 0.1104 - accuracy: 0.8965 - val_loss: 0.2321 - val_accuracy: 0.9091\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 144s 170ms/step - loss: 0.0984 - accuracy: 0.9105 - val_loss: 0.2165 - val_accuracy: 0.9293\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0918 - accuracy: 0.9177 - val_loss: 0.1605 - val_accuracy: 0.9293\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 138s 162ms/step - loss: 0.0872 - accuracy: 0.9228 - val_loss: 0.1843 - val_accuracy: 0.9192\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0842 - accuracy: 0.9258 - val_loss: 0.1469 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 140s 165ms/step - loss: 0.0819 - accuracy: 0.9284 - val_loss: 0.1457 - val_accuracy: 0.9596\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 117s 137ms/step - loss: 0.0800 - accuracy: 0.9303 - val_loss: 0.1708 - val_accuracy: 0.9495\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 122s 144ms/step - loss: 0.0789 - accuracy: 0.9316 - val_loss: 0.1897 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 123s 145ms/step - loss: 0.0776 - accuracy: 0.9331 - val_loss: 0.1580 - val_accuracy: 0.9394\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 127s 149ms/step - loss: 0.0766 - accuracy: 0.9340 - val_loss: 0.1477 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 129s 151ms/step - loss: 0.0756 - accuracy: 0.9349 - val_loss: 0.1344 - val_accuracy: 0.9495\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 124s 146ms/step - loss: 0.0749 - accuracy: 0.9353 - val_loss: 0.1792 - val_accuracy: 0.9495\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 123s 145ms/step - loss: 0.0743 - accuracy: 0.9361 - val_loss: 0.1471 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 123s 145ms/step - loss: 0.0739 - accuracy: 0.9368 - val_loss: 0.1886 - val_accuracy: 0.9091\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 122s 143ms/step - loss: 0.0732 - accuracy: 0.9373 - val_loss: 0.1369 - val_accuracy: 0.9596\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 124s 145ms/step - loss: 0.0727 - accuracy: 0.9381 - val_loss: 0.1661 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 123s 144ms/step - loss: 0.0722 - accuracy: 0.9384 - val_loss: 0.1648 - val_accuracy: 0.9495\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 124s 145ms/step - loss: 0.0719 - accuracy: 0.9387 - val_loss: 0.1540 - val_accuracy: 0.9596\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 121s 142ms/step - loss: 0.0714 - accuracy: 0.9391 - val_loss: 0.1119 - val_accuracy: 0.9596\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 125s 147ms/step - loss: 0.0712 - accuracy: 0.9394 - val_loss: 0.1585 - val_accuracy: 0.9495\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 127s 149ms/step - loss: 0.0706 - accuracy: 0.9402 - val_loss: 0.1748 - val_accuracy: 0.9596\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 124s 145ms/step - loss: 0.0705 - accuracy: 0.9400 - val_loss: 0.1488 - val_accuracy: 0.9394\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 126s 148ms/step - loss: 0.0701 - accuracy: 0.9402 - val_loss: 0.1476 - val_accuracy: 0.9596\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 124s 146ms/step - loss: 0.0700 - accuracy: 0.9404 - val_loss: 0.1431 - val_accuracy: 0.9495\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 123s 145ms/step - loss: 0.0697 - accuracy: 0.9408 - val_loss: 0.1592 - val_accuracy: 0.9596\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 126s 147ms/step - loss: 0.0691 - accuracy: 0.9411 - val_loss: 0.1630 - val_accuracy: 0.9495\n",
      "Epoch 27: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re3.h5\n",
      "34063/34063 [==============================] - 250s 7ms/step\n",
      "ModelAccuracy: 77.124%\n",
      "True Win Predictions Mean of all: 29.327%\n",
      "XXX Loss Buy Mean of all: 2.203%\n",
      "Missed good deal off all: 20.673%\n",
      "Good Zero prediction Mean: 47.797%\n",
      "good fiability\n",
      "========= Win Ratio:93.01300348874089 ====================\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_64 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_66 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_67 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_106 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_107 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_108 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_109 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_110 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 156s 172ms/step - loss: 0.1152 - accuracy: 0.8746 - val_loss: 0.3874 - val_accuracy: 0.8081\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0926 - accuracy: 0.9058 - val_loss: 0.1719 - val_accuracy: 0.9091\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0810 - accuracy: 0.9206 - val_loss: 0.2072 - val_accuracy: 0.8990\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 145s 171ms/step - loss: 0.0745 - accuracy: 0.9284 - val_loss: 0.1053 - val_accuracy: 0.9798\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 142s 166ms/step - loss: 0.0703 - accuracy: 0.9335 - val_loss: 0.1218 - val_accuracy: 0.9697\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0674 - accuracy: 0.9373 - val_loss: 0.1066 - val_accuracy: 0.9697\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0655 - accuracy: 0.9395 - val_loss: 0.1442 - val_accuracy: 0.9394\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0639 - accuracy: 0.9414 - val_loss: 0.1431 - val_accuracy: 0.9495\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0626 - accuracy: 0.9430 - val_loss: 0.0955 - val_accuracy: 0.9899\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 143s 167ms/step - loss: 0.0617 - accuracy: 0.9442 - val_loss: 0.0982 - val_accuracy: 0.9697\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 139s 164ms/step - loss: 0.0609 - accuracy: 0.9450 - val_loss: 0.1194 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 140s 165ms/step - loss: 0.0600 - accuracy: 0.9460 - val_loss: 0.0858 - val_accuracy: 0.9798\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 143s 167ms/step - loss: 0.0594 - accuracy: 0.9468 - val_loss: 0.0898 - val_accuracy: 0.9798\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0590 - accuracy: 0.9473 - val_loss: 0.1043 - val_accuracy: 0.9697\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0583 - accuracy: 0.9480 - val_loss: 0.0963 - val_accuracy: 0.9899\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0579 - accuracy: 0.9484 - val_loss: 0.0869 - val_accuracy: 0.9798\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0575 - accuracy: 0.9493 - val_loss: 0.0805 - val_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0572 - accuracy: 0.9490 - val_loss: 0.0964 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 146s 172ms/step - loss: 0.0568 - accuracy: 0.9498 - val_loss: 0.0955 - val_accuracy: 0.9899\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0564 - accuracy: 0.9502 - val_loss: 0.0779 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 138s 162ms/step - loss: 0.0561 - accuracy: 0.9504 - val_loss: 0.0863 - val_accuracy: 0.9798\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0559 - accuracy: 0.9505 - val_loss: 0.0839 - val_accuracy: 0.9798\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 141s 166ms/step - loss: 0.0556 - accuracy: 0.9511 - val_loss: 0.0810 - val_accuracy: 1.0000\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 140s 164ms/step - loss: 0.0553 - accuracy: 0.9516 - val_loss: 0.0813 - val_accuracy: 0.9798\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0551 - accuracy: 0.9516 - val_loss: 0.0793 - val_accuracy: 1.0000\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 142s 166ms/step - loss: 0.0549 - accuracy: 0.9519 - val_loss: 0.0980 - val_accuracy: 0.9697\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0547 - accuracy: 0.9522 - val_loss: 0.0962 - val_accuracy: 0.9798\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0544 - accuracy: 0.9525 - val_loss: 0.0768 - val_accuracy: 1.0000\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 153s 180ms/step - loss: 0.0541 - accuracy: 0.9524 - val_loss: 0.0899 - val_accuracy: 0.9798\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0542 - accuracy: 0.9529 - val_loss: 0.0930 - val_accuracy: 0.9899\n",
      "Epoch 31/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0538 - accuracy: 0.9530 - val_loss: 0.0924 - val_accuracy: 0.9596\n",
      "Epoch 32/500\n",
      "852/852 [==============================] - 141s 166ms/step - loss: 0.0536 - accuracy: 0.9532 - val_loss: 0.1092 - val_accuracy: 0.9697\n",
      "Epoch 33/500\n",
      "852/852 [==============================] - 144s 170ms/step - loss: 0.0533 - accuracy: 0.9537 - val_loss: 0.0784 - val_accuracy: 0.9798\n",
      "Epoch 33: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re4.h5\n",
      "34063/34063 [==============================] - 252s 7ms/step\n",
      "ModelAccuracy: 75.777%\n",
      "True Win Predictions Mean of all: 27.574%\n",
      "XXX Loss Buy Mean of all: 1.797%\n",
      "Missed good deal off all: 22.426%\n",
      "Good Zero prediction Mean: 48.203%\n",
      "good fiability\n",
      "========= Win Ratio:93.88172006400872 ====================\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_69 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_111 (Dense)           (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_70 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_112 (Dense)           (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_113 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_114 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_115 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_116 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_117 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_118 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 150s 166ms/step - loss: 0.1075 - accuracy: 0.8765 - val_loss: 0.3980 - val_accuracy: 0.8485\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0836 - accuracy: 0.9112 - val_loss: 0.3986 - val_accuracy: 0.8485\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 146s 172ms/step - loss: 0.0721 - accuracy: 0.9268 - val_loss: 0.1729 - val_accuracy: 0.9293\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0660 - accuracy: 0.9348 - val_loss: 0.2162 - val_accuracy: 0.8990\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0622 - accuracy: 0.9397 - val_loss: 0.1991 - val_accuracy: 0.8788\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 146s 172ms/step - loss: 0.0596 - accuracy: 0.9430 - val_loss: 0.1650 - val_accuracy: 0.9192\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 146s 172ms/step - loss: 0.0575 - accuracy: 0.9457 - val_loss: 0.1275 - val_accuracy: 0.9495\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 149s 174ms/step - loss: 0.0563 - accuracy: 0.9473 - val_loss: 0.1415 - val_accuracy: 0.9192\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0549 - accuracy: 0.9492 - val_loss: 0.1403 - val_accuracy: 0.9394\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0539 - accuracy: 0.9505 - val_loss: 0.1100 - val_accuracy: 0.9596\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0530 - accuracy: 0.9518 - val_loss: 0.0996 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 149s 175ms/step - loss: 0.0523 - accuracy: 0.9523 - val_loss: 0.1651 - val_accuracy: 0.9394\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 148s 173ms/step - loss: 0.0519 - accuracy: 0.9531 - val_loss: 0.1251 - val_accuracy: 0.9495\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 141s 166ms/step - loss: 0.0513 - accuracy: 0.9535 - val_loss: 0.0997 - val_accuracy: 0.9697\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 142s 167ms/step - loss: 0.0509 - accuracy: 0.9539 - val_loss: 0.1181 - val_accuracy: 0.9596\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 153s 180ms/step - loss: 0.0505 - accuracy: 0.9548 - val_loss: 0.1060 - val_accuracy: 0.9596\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 149s 175ms/step - loss: 0.0500 - accuracy: 0.9553 - val_loss: 0.1312 - val_accuracy: 0.9394\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0497 - accuracy: 0.9555 - val_loss: 0.1051 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 148s 174ms/step - loss: 0.0493 - accuracy: 0.9560 - val_loss: 0.1187 - val_accuracy: 0.9394\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0490 - accuracy: 0.9565 - val_loss: 0.1320 - val_accuracy: 0.9596\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0486 - accuracy: 0.9569 - val_loss: 0.0903 - val_accuracy: 0.9697\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 139s 163ms/step - loss: 0.0484 - accuracy: 0.9570 - val_loss: 0.1119 - val_accuracy: 0.9495\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0482 - accuracy: 0.9571 - val_loss: 0.1084 - val_accuracy: 0.9697\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 143s 168ms/step - loss: 0.0479 - accuracy: 0.9577 - val_loss: 0.1094 - val_accuracy: 0.9697\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 141s 166ms/step - loss: 0.0476 - accuracy: 0.9582 - val_loss: 0.1138 - val_accuracy: 0.9798\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0474 - accuracy: 0.9583 - val_loss: 0.1107 - val_accuracy: 0.9596\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 146s 171ms/step - loss: 0.0474 - accuracy: 0.9583 - val_loss: 0.0990 - val_accuracy: 0.9495\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0472 - accuracy: 0.9586 - val_loss: 0.0924 - val_accuracy: 0.9798\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0469 - accuracy: 0.9587 - val_loss: 0.0923 - val_accuracy: 0.9697\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 146s 172ms/step - loss: 0.0467 - accuracy: 0.9591 - val_loss: 0.1402 - val_accuracy: 0.9495\n",
      "Epoch 31/500\n",
      "852/852 [==============================] - 151s 177ms/step - loss: 0.0464 - accuracy: 0.9592 - val_loss: 0.0738 - val_accuracy: 0.9798\n",
      "Epoch 32/500\n",
      "852/852 [==============================] - 145s 170ms/step - loss: 0.0464 - accuracy: 0.9595 - val_loss: 0.0982 - val_accuracy: 0.9798\n",
      "Epoch 33/500\n",
      "852/852 [==============================] - 144s 169ms/step - loss: 0.0463 - accuracy: 0.9595 - val_loss: 0.1386 - val_accuracy: 0.9293\n",
      "Epoch 34/500\n",
      "852/852 [==============================] - 132s 155ms/step - loss: 0.0461 - accuracy: 0.9599 - val_loss: 0.0691 - val_accuracy: 0.9899\n",
      "Epoch 35/500\n",
      "852/852 [==============================] - 130s 153ms/step - loss: 0.0459 - accuracy: 0.9602 - val_loss: 0.0930 - val_accuracy: 0.9697\n",
      "Epoch 36/500\n",
      "852/852 [==============================] - 131s 153ms/step - loss: 0.0456 - accuracy: 0.9602 - val_loss: 0.0936 - val_accuracy: 0.9798\n",
      "Epoch 37/500\n",
      "852/852 [==============================] - 130s 153ms/step - loss: 0.0457 - accuracy: 0.9603 - val_loss: 0.1179 - val_accuracy: 0.9697\n",
      "Epoch 38/500\n",
      "852/852 [==============================] - 126s 148ms/step - loss: 0.0454 - accuracy: 0.9602 - val_loss: 0.1037 - val_accuracy: 0.9596\n",
      "Epoch 39/500\n",
      "852/852 [==============================] - 128s 150ms/step - loss: 0.0454 - accuracy: 0.9602 - val_loss: 0.0938 - val_accuracy: 0.9899\n",
      "Epoch 40/500\n",
      "852/852 [==============================] - 130s 153ms/step - loss: 0.0453 - accuracy: 0.9607 - val_loss: 0.0682 - val_accuracy: 0.9899\n",
      "Epoch 41/500\n",
      "852/852 [==============================] - 124s 146ms/step - loss: 0.0451 - accuracy: 0.9613 - val_loss: 0.1045 - val_accuracy: 0.9596\n",
      "Epoch 42/500\n",
      "852/852 [==============================] - 129s 152ms/step - loss: 0.0448 - accuracy: 0.9614 - val_loss: 0.0922 - val_accuracy: 0.9798\n",
      "Epoch 43/500\n",
      "852/852 [==============================] - 127s 149ms/step - loss: 0.0450 - accuracy: 0.9611 - val_loss: 0.0934 - val_accuracy: 0.9596\n",
      "Epoch 44/500\n",
      "852/852 [==============================] - 129s 151ms/step - loss: 0.0447 - accuracy: 0.9616 - val_loss: 0.1016 - val_accuracy: 0.9596\n",
      "Epoch 45/500\n",
      "852/852 [==============================] - 128s 150ms/step - loss: 0.0447 - accuracy: 0.9615 - val_loss: 0.0827 - val_accuracy: 0.9798\n",
      "Epoch 46/500\n",
      "852/852 [==============================] - 131s 154ms/step - loss: 0.0445 - accuracy: 0.9614 - val_loss: 0.0763 - val_accuracy: 0.9697\n",
      "Epoch 47/500\n",
      "852/852 [==============================] - 130s 153ms/step - loss: 0.0445 - accuracy: 0.9617 - val_loss: 0.0969 - val_accuracy: 0.9697\n",
      "Epoch 48/500\n",
      "852/852 [==============================] - 133s 156ms/step - loss: 0.0443 - accuracy: 0.9619 - val_loss: 0.0862 - val_accuracy: 0.9899\n",
      "Epoch 49/500\n",
      "852/852 [==============================] - 130s 153ms/step - loss: 0.0442 - accuracy: 0.9620 - val_loss: 0.1187 - val_accuracy: 0.9596\n",
      "Epoch 50/500\n",
      "852/852 [==============================] - 131s 153ms/step - loss: 0.0443 - accuracy: 0.9619 - val_loss: 0.0844 - val_accuracy: 0.9596\n",
      "Epoch 50: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re5.h5\n",
      "34063/34063 [==============================] - 240s 7ms/step\n",
      "ModelAccuracy: 73.923%\n",
      "True Win Predictions Mean of all: 25.311%\n",
      "XXX Loss Buy Mean of all: 1.388%\n",
      "Missed good deal off all: 24.689%\n",
      "Good Zero prediction Mean: 48.612%\n",
      "good fiability\n",
      "========= Win Ratio:94.80130341960373 ====================\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_74 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_119 (Dense)           (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_120 (Dense)           (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_76 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_121 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_77 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_122 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_123 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_124 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_125 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 138s 152ms/step - loss: 0.0991 - accuracy: 0.8761 - val_loss: 0.3366 - val_accuracy: 0.8384\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 126s 148ms/step - loss: 0.0757 - accuracy: 0.9133 - val_loss: 0.2675 - val_accuracy: 0.9091\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 127s 150ms/step - loss: 0.0647 - accuracy: 0.9297 - val_loss: 0.2249 - val_accuracy: 0.8889\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 128s 150ms/step - loss: 0.0588 - accuracy: 0.9380 - val_loss: 0.2712 - val_accuracy: 0.9091\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 125s 146ms/step - loss: 0.0553 - accuracy: 0.9429 - val_loss: 0.1513 - val_accuracy: 0.9293\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 125s 146ms/step - loss: 0.0525 - accuracy: 0.9468 - val_loss: 0.2663 - val_accuracy: 0.8990\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 124s 145ms/step - loss: 0.0507 - accuracy: 0.9491 - val_loss: 0.1908 - val_accuracy: 0.9091\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 122s 143ms/step - loss: 0.0493 - accuracy: 0.9512 - val_loss: 0.1694 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 116s 136ms/step - loss: 0.0482 - accuracy: 0.9527 - val_loss: 0.1760 - val_accuracy: 0.9091\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0472 - accuracy: 0.9537 - val_loss: 0.1258 - val_accuracy: 0.9495\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0464 - accuracy: 0.9549 - val_loss: 0.1679 - val_accuracy: 0.9192\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0457 - accuracy: 0.9559 - val_loss: 0.1564 - val_accuracy: 0.9192\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 114s 133ms/step - loss: 0.0452 - accuracy: 0.9566 - val_loss: 0.1258 - val_accuracy: 0.9495\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0448 - accuracy: 0.9573 - val_loss: 0.0971 - val_accuracy: 0.9697\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0442 - accuracy: 0.9583 - val_loss: 0.1229 - val_accuracy: 0.9697\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0438 - accuracy: 0.9581 - val_loss: 0.1519 - val_accuracy: 0.9293\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0433 - accuracy: 0.9588 - val_loss: 0.1381 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0430 - accuracy: 0.9591 - val_loss: 0.1049 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 116s 137ms/step - loss: 0.0428 - accuracy: 0.9600 - val_loss: 0.1378 - val_accuracy: 0.9495\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0424 - accuracy: 0.9603 - val_loss: 0.1210 - val_accuracy: 0.9394\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 111s 131ms/step - loss: 0.0421 - accuracy: 0.9607 - val_loss: 0.1027 - val_accuracy: 0.9495\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0419 - accuracy: 0.9606 - val_loss: 0.1159 - val_accuracy: 0.9596\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 115s 135ms/step - loss: 0.0416 - accuracy: 0.9614 - val_loss: 0.1381 - val_accuracy: 0.9293\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0415 - accuracy: 0.9612 - val_loss: 0.1075 - val_accuracy: 0.9596\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0411 - accuracy: 0.9617 - val_loss: 0.1203 - val_accuracy: 0.9596\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0409 - accuracy: 0.9619 - val_loss: 0.1262 - val_accuracy: 0.9495\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 115s 135ms/step - loss: 0.0408 - accuracy: 0.9620 - val_loss: 0.1409 - val_accuracy: 0.9495\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 112s 131ms/step - loss: 0.0407 - accuracy: 0.9621 - val_loss: 0.1049 - val_accuracy: 0.9697\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0406 - accuracy: 0.9625 - val_loss: 0.1389 - val_accuracy: 0.9394\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0403 - accuracy: 0.9626 - val_loss: 0.1049 - val_accuracy: 0.9596\n",
      "Epoch 30: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re6.h5\n",
      "34063/34063 [==============================] - 222s 6ms/step\n",
      "ModelAccuracy: 72.088%\n",
      "True Win Predictions Mean of all: 23.231%\n",
      "XXX Loss Buy Mean of all: 1.143%\n",
      "Missed good deal off all: 26.769%\n",
      "Good Zero prediction Mean: 48.857%\n",
      "good fiability\n",
      "========= Win Ratio:95.31057684417821 ====================\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_79 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_80 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_129 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_130 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_131 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_132 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_133 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_134 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 138s 151ms/step - loss: 0.0896 - accuracy: 0.8783 - val_loss: 0.4913 - val_accuracy: 0.8081\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 128s 150ms/step - loss: 0.0671 - accuracy: 0.9173 - val_loss: 0.3697 - val_accuracy: 0.8687\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 129s 152ms/step - loss: 0.0565 - accuracy: 0.9344 - val_loss: 0.2509 - val_accuracy: 0.8788\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 131s 154ms/step - loss: 0.0508 - accuracy: 0.9428 - val_loss: 0.2435 - val_accuracy: 0.8990\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 131s 154ms/step - loss: 0.0477 - accuracy: 0.9474 - val_loss: 0.1470 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 135s 159ms/step - loss: 0.0454 - accuracy: 0.9509 - val_loss: 0.1511 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 134s 157ms/step - loss: 0.0437 - accuracy: 0.9539 - val_loss: 0.1211 - val_accuracy: 0.9697\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 133s 156ms/step - loss: 0.0424 - accuracy: 0.9555 - val_loss: 0.1535 - val_accuracy: 0.9293\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 133s 156ms/step - loss: 0.0414 - accuracy: 0.9569 - val_loss: 0.1216 - val_accuracy: 0.9798\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 119s 139ms/step - loss: 0.0406 - accuracy: 0.9581 - val_loss: 0.1308 - val_accuracy: 0.9596\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 112s 131ms/step - loss: 0.0398 - accuracy: 0.9592 - val_loss: 0.1034 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 111s 130ms/step - loss: 0.0392 - accuracy: 0.9600 - val_loss: 0.1384 - val_accuracy: 0.9394\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 111s 130ms/step - loss: 0.0387 - accuracy: 0.9607 - val_loss: 0.1129 - val_accuracy: 0.9697\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 107s 126ms/step - loss: 0.0383 - accuracy: 0.9610 - val_loss: 0.1242 - val_accuracy: 0.9596\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 110s 129ms/step - loss: 0.0380 - accuracy: 0.9619 - val_loss: 0.1062 - val_accuracy: 0.9697\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0375 - accuracy: 0.9628 - val_loss: 0.1054 - val_accuracy: 0.9596\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 110s 129ms/step - loss: 0.0372 - accuracy: 0.9631 - val_loss: 0.1887 - val_accuracy: 0.9091\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 110s 129ms/step - loss: 0.0370 - accuracy: 0.9635 - val_loss: 0.1217 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0365 - accuracy: 0.9641 - val_loss: 0.0861 - val_accuracy: 0.9798\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 112s 131ms/step - loss: 0.0364 - accuracy: 0.9639 - val_loss: 0.0873 - val_accuracy: 0.9798\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 110s 130ms/step - loss: 0.0361 - accuracy: 0.9645 - val_loss: 0.1319 - val_accuracy: 0.9495\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 110s 129ms/step - loss: 0.0358 - accuracy: 0.9648 - val_loss: 0.0980 - val_accuracy: 0.9697\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 109s 128ms/step - loss: 0.0356 - accuracy: 0.9653 - val_loss: 0.1087 - val_accuracy: 0.9798\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 111s 131ms/step - loss: 0.0354 - accuracy: 0.9654 - val_loss: 0.1005 - val_accuracy: 0.9596\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 110s 129ms/step - loss: 0.0350 - accuracy: 0.9660 - val_loss: 0.1174 - val_accuracy: 0.9596\n",
      "Epoch 25: early stopping\n",
      "/UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_true_win_model_Re7.h5\n",
      "34063/34063 [==============================] - 138s 4ms/step\n",
      "ModelAccuracy: 70.156%\n",
      "True Win Predictions Mean of all: 21.060%\n",
      "XXX Loss Buy Mean of all: 0.904%\n",
      "Missed good deal off all: 28.940%\n",
      "Good Zero prediction Mean: 49.096%\n",
      "good fiability\n",
      "========= Win Ratio:95.88417410307777 ====================\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_84 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_135 (Dense)           (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_136 (Dense)           (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_137 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_87 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_138 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_139 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_CLOSE/tp166_w16_max16min_Model_v6.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 104s 115ms/step - loss: 0.0810 - accuracy: 0.8823 - val_loss: 0.3671 - val_accuracy: 0.8283\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 97s 114ms/step - loss: 0.0580 - accuracy: 0.9233 - val_loss: 0.2802 - val_accuracy: 0.8990\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 98s 115ms/step - loss: 0.0486 - accuracy: 0.9396 - val_loss: 0.1913 - val_accuracy: 0.9091\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 97s 114ms/step - loss: 0.0438 - accuracy: 0.9476 - val_loss: 0.1993 - val_accuracy: 0.9293\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 99s 116ms/step - loss: 0.0406 - accuracy: 0.9525 - val_loss: 0.1931 - val_accuracy: 0.9293\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 100s 117ms/step - loss: 0.0385 - accuracy: 0.9559 - val_loss: 0.2041 - val_accuracy: 0.9394\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 114s 133ms/step - loss: 0.0370 - accuracy: 0.9579 - val_loss: 0.0905 - val_accuracy: 0.9596\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0359 - accuracy: 0.9596 - val_loss: 0.1323 - val_accuracy: 0.9293\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0350 - accuracy: 0.9611 - val_loss: 0.1630 - val_accuracy: 0.9394\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0341 - accuracy: 0.9626 - val_loss: 0.1342 - val_accuracy: 0.9394\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0335 - accuracy: 0.9636 - val_loss: 0.1116 - val_accuracy: 0.9495\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 117s 138ms/step - loss: 0.0329 - accuracy: 0.9646 - val_loss: 0.1485 - val_accuracy: 0.9495\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 107s 125ms/step - loss: 0.0325 - accuracy: 0.9652 - val_loss: 0.1408 - val_accuracy: 0.9596\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 111s 130ms/step - loss: 0.0321 - accuracy: 0.9654 - val_loss: 0.1029 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 117s 138ms/step - loss: 0.0317 - accuracy: 0.9662 - val_loss: 0.0872 - val_accuracy: 0.9596\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 114s 133ms/step - loss: 0.0312 - accuracy: 0.9669 - val_loss: 0.0904 - val_accuracy: 0.9697\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0311 - accuracy: 0.9673 - val_loss: 0.0623 - val_accuracy: 0.9798\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0308 - accuracy: 0.9680 - val_loss: 0.1004 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 112s 132ms/step - loss: 0.0304 - accuracy: 0.9679 - val_loss: 0.1014 - val_accuracy: 0.9495\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 112s 131ms/step - loss: 0.0303 - accuracy: 0.9682 - val_loss: 0.0785 - val_accuracy: 0.9697\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 114s 133ms/step - loss: 0.0301 - accuracy: 0.9687 - val_loss: 0.0976 - val_accuracy: 0.9495\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0298 - accuracy: 0.9691 - val_loss: 0.0837 - val_accuracy: 0.9798\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 115s 135ms/step - loss: 0.0297 - accuracy: 0.9692 - val_loss: 0.1032 - val_accuracy: 0.9697\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0295 - accuracy: 0.9695 - val_loss: 0.0855 - val_accuracy: 0.9697\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0291 - accuracy: 0.9698 - val_loss: 0.1049 - val_accuracy: 0.9495\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0291 - accuracy: 0.9698 - val_loss: 0.1104 - val_accuracy: 0.9495\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 114s 134ms/step - loss: 0.0291 - accuracy: 0.9704 - val_loss: 0.0879 - val_accuracy: 0.9596\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 113s 133ms/step - loss: 0.0288 - accuracy: 0.9706 - val_loss: 0.0974 - val_accuracy: 0.9394\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 113s 132ms/step - loss: 0.0287 - accuracy: 0.9703 - val_loss: 0.0785 - val_accuracy: 0.9596\n",
      "Epoch 30/500\n",
      "746/852 [=========================>....] - ETA: 15s - loss: 0.0285 - accuracy: 0.9710"
     ]
    }
   ],
   "source": [
    "#Change retaindt\n",
    "init_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "predictions = init_model.predict(dt[:, :-1])\n",
    "TrueWinPred = predictions.round()\n",
    "for rrr in range(1,10):\n",
    "    retrain_dt=dt\n",
    "    class_1_weight=TrueWinPred.mean()\n",
    "\n",
    "    import gc\n",
    "    from keras.layers import Dense\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import Nadam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "    # Define the class weights\n",
    "    index_20pct=int(retrain_dt.shape[1]*0.2)\n",
    "    class_weights = {0: 1-class_1_weight, 1: class_1_weight}\n",
    "    gc.collect()\n",
    "\n",
    "    SizeTunner = 1.5\n",
    "    IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"saving file in: \" + Model_FileName)\n",
    "    history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                        TrueWinPred[index_20pct:],\n",
    "                        validation_data=(retrain_dt[:index_20pct, :-1], TrueWinPred[:index_20pct]),\n",
    "                        epochs=500,\n",
    "                        batch_size=256*5,\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weights)\n",
    "\n",
    "    true_win_model=model\n",
    "    wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re{rrr}.h5\"\n",
    "    true_win_model.save(wheretosave)\n",
    "    print(wheretosave)\n",
    "    USED_MODEL=true_win_model\n",
    "    bad_Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "    Pred02=bad_Prediction_Note.round()\n",
    "    Original_Traget_Data=Y\n",
    "    Predicted_Data=Pred02[:,0]\n",
    "\n",
    "    BadTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    BadModelAccuracy=hp(BadTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "    BadTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    BadTrueWinPred_Mean=hp(BadTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "    BadLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    BadLossPred_Mean=hp(BadLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "    BadMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    BadMissedDeal_Mean=hp(BadMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "    BadGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    BadGoodZero_Mean=hp(BadGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "    fiability=BadTrueWinPred_Mean + BadLossPred_Mean + BadMissedDeal_Mean + BadGoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=BadTrueWinPred_Mean/(BadLossPred_Mean+BadTrueWinPred_Mean)\n",
    "    print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "    TrueWinPred=BadTrueWinPred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratif model v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Conv1D, Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Reshape\n",
    "\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 0.2\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Reshape((1, IN_DIM), input_shape=(IN_DIM,)))\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(128, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, activation='elu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = SGD(lr=0.0001, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "                    monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=15, verbose=1)\n",
    "]\n",
    "\n",
    "print(f\"saving file in: {DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5\")\n",
    "\n",
    "# Reshape the input data to have a single channel\n",
    "X_train = dt[index_20pct:, :-1]\n",
    "y_train = dt[index_20pct:, -1]\n",
    "X_val = dt[:index_20pct, :-1]\n",
    "y_val = dt[:index_20pct, -1]\n",
    "\n",
    "\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=30,\n",
    "                    batch_size=256*5,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "minicnn_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5'\n",
    "model.save(minicnn_model_file)\n",
    "print(minicnn_model_file)\n",
    "minicnn_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "class_weights = {0: 3., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 2\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='softplus'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = Nadam(lr=0.005, beta_1=0.8, beta_2=0.99, clipnorm=1.0) #faster\n",
    "#optimizer = Nadam(lr=0.0005, beta_1=0.95, beta_2=0.999, epsilon=1e-08) #optimizer more accurate\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=20, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\")\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win-Loss Double Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep neural network to predict a binary outcome (win or loss) and applying class weights to the loss function. To implement cost-sensitive learning or ensemble methods, you can make the following modifications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# # Custom cost-sensitive loss function\n",
    "# def cost_sensitive_loss(y_true, y_pred):\n",
    "#     cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    \n",
    "#     y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "#     y_pred_1_probs = 1 - y_pred_probs\n",
    "    \n",
    "#     y_true_int = tf.cast(y_true, tf.int32)\n",
    "#     cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "#     loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# # Function to create the model\n",
    "# def create_model(input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "#     model.add(Dense(200, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Prepare the data\n",
    "# retrain_dt = dt\n",
    "# index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "# X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "# y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "# # Define the optimizer and callbacks\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model for class 1 (win)\n",
    "# model_win = create_model(input_dim=IN_DIM)\n",
    "# model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "# model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Train the model for class 0 (loss)\n",
    "# model_loss = create_model(input_dim=IN_DIM)\n",
    "# model_loss.compile(optimizer=optimizer, loss=cost_sensitive_loss,metrics=['accuracy'])\n",
    "# model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Combine the predictions from both models\n",
    "# win_preds = model_win.predict(dt[:, 0:-1])\n",
    "# loss_preds = model_loss.predict(dt[:, 0:-1])\n",
    "\n",
    "# # Use a strategy such as averaging, voting, or another combination method\n",
    "# combined_preds = (win_preds + loss_preds) / 2\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Additional metrics\n",
    "# true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "# false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "# true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "# false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "# print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "# print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "# print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "# print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate\n",
    "# from keras.models import Model\n",
    "\n",
    "# # 1. Freeze the weights of model_win and model_loss\n",
    "# for layer in model_win.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# for layer in model_loss.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # 2. Add an additional layer to each model to obtain the intermediate features\n",
    "# model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "# model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# # 3. Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "# combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "# combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# # 4. Train the new model to make predictions using the intermediate features from both models\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the combined model\n",
    "# combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Custom cost-sensitive loss function\n",
    "def cost_sensitive_loss(y_true, y_pred):\n",
    "    cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    y_pred_1_probs = 1 - y_pred_probs\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "    loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "    model.add(Dense(200, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, loss_function):\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Prepare the data\n",
    "retrain_dt = dt\n",
    "index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "\n",
    "# Train the model for class 1 (win)\n",
    "model_win = create_model(input_dim=IN_DIM)\n",
    "model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Train the model for class 0 (loss)\n",
    "model_loss = create_model(input_dim=IN_DIM)\n",
    "model_loss.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# Freeze the weights of model_win and model_loss\n",
    "for layer in model_win.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model_loss.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add an additional layer to each model\n",
    "# Add an additional layer to each model to obtain the intermediate features\n",
    "model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# Train the new model to make predictions using the intermediate features from both models\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the combined model\n",
    "combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Make predictions using the combined model\n",
    "combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "# Create a new input layer for the single input\n",
    "single_input = Input(shape=(IN_DIM,))\n",
    "\n",
    "# Duplicate the input for the two original models\n",
    "input_duplication = Lambda(lambda x: tf.tile(tf.expand_dims(x, axis=1), [1, 2, 1]))(single_input)\n",
    "input_for_model_win = Lambda(lambda x: x[:, 0])(input_duplication)\n",
    "input_for_model_loss = Lambda(lambda x: x[:, 1])(input_duplication)\n",
    "\n",
    "# Feed the duplicated input into the original models\n",
    "model_win_output = model_win(input_for_model_win)\n",
    "model_loss_output = model_loss(input_for_model_loss)\n",
    "\n",
    "# Combine the outputs of the original models\n",
    "combined_input = concatenate([model_win_output, model_loss_output])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# Create the new model\n",
    "single_input_combined_model = Model(inputs=single_input, outputs=combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_preds = single_input_combined_model.predict(dt[:, 0:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very_deep_good_model=model\n",
    "wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_CombinedLOssWin.h5\"\n",
    "combined_model.save(wheretosave)\n",
    "print(wheretosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test On special coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Data\n",
    "\n",
    "# MAX_FORCAST_SIZE=40\n",
    "Mlist=[              f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re2.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re4.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re5.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re6.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re7.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re8.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re9.h5\",\n",
    "                       f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\",\n",
    "    \n",
    "                        ]\n",
    "\n",
    "# Mlist=[              \n",
    "#                        '/UltimeTradingBot/Data/IS_CLOSE/TP270.0_MHT240min_Model_Re0.h5',\n",
    "#                        '/UltimeTradingBot/Data/IS_CLOSE/TP270.0_MHT240min_Model_Re2.h5',\n",
    "#                        '/UltimeTradingBot/Data/IS_CLOSE/TP270.0_MHT240min_Model_Re4.h5',\n",
    "#                         '/UltimeTradingBot/Data/IS_CLOSE/TP270.0_MHT240min_Model_Re1.h5',\n",
    "#                         '/UltimeTradingBot/Data/IS_CLOSE/TP270.0_MHT240min_Model_Re3.h5',\n",
    "#                                                 ]\n",
    "\n",
    "BAD_PERIOD_START=\"2022-08-30\"\n",
    "BAD_PERIOD_END=\"2022-11-22\"\n",
    "pair_to_test=\"APE/USDT\"\n",
    "MAX_FORCAST_SIZE=120\n",
    "\n",
    "BUY_PCT_TEST=1.5\n",
    "loc_start=0\n",
    "loc_end=1000000\n",
    "\n",
    "\n",
    "i_start=71000\n",
    "i_end=i_start+200\n",
    "\n",
    "# loc_start=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_START))\n",
    "# loc_end=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_END))\n",
    "\n",
    "pair=pair_to_test\n",
    "OnePair_DF=maxi_expand(pair=pair,i=loc_start,j=loc_end,window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT_TEST,SELL_PCT=SELL_PCT,buy_function=is_high_win,\n",
    "                           w1m=6,w5m=10,w15m=50,w1h=8,w1d=7,\n",
    "                           btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15)\n",
    "\n",
    "\n",
    "\n",
    "OnePair_DT=OnePair_DF.to_numpy()\n",
    "gc.collect()\n",
    "OnePair_DT=fixdt(OnePair_DT)\n",
    "print(OnePair_DT[0,0] == OnePair_DF.iloc[0,0])\n",
    "print(OnePair_DT[5,5] == OnePair_DF.iloc[5,5])\n",
    "hp(OnePair_DF.buy.mean(),\"Buy mean pct\")\n",
    "\n",
    "\n",
    "\n",
    "plot_data(\"Original\", pair_to_test, winratio, OnePair_DF, i_start, 600, OnePair_DF.buy,dot_color=\"r\",fig_width=20, fig_height=7)\n",
    "\n",
    "for Model_FileName in Mlist:\n",
    "\n",
    "    USED_MODEL=load_model(Model_FileName)\n",
    "\n",
    "\n",
    "    OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "    OnePair_Pred=OnePair_PredNote.round()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    Original_Traget_Data=OnePair_DT[:,-1]\n",
    "    Predicted_Data=OnePair_Pred[:,0]\n",
    "    gc.collect()\n",
    "    TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "    gc.collect()\n",
    "    TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "    gc.collect()\n",
    "    LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "    gc.collect()\n",
    "\n",
    "    MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "    gc.collect()\n",
    "\n",
    "    GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "    gc.collect()\n",
    "\n",
    "    fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "    print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "\n",
    "\n",
    "    plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 600, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## External and other Models\n",
    "\n",
    "\n",
    "Mlist=[ f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "    \n",
    "                        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_data(\"Original\", pair_to_test, winratio, OnePair_DF, i_start, 600, OnePair_DF.buy,dot_color=\"r\",fig_width=20, fig_height=7)\n",
    "\n",
    "for Model_FileName in Mlist:\n",
    "\n",
    "    USED_MODEL=load_model(Model_FileName)\n",
    "\n",
    "\n",
    "    OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "    OnePair_Pred=OnePair_PredNote.round()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    Original_Traget_Data=OnePair_DT[:,-1]\n",
    "    Predicted_Data=OnePair_Pred[:,0]\n",
    "    gc.collect()\n",
    "    TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "    gc.collect()\n",
    "    TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "    gc.collect()\n",
    "    LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "    gc.collect()\n",
    "\n",
    "    MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "    gc.collect()\n",
    "\n",
    "    GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "    gc.collect()\n",
    "\n",
    "    fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "    print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "\n",
    "\n",
    "    plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 600, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start+15000, 1000, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, window_size, PREDICTION_TO_TEST, dot_color=\"g\", fig_width=20, fig_height=5):\n",
    "    i_end = i_start + window_size\n",
    "    mname = Model_FileName.replace(\"/UltimeTradingBot/Data\",\"\")\n",
    "    coin = pair_to_test.replace('/', '-')\n",
    "    mtitle = f\"{coin} WinRatio:{hp(winratio)}% - {mname}\".replace(\"/\", \"-\")\n",
    "\n",
    "    x = np.linspace(0, 10, 500)\n",
    "    dashes = []  # 10 points on, 5 off, 100 on, 5 off\n",
    "    fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "    line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "                 label='price',c=\"w\")\n",
    "    line1.set_dashes(dashes)\n",
    "    plt.plot(OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].price, 'ro',c=dot_color,markersize=5)\n",
    "    plt.title(mtitle)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 300, PREDICTION_TO_TEST,dot_color=\"g\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "precision=0.0\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "Predicted_Data=OnePair_Pred[:300000,0]\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(XX,YY,USEDMODEL)\n",
    "XX=OnePair_DT[:100000,:-1]\n",
    "YY=OnePair_DT[:100000,-1]\n",
    "\n",
    "# Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# Initial_Pred_Note=model_init.predict( XX)\n",
    "goodp=(Good_Prediction_Note-precision).round()\n",
    "# badp=(Bad_Prediction_Note).round()\n",
    "# initp=Initial_Pred_Note.round()\n",
    "\n",
    "# Original_Traget_Data=YY\n",
    "\n",
    "#Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
