{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single AI crypto concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINDOW:240 - BUY_PCT:2.7 MAX_FORCAST_SIZE:240 - BUY_MODE:IS_GOOD \n"
     ]
    }
   ],
   "source": [
    "from xdata_config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Imports and fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 01:43:02.916085: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-01 01:43:02.916120: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-01 01:43:02.963712: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-05-01 01:43:03.789937: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-01 01:43:03.790050: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-01 01:43:03.790063: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 3900.05 MB\n"
     ]
    }
   ],
   "source": [
    "from functions_module import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.constraints import MaxNorm\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "from keras.models import load_model\n",
    "from keras.layers import BatchNormalization, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_optimal_5m(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=MAX_FORCAST_SIZE):\n",
    "    #df = df.fillna(0)\n",
    "    mino = BUY_PCT / 100.0\n",
    "    maxo = SELL_PCT / 100.0\n",
    "    window=max(7,int(window/5))\n",
    "    max_forecast_size=window#MAX_FORCAST_SIZE\n",
    "    try:\n",
    "        after_dip_val=AFTER_MARK\n",
    "    except Exception:\n",
    "        after_dip_val=3\n",
    "    print(f\"after mark = : {after_dip_val}\")\n",
    "    try:\n",
    "        print(f\"optimalbuy buy maximum forcast size={max_forecast_size} at {BUY_PCT}% of the current price \")\n",
    "    except Exception:\n",
    "        max_forecast_size = 3\n",
    "        print(\"optimalbuy buy default window=3\")\n",
    "\n",
    "    rolling_max_close_diff = ((df['close-1_5min'].rolling(window=window).max().shift(-window+1) / df['close']) - 1).fillna(0)\n",
    "    df['buy']=(rolling_max_close_diff >= mino).astype(int)\n",
    "\n",
    "    # Compute rolling minimum values\n",
    "    window_list=[window]#[3, 5, 7, 10, 15, 20]\n",
    "\n",
    "    for window_size in window_list:\n",
    "        col_name = f'ismin{window_size}'\n",
    "        rolling_min = (df['close'].shift(after_dip_val) <= df.shift(-window_size-1)['close-1_5min'].rolling(2*window_size).min())\n",
    "        df = df.assign(**{col_name: rolling_min.astype(int)})\n",
    "\n",
    "    df['ismin'] = df[[f'ismin{window_size}' for window_size in window_list ]].any(axis=1).astype(int)        \n",
    "\n",
    "    # # Compute buy and sell signals\n",
    "    rolling_low_close_diff =  ((df['low-1_5min'].rolling(window=int(window/2)).min().shift(-int(window/2)+1)/ df['close'] ) -1).fillna(0)\n",
    "    df['sell'] = (rolling_low_close_diff <= -maxo).astype(int)\n",
    "\n",
    "\n",
    "    # Compute final buy signal\n",
    "    df['buy'] = ((df['buy'] == 1) & (df['sell'] == 0) & (df['ismin'] == 1)).astype(int)\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=['sell', 'ismin'] + [f'ismin{window_size}' for window_size in window_list], errors='ignore')\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def full_expand_costum(df1m,df5m,df15m,df1h,df1d,w1m=10,w5m=30,w15m=30,w1h=3,w1d=7):\n",
    "    d1min=df1m.copy()\n",
    "    d1min=expand_previous(d1min,window=w1m).drop(columns=[\"volume\"])\n",
    "    d1min=rapid1d_expand(d1min,df1d,w1d)\n",
    "    d1min=rapid1h_expand(d1min,df1h,w1h)\n",
    "    d1min=rapid15m_expand(d1min,df15m,w15m)\n",
    "    d1min=rapid5m_expand(d1min,df5m,w5m)\n",
    "    return d1min\n",
    "\n",
    "def maxi_expand(pair=\"GMT/USDT\", i=0, j=10000, window=2, metadata=MetaData,\n",
    "                 high_weight=1, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT,\n",
    "                 buy_function=buy_alwase,w1m=6,w5m=30,w15m=30,w1h=3,w1d=7,btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=30,btc_w1d=30):\n",
    "    start_index=i\n",
    "    end_index=j\n",
    "    window_size=window\n",
    "    buy_fn=buy_function\n",
    "    \"\"\"\n",
    "    This function takes in several parameters to calculate technical indicators and returns a merged dataframe.\n",
    "    \n",
    "    :param pair: str, default \"GMT/USDT\"\n",
    "        The trading pair to analyze.\n",
    "        \n",
    "    :param start_index: int, default 0\n",
    "        The start index for selecting data.\n",
    "        \n",
    "    :param end_index: int, default 10000\n",
    "        The end index for selecting data.\n",
    "    \n",
    "    :param window_size: int, default 2\n",
    "        The window size to use for analyzing the data.\n",
    "    \n",
    "    :param metadata: MetaData\n",
    "        The metadata to use for analyzing the data.\n",
    "    \n",
    "    :param high_weight: int, default 1\n",
    "        The weight to use for calculating the high.\n",
    "    \n",
    "    :param BUY_PCT: float, default BUY_PCT\n",
    "        The buy pct to use for analyzing the data.\n",
    "    \n",
    "    :param SELL_PCT: float, default SELL_PCT\n",
    "        The sell pct to use for analyzing the data.\n",
    "    \n",
    "    :param buy_fn: function, default buy_min_up\n",
    "        The buy function to use for analyzing the data.\n",
    "    \n",
    "    :return: pd.DataFrame\n",
    "        A merged dataframe containing the calculated technical indicators.\n",
    "    \"\"\"\n",
    "    print(f\"maxi custum expend : {pair} with those parameters: w1m={w1m},w5m={w5m},w15m={w15m},w1h={w1h},w1d={w1d} btc_w1m={btc_w1m},btc_w5m={btc_w5m},btc_w15m={btc_w15m},btc_w1h={btc_w1h},btc_w1d={btc_w1d}\")\n",
    "    # Select data\n",
    "    pair_df = df_list1m[pair].iloc[start_index:end_index]\n",
    "    btc_df = df_list1m[\"BTC/USDT\"].loc[(pair_df.index[0] - pd.DateOffset(days=window_size+1)).round(freq='1 min'):pair_df.index[-1]+pd.Timedelta(f\"{window_size} day\")]\n",
    "    # Calculate technical indicators\n",
    "    pair_full = full_expand_costum(pair_df, df_list5m[pair], df_list15m[pair], df_list1h[pair], df_list1d[pair],w1m=w1m,w5m=w5m,w15m=w15m,w1h=w1h,w1d=w1d)\n",
    "    btc_full = full_expand_costum(btc_df, df_list5m[\"BTC/USDT\"], df_list15m[\"BTC/USDT\"], df_list1h[\"BTC/USDT\"], df_list1d[\"BTC/USDT\"], w1m=btc_w1m,w5m=btc_w5m,w15m=btc_w15m,w1h=btc_w1h,w1d=btc_w1d)   \n",
    "    btc_full = btc_full.add_prefix(\"BTC_\")\n",
    "    merged = pd.merge(pair_full, btc_full, left_index=True, right_index=True)\n",
    "    day_expand(merged)\n",
    "    Meta_expand(merged, metadata, pair)\n",
    "    #print(merged.columns)\n",
    "    merged = buy_fn(merged, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=MAX_FORCAST_SIZE)\n",
    "    merged[\"high\"] = (merged[\"open\"] + high_weight * merged[\"high\"] + merged[\"low\"] + merged[\"close\"]) / (3 + high_weight)\n",
    "    merged[\"BTC_high\"] = (merged[\"BTC_open\"] + high_weight * merged[\"BTC_high\"] + merged[\"BTC_low\"] + merged[\"BTC_close\"]) / (3 + high_weight)\n",
    "    merged.rename(columns={\"high\":\"price\"},inplace=True)\n",
    "    merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace=True)\n",
    "    merged = merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    open_high_low_close_cols = merged.columns.str.contains(\"open|high|low|close\")\n",
    "    # merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"BTC_price\"] - merged.loc[:, open_high_low_close_cols & merged.columns.str.contains(\"BTC\")]) / merged[\"BTC_price\"]\n",
    "    # )\n",
    "    # merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")] = (\n",
    "    #     (merged[\"price\"] - merged.loc[:, open_high_low_close_cols & ~merged.columns.str.contains(\"BTC\")]) / merged[\"price\"]\n",
    "    # )\n",
    "    for key in merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"BTC_price\"]-merged[key])/merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "    key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            merged[key]=(merged[\"price\"]-merged[key])/merged[\"price\"]\n",
    "\n",
    "    merged=merged.dropna()\n",
    "    print(f'######################  max expend {pair} - shape {merged.shape}  buy mean : {hp(merged.buy.mean())} ############################')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buy_optimal(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=3):\n",
    "    #df = df.fillna(0)\n",
    "    mino = BUY_PCT / 100.0\n",
    "    maxo = SELL_PCT / 100.0\n",
    "    window=15\n",
    "    max_forecast_size=15#MAX_FORCAST_SIZE\n",
    "    try:\n",
    "        after_dip_val=1\n",
    "    except Exception:\n",
    "        after_dip_val=1\n",
    "    print(f\"after mark = : {after_dip_val}\")\n",
    "    try:\n",
    "        print(f\"optimalbuy buy maximum forcast size={max_forecast_size} at {BUY_PCT}% of the current price \")\n",
    "    except:\n",
    "        max_forecast_size = 3\n",
    "        print(\"optimalbuy buy default window=3\")\n",
    "\n",
    "    rolling_max_close_diff = ((df['close'].rolling(window=window).max().shift(-window+1) / df['close']) - 1).fillna(0)\n",
    "    df['buy']=(rolling_max_close_diff >= mino).astype(int)\n",
    "\n",
    "    # Compute rolling minimum values\n",
    "\n",
    "    window_list=[5,window]#[3, 5, 7, 10, 15, 20]\n",
    "\n",
    "    for window_size in window_list:\n",
    "        col_name = f'ismin{window_size}'\n",
    "        rolling_min = (df['close'].shift(after_dip_val) <= df.shift(-window_size-1)['close'].rolling(2*window_size).min())\n",
    "        df = df.assign(**{col_name: rolling_min.astype(int)})\n",
    "\n",
    "    df['ismin'] = df[[f'ismin{window_size}' for window_size in window_list ]].any(axis=1).astype(int)        \n",
    "\n",
    "    # # Compute buy and sell signals\n",
    "    rolling_low_close_diff =  ((df['low'].rolling(window=int(window/2)).min().shift(-int(window/2)+1)/ df['close'] ) -1).fillna(0)\n",
    "    df['sell'] = (rolling_low_close_diff <= -maxo).astype(int)\n",
    "\n",
    "\n",
    "    # Compute final buy signal\n",
    "    df['buy'] = ((df['buy'] == 1) & (df['sell'] == 0) & (df['ismin'] == 1)).astype(int)\n",
    "    # Remove unnecessary columns\n",
    "    df = df.drop(columns=['sell', 'ismin'] + [f'ismin{window_size}' for window_size in window_list], errors='ignore')\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_local_min(df, BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_near_min_v1(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5, num_values=3):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum\n",
    "    or near the local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "    num_values (int): The number of values near the local minimum to include.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum and its neighbors.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum or near the local minimum\n",
    "    close_diff = np.abs(df['close'] - rolling_min_close)\n",
    "    threshold = close_diff.nsmallest(num_values+1).iloc[-1]\n",
    "    local_min = (close_diff <= threshold).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    return df\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def is_near_min(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5, num_values=3):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a local minimum\n",
    "    or within the specified range before or after the local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "    num_values (int): The number of values before and after the local minimum to include.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating local minimum and its neighbors.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min\n",
    "\n",
    "    # Include values before and after the local minimum\n",
    "    for i in range(1, num_values+1):\n",
    "        df['buy'] = df['buy'] | df['buy'].shift(-i) | df['buy'].shift(i)\n",
    "\n",
    "    # Fill any NaN values introduced by shifting with 0\n",
    "    df['buy'].fillna(0, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_max_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=7, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close)\n",
    "\n",
    "    # Compute maximum price change over next `window` rows\n",
    "    # max_price = df['close'].rolling(window=window, min_periods=1).max()\n",
    "    # max_price_shifted = max_price.shift(-window)\n",
    "    # max_price_change = ((max_price_shifted - max_price)/max_price).fillna(0)\n",
    "    max_price = df['close'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (local_min & win).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_close_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "\n",
    "    max_price = df['close'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (win).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "def is_high_win(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is a winning trade.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    BUY_PCT (float): Minimum percentage change required for a trade to be profitable.\n",
    "    window (int): The window size for computing maximum price change.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating winning trades.\n",
    "    \"\"\"\n",
    "    max_price = df['high'].shift(periods=-window+1).rolling(window=window, min_periods=1).max()#.pct_change(window=window-1)\n",
    "\n",
    "    # Check if the maximum price change is greater than the BUY_PCT threshold\n",
    "    win = (max_price >= (BUY_PCT/100+1)*df['close'])\n",
    "\n",
    "    # Combine local minimum and winning trade conditions\n",
    "    df['buy'] = (win).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def is_local_min_plus_5(df, BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT, window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is 5 minutes after a local minimum.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    window (int): The window size for computing local minimum.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating 5 minutes after a local minimum.\n",
    "    \"\"\"\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Shift local minimum by 5 rows\n",
    "    local_min_plus_5 = local_min.shift(AFTER_MARK).fillna(0).astype(int)\n",
    "\n",
    "    # Add the local minimum column to the DataFrame\n",
    "    df['buy'] = local_min_plus_5\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=maxi_expand(pair=pair,i=0,j=len(df_list1m[pair]),window=2,metadata=MetaData,BUY_PCT=1.7,SELL_PCT=0.3,buy_function=is_local_min)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special list if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binance_USDT_HALAL.index(\"ROSE/USDT\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chking import\n",
    "# MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 17] File exists: '/UltimeTradingBot/Data/IS_GOOD'\n",
      "Results dir: /UltimeTradingBot/Data/IS_GOOD\n"
     ]
    }
   ],
   "source": [
    "if BUY_MODE==\"BUY_ONLY\":\n",
    "    buy_function=buy_up_only\n",
    "elif BUY_MODE==\"BUY_UP\":\n",
    "    buy_function=buy_up\n",
    "elif  BUY_MODE==\"BUY_DIP\":\n",
    "    buy_function=buy_min_up\n",
    "elif  BUY_MODE==\"AFTER_DEPTH\":\n",
    "    buy_function=buy_after_depth\n",
    "elif  BUY_MODE==\"BUY_UP_CLOSE\":\n",
    "    buy_function=buy_up_close\n",
    "elif  BUY_MODE==\"AFTER_DEPTH_CLOSE\":\n",
    "    buy_function=buy_after_depth_close\n",
    "elif  BUY_MODE==\"BUY_TEST\":\n",
    "    buy_function=buy_test\n",
    "elif BUY_MODE==\"BUY_MIN_CLOSE\":\n",
    "    buy_function=buy_min_close\n",
    "elif  BUY_MODE==\"SELL_TEST\":\n",
    "    buy_function=sell_test\n",
    "elif  BUY_MODE==\"BUY_FIX\":\n",
    "    buy_function=buy_fix\n",
    "elif  BUY_MODE==\"BUY_OPTIMAL\":\n",
    "    buy_function=buy_optimal\n",
    "elif  BUY_MODE==\"IS_MIN\":\n",
    "    buy_function=is_local_min\n",
    "elif  BUY_MODE==\"IS_5MIN\":\n",
    "    buy_function=is_local_min_plus_5\n",
    "elif  BUY_MODE==\"IS_HIGH\":\n",
    "    buy_function=is_high_win\n",
    "elif  BUY_MODE==\"IS_CLOSE\":\n",
    "    buy_function=is_close_win\n",
    "elif BUY_MODE==\"IS_GOOD\":\n",
    "    buy_function=is_good_to_buy\n",
    "try:\n",
    "    os.mkdir(DATA_DIR, mode = 0o777)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "print(f\"Results dir: {DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def sma(df, column='close', window=10):\n",
    "    return df[column].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    \n",
    "def is_local_min_plus_5_with_sma(df,BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT , window=5):\n",
    "    \"\"\"\n",
    "    Add a binary column to the OHLCV DataFrame to indicate if the close price in the row is 5 minutes after a local minimum\n",
    "    and confirms a change in trend using Simple Moving Averages.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the OHLCV data.\n",
    "    short_sma_period (int): The window size for the short-term Simple Moving Average.\n",
    "    long_sma_period (int): The window size for the long-term Simple Moving Average.\n",
    "    window (int): The window size for computing local minimum.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The input DataFrame with an additional column indicating 5 minutes after a local minimum\n",
    "                      and a change in trend.\n",
    "    \"\"\"\n",
    "    short_sma_period=10\n",
    "    long_sma_period=30\n",
    "    # Compute rolling minimum close\n",
    "    rolling_min_close = df['close'].rolling(window=window, center=True, min_periods=1).min()\n",
    "\n",
    "    # Check if close price is a local minimum\n",
    "    local_min = (df['close'] == rolling_min_close).astype(int)\n",
    "\n",
    "    # Shift local minimum by 5 rows\n",
    "    local_min_plus_5 = local_min.shift(5).fillna(0).astype(int)\n",
    "\n",
    "    # Calculate short-term and long-term Simple Moving Averages\n",
    "    df['short_sma'] = sma(df, window=short_sma_period)\n",
    "    df['long_sma'] = sma(df, window=long_sma_period)\n",
    "\n",
    "    # Identify trend change by comparing short-term and long-term SMAs\n",
    "    trend_change = (df['short_sma'] > df['long_sma']).astype(int)\n",
    "\n",
    "    # Combine local minimum and trend change conditions\n",
    "    df['buy'] = (local_min_plus_5 & trend_change)\n",
    "\n",
    "    # Clean up DataFrame by removing unnecessary columns\n",
    "    df.drop(columns=['short_sma', 'long_sma'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def rsi(df, column='close', window=14):\n",
    "    delta = df[column].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(df, column='close', short_period=12, long_period=26, signal_period=9):\n",
    "    exp1 = df[column].ewm(span=short_period, adjust=False).mean()\n",
    "    exp2 = df[column].ewm(span=long_period, adjust=False).mean()\n",
    "    macd_line = exp1 - exp2\n",
    "    signal_line = macd_line.ewm(span=signal_period, adjust=False).mean()\n",
    "    return macd_line, signal_line\n",
    "\n",
    "def sma(df, column='close', window=10):\n",
    "    return df[column].rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "def buy_signal_dl(df,BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT , window=5):\n",
    "    rsi_period=14\n",
    "    macd_short_period=12\n",
    "    macd_long_period=26\n",
    "    macd_signal_period=9\n",
    "    short_sma_period=10\n",
    "    long_sma_period=30\n",
    "    target_pct=BUY_PCT\n",
    "    # Calculate RSI\n",
    "    df['rsi'] = rsi(df, window=rsi_period)\n",
    "\n",
    "    # Calculate MACD and MACD Signal\n",
    "    df['macd'], df['macd_signal'] = macd(df, short_period=macd_short_period, long_period=macd_long_period, signal_period=macd_signal_period)\n",
    "\n",
    "    # Calculate short-term and long-term Simple Moving Averages\n",
    "    df['short_sma'] = sma(df, window=short_sma_period)\n",
    "    df['long_sma'] = sma(df, window=long_sma_period)\n",
    "\n",
    "    # Identify trend change by comparing short-term and long-term SMAs\n",
    "    trend_change = (df['short_sma'] > df['long_sma']).astype(int)\n",
    "\n",
    "    # RSI Oversold Condition\n",
    "    oversold = (df['rsi'] < 30).astype(int)\n",
    "\n",
    "    # MACD Crossover\n",
    "    macd_crossover = ((df['macd'] > df['macd_signal']) & (df['macd'].shift(1) <= df['macd_signal'].shift(1))).astype(int)\n",
    "\n",
    "    # Combine conditions\n",
    "    combined_signal = (trend_change & oversold & macd_crossover)\n",
    "\n",
    "    # Calculate future price after window periods\n",
    "    future_price = df['close'].shift(-window)\n",
    "\n",
    "    # Calculate the percentage change between the current and future price\n",
    "    pct_change = ((future_price - df['close']) / df['close']) * 100\n",
    "\n",
    "    # Check if the percentage change is greater than the target percentage\n",
    "    target_reached = (pct_change >= target_pct).astype(int)\n",
    "\n",
    "    # Combine combined_signal and target_reached conditions\n",
    "    df['buy'] = (combined_signal & target_reached)\n",
    "\n",
    "    # Clean up DataFrame by removing unnecessary columns\n",
    "    df.drop(columns=['rsi', 'macd', 'macd_signal', 'short_sma', 'long_sma'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def buy_signal_dl_v2(df,BUY_PCT=BUY_PCT, SELL_PCT=SELL_PCT , window=5 ):\n",
    "    \n",
    "    rsi_period=14\n",
    "    macd_short_period=12\n",
    "    macd_long_period=26\n",
    "    macd_signal_period=9\n",
    "    short_sma_period=10\n",
    "    long_sma_period=30\n",
    "    target_pct=BUY_PCT\n",
    "    min_pct_change=SELL_PCT\n",
    "    \n",
    "    # Calculate RSI\n",
    "    df['rsi'] = rsi(df, window=rsi_period)\n",
    "\n",
    "    # Calculate MACD and MACD Signal\n",
    "    df['macd'], df['macd_signal'] = macd(df, short_period=macd_short_period, long_period=macd_long_period, signal_period=macd_signal_period)\n",
    "\n",
    "    # Calculate short-term and long-term Simple Moving Averages\n",
    "    df['short_sma'] = sma(df, window=short_sma_period)\n",
    "    df['long_sma'] = sma(df, window=long_sma_period)\n",
    "\n",
    "    # Identify trend change by comparing short-term and long-term SMAs\n",
    "    trend_change = (df['short_sma'] > df['long_sma']).astype(int)\n",
    "\n",
    "    # RSI Oversold Condition\n",
    "    oversold = (df['rsi'] < 30).astype(int)\n",
    "\n",
    "    # MACD Crossover\n",
    "    macd_crossover = ((df['macd'] > df['macd_signal']) & (df['macd'].shift(1) <= df['macd_signal'].shift(1))).astype(int)\n",
    "\n",
    "    # Combine conditions\n",
    "    combined_signal = (trend_change & oversold & macd_crossover)\n",
    "\n",
    "    # Calculate future price after window periods\n",
    "    future_price = df['close'].shift(-window)\n",
    "\n",
    "    # Calculate the percentage change between the current and future price\n",
    "    pct_change = ((future_price - df['close']) / df['close']) * 100\n",
    "\n",
    "    # Check if the percentage change is greater than the target percentage\n",
    "    target_reached = (pct_change >= target_pct).astype(int)\n",
    "\n",
    "    # Check if the percentage change is greater than the minimum allowed fluctuation\n",
    "    min_change_reached = (pct_change >= min_pct_change).astype(int)\n",
    "\n",
    "    # Combine combined_signal, target_reached, and min_change_reached conditions\n",
    "    df['buy'] = (combined_signal & target_reached & min_change_reached)\n",
    "\n",
    "    # Clean up DataFrame by removing unnecessary columns\n",
    "    df.drop(columns=['rsi', 'macd', 'macd_signal', 'short_sma', 'long_sma'], inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxi custum expend : GMT/USDT with those parameters: w1m=2,w5m=2,w15m=2,w1h=2,w1d=2 btc_w1m=2,btc_w5m=2,btc_w15m=2,btc_w1h=2,btc_w1d=2\n",
      "Precent Mean: 2.908%\n",
      "######################  max expend GMT/USDT - shape (365805, 87)  buy mean : 2.908 ############################\n",
      "False\n",
      "False\n",
      "Buy mean pct: 2.908%\n",
      "Precent Mean: 290.800%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BAD_PERIOD_START=\"2022-08-30\"\n",
    "BAD_PERIOD_END=\"2022-11-22\"\n",
    "pair_to_test=\"GMT/USDT\"\n",
    "# MAX_FORCAST_SIZE=120\n",
    "\n",
    "BUY_PCT_TEST=1.5\n",
    "loc_start=0\n",
    "loc_end=1000000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# loc_start=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_START))\n",
    "# loc_end=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_END))\n",
    "\n",
    "pair=pair_to_test\n",
    "OnePair_DF=maxi_expand(pair=pair,i=loc_start,j=loc_end,window=2,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function,\n",
    "                           w1m=2,w5m=2,w15m=2,w1h=2,w1d=2,\n",
    "                           btc_w1m=2,btc_w5m=2,btc_w15m=2,btc_w1h=2,btc_w1d=2)\n",
    "\n",
    "\n",
    "\n",
    "OnePair_DT=OnePair_DF.to_numpy()\n",
    "gc.collect()\n",
    "OnePair_DT=fixdt(OnePair_DT)\n",
    "print(OnePair_DT[0,0] == OnePair_DF.iloc[0,0])\n",
    "print(OnePair_DT[5,5] == OnePair_DF.iloc[5,5])\n",
    "avggg=hp(OnePair_DF.buy.mean(),\"Buy mean pct\")\n",
    "\n",
    "i_win=1440*2\n",
    "\n",
    "i_start=i_win*20\n",
    "\n",
    "plot_data(\"Original\", pair_to_test, avggg, OnePair_DF, i_start, i_win, OnePair_DF.buy,dot_color=\"r\",fig_width=25, fig_height=7)\n",
    "plot_data(\"Original\", pair_to_test, avggg, OnePair_DF, i_start, i_win, OnePair_DF.buy,dot_color=\"r\",fig_width=25, fig_height=7)\n",
    "plot_data(\"Original\", pair_to_test, avggg, OnePair_DF, i_start, i_win, OnePair_DF.buy,dot_color=\"r\",fig_width=25, fig_height=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xdf=pd.DataFrame()\n",
    "# count=0\n",
    "# row_numbers=10000\n",
    "# for pair in pair_list[:]:\n",
    "#     if pair != \"BTC/USDT\" and pair != \"EUR/USDT\" and pair != \"ETH/USDT\" :\n",
    "#         print(\"working on: \"+pair ,end=\" -->\")\n",
    "#         try:\n",
    "            \n",
    "#             df=maxi_expand(pair=pair,i=0,j=len(df_list1m[pair]),window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function,\n",
    "#                            w1m=6,w5m=10,w15m=50,w1h=8,w1d=7,\n",
    "#                            btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15)\n",
    "#             print(\"df original shape \"+str(df.shape))\n",
    "#             print(f\"df original shape buy mean : {df.buy.mean()*100}\")\n",
    "#             df=df.reset_index()\n",
    "#             try:df.pop(\"num_index\")\n",
    "#             except: pass\n",
    "#             try:df.pop(\"index\")\n",
    "#             except: pass\n",
    "#             try:df.pop(\"date\")\n",
    "#             except: pass\n",
    "#             df=data_shufler(df)            \n",
    "#             #df=data_chooser(df,weight=50,row_numbers=df.buy.sum()*2)\n",
    "#             df=data_chooser50(df,row_numbers=row_numbers)\n",
    "#             gc.collect()\n",
    "#             df=data_cleanup(df)\n",
    "#             df=df.dropna()\n",
    "#             print(pair+f\" is processed -- {count}/{len(pair_list)}\")\n",
    "#             # print(df.iloc[0:1])\n",
    "#         except Exception as e:\n",
    "#             print(f\"error while processing {pair} {count}/{len(pair_list)}\")\n",
    "#             print(e)\n",
    "#         xdf=pd.concat([xdf,df],axis=0)\n",
    "#         count+=1\n",
    "#         del(df)\n",
    "#         gc.collect()\n",
    "# df=xdf\n",
    "# del xdf\n",
    "# gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on: ENS/USDTworking on: APE/USDTworking on: SNM/BUSDworking on: TRX/USDT --> --> --> -->maxi custum expend : ENS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15maxi custum expend : APE/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15maxi custum expend : SNM/BUSD with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "maxi custum expend : TRX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "\n",
      "\n",
      "Precent Mean: 5.855%\n",
      "######################  max expend SNM/BUSD - shape (156489, 499)  buy mean : 5.855 ############################\n",
      "df original shape (156489, 499)\n",
      "df original shape buy mean : 5.854724613231601\n",
      "SNM/BUSD is processed -- 0/112\n",
      "working on: LUNA/USDT -->maxi custum expend : LUNA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.199%\n",
      "######################  max expend APE/USDT - shape (347087, 499)  buy mean : 3.199 ############################\n",
      "df original shape (347087, 499)\n",
      "df original shape buy mean : 3.1991978956284735\n",
      "APE/USDT is processed -- 0/112\n",
      "working on: XRP/USDT -->maxi custum expend : XRP/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.538%\n",
      "######################  max expend ENS/USDT - shape (455098, 499)  buy mean : 3.538 ############################\n",
      "Precent Mean: 1.810%\n",
      "######################  max expend TRX/USDT - shape (455093, 499)  buy mean : 1.81 ############################\n",
      "df original shape (455098, 499)\n",
      "df original shape buy mean : 3.538139038185182\n",
      "df original shape (455093, 499)\n",
      "df original shape buy mean : 1.8099597225182544\n",
      "TRX/USDT is processed -- 0/112\n",
      "ENS/USDT is processed -- 0/112working on: NEAR/USDT\n",
      " -->working on: DOGE/USDTmaxi custum expend : NEAR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15 -->\n",
      "maxi custum expend : DOGE/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.544%\n",
      "######################  max expend LUNA/USDT - shape (942285, 499)  buy mean : 3.544 ############################\n",
      "df original shape (942285, 499)\n",
      "df original shape buy mean : 3.5435139050287336\n",
      "LUNA/USDT is processed -- 0/112\n",
      "working on: GMT/USDT -->maxi custum expend : GMT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.670%\n",
      "######################  max expend XRP/USDT - shape (968516, 499)  buy mean : 2.67 ############################\n",
      "df original shape (968516, 499)\n",
      "df original shape buy mean : 2.6697545523254136\n",
      "XRP/USDT is processed -- 0/112\n",
      "working on: IDEX/USDT -->maxi custum expend : IDEX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.877%\n",
      "######################  max expend DOGE/USDT - shape (968520, 499)  buy mean : 2.877 ############################\n",
      "df original shape (968520, 499)\n",
      "df original shape buy mean : 2.8772766695576757\n",
      "Precent Mean: 3.503%\n",
      "######################  max expend NEAR/USDT - shape (968528, 499)  buy mean : 3.503 ############################\n",
      "df original shape (968528, 499)\n",
      "df original shape buy mean : 3.5029446748054776\n",
      "DOGE/USDT is processed -- 0/112\n",
      "working on: LTC/USDT -->maxi custum expend : LTC/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "NEAR/USDT is processed -- 0/112\n",
      "working on: AXS/USDT -->maxi custum expend : AXS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.940%\n",
      "######################  max expend GMT/USDT - shape (358605, 499)  buy mean : 2.94 ############################\n",
      "df original shape (358605, 499)\n",
      "df original shape buy mean : 2.9402824835124997\n",
      "GMT/USDT is processed -- 0/112\n",
      "working on: UST/USDT -->maxi custum expend : UST/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 0.148%Precent Mean: 4.574%\n",
      "\n",
      "######################  max expend UST/USDT - shape (180050, 499)  buy mean : 0.148 ##################################################  max expend IDEX/USDT - shape (455090, 499)  buy mean : 4.574 ############################\n",
      "\n",
      "df original shape (180050, 499)\n",
      "df original shape (455090, 499)df original shape buy mean : 0.14829214107192448\n",
      "\n",
      "df original shape buy mean : 4.574479773231668\n",
      "UST/USDT is processed -- 0/112\n",
      "working on: SOL/USDT -->maxi custum expend : SOL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "IDEX/USDT is processed -- 0/112\n",
      "working on: AVAX/USDT -->maxi custum expend : AVAX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.050%\n",
      "######################  max expend AXS/USDT - shape (455094, 499)  buy mean : 3.05 ############################\n",
      "df original shape (455094, 499)\n",
      "df original shape buy mean : 3.049919357319587\n",
      "AXS/USDT is processed -- 0/112\n",
      "working on: GAL/USDT -->maxi custum expend : GAL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.858%\n",
      "######################  max expend GAL/USDT - shape (276534, 499)  buy mean : 2.858 ############################\n",
      "df original shape (276534, 499)\n",
      "df original shape buy mean : 2.8575148083056696\n",
      "GAL/USDT is processed -- 0/112\n",
      "working on: GALA/USDT -->maxi custum expend : GALA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.008%\n",
      "######################  max expend LTC/USDT - shape (968519, 499)  buy mean : 3.008 ############################\n",
      "df original shape (968519, 499)\n",
      "df original shape buy mean : 3.0077881796846526\n",
      "LTC/USDT is processed -- 0/112\n",
      "working on: MANA/USDT -->maxi custum expend : MANA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.881%\n",
      "######################  max expend GALA/USDT - shape (455096, 499)  buy mean : 2.881 ############################\n",
      "df original shape (455096, 499)\n",
      "df original shape buy mean : 2.8813700845535886\n",
      "Precent Mean: 3.226%\n",
      "######################  max expend AVAX/USDT - shape (968523, 499)  buy mean : 3.226 ############################\n",
      "df original shape (968523, 499)\n",
      "df original shape buy mean : 3.22635600806589\n",
      "Precent Mean: 3.367%\n",
      "######################  max expend SOL/USDT - shape (968522, 499)  buy mean : 3.367 ############################\n",
      "df original shape (968522, 499)\n",
      "df original shape buy mean : 3.3671924850442223\n",
      "GALA/USDT is processed -- 0/112\n",
      "working on: SHIB/USDT -->maxi custum expend : SHIB/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "AVAX/USDT is processed -- 0/112\n",
      "working on: DOT/USDT -->maxi custum expend : DOT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "SOL/USDT is processed -- 0/112\n",
      "working on: LAZIO/USDT -->maxi custum expend : LAZIO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.744%\n",
      "######################  max expend MANA/USDT - shape (449670, 499)  buy mean : 2.744 ############################\n",
      "df original shape (449670, 499)\n",
      "df original shape buy mean : 2.744012275668824\n",
      "MANA/USDT is processed -- 0/112\n",
      "working on: DAR/USDT -->maxi custum expend : DAR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.640%\n",
      "######################  max expend SHIB/USDT - shape (455097, 499)  buy mean : 2.64 ############################\n",
      "df original shape (455097, 499)\n",
      "df original shape buy mean : 2.640096506898529\n",
      "SHIB/USDT is processed -- 0/112\n",
      "working on: ZIL/USDT -->maxi custum expend : ZIL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.947%\n",
      "######################  max expend LAZIO/USDT - shape (455102, 499)  buy mean : 4.947 ############################\n",
      "df original shape (455102, 499)\n",
      "df original shape buy mean : 4.946803134242433\n",
      "LAZIO/USDT is processed -- 0/112\n",
      "working on: ALICE/USDT -->maxi custum expend : ALICE/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.293%\n",
      "######################  max expend DAR/USDT - shape (455101, 499)  buy mean : 3.293 ############################\n",
      "df original shape (455101, 499)\n",
      "df original shape buy mean : 3.2931151546579773\n",
      "DAR/USDT is processed -- 0/112\n",
      "working on: WAVES/USDT -->maxi custum expend : WAVES/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.648%\n",
      "######################  max expend ZIL/USDT - shape (455098, 499)  buy mean : 2.648 ############################\n",
      "df original shape (455098, 499)\n",
      "df original shape buy mean : 2.648440555660539\n",
      "Precent Mean: 3.208%\n",
      "######################  max expend DOT/USDT - shape (968524, 499)  buy mean : 3.208 ############################\n",
      "df original shape (968524, 499)\n",
      "df original shape buy mean : 3.208387195361189\n",
      "ZIL/USDT is processed -- 0/112\n",
      "working on: SLP/USDT -->maxi custum expend : SLP/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.874%\n",
      "######################  max expend ALICE/USDT - shape (455103, 499)  buy mean : 2.874 ############################\n",
      "df original shape (455103, 499)\n",
      "df original shape buy mean : 2.8742943905006118\n",
      "DOT/USDT is processed -- 0/112\n",
      "working on: ADA/USDT -->maxi custum expend : ADA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "ALICE/USDT is processed -- 0/112\n",
      "working on: ROSE/USDT -->maxi custum expend : ROSE/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.086%\n",
      "######################  max expend WAVES/USDT - shape (455102, 499)  buy mean : 3.086 ############################\n",
      "df original shape (455102, 499)\n",
      "df original shape buy mean : 3.085681891092546\n",
      "WAVES/USDT is processed -- 0/112\n",
      "working on: ETC/USDT -->maxi custum expend : ETC/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.172%\n",
      "######################  max expend SLP/USDT - shape (455106, 499)  buy mean : 4.172 ############################\n",
      "df original shape (455106, 499)\n",
      "df original shape buy mean : 4.171555637587727\n",
      "SLP/USDT is processed -- 0/112\n",
      "working on: EOS/USDT -->maxi custum expend : EOS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.240%\n",
      "######################  max expend ROSE/USDT - shape (455103, 499)  buy mean : 3.24 ############################\n",
      "df original shape (455103, 499)\n",
      "df original shape buy mean : 3.2397061764040234\n",
      "ROSE/USDT is processed -- 0/112\n",
      "working on: ZEC/USDT -->maxi custum expend : ZEC/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.466%\n",
      "######################  max expend ETC/USDT - shape (455111, 499)  buy mean : 3.466 ############################\n",
      "df original shape (455111, 499)\n",
      "df original shape buy mean : 3.465967643058507\n",
      "ETC/USDT is processed -- 0/112\n",
      "working on: MBOX/USDT -->maxi custum expend : MBOX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.996%\n",
      "######################  max expend EOS/USDT - shape (455107, 499)  buy mean : 2.996 ############################\n",
      "df original shape (455107, 499)\n",
      "df original shape buy mean : 2.996218471700062\n",
      "Precent Mean: 2.874%\n",
      "######################  max expend ADA/USDT - shape (968514, 499)  buy mean : 2.874 ############################\n",
      "df original shape (968514, 499)\n",
      "df original shape buy mean : 2.8739904637413605\n",
      "EOS/USDT is processed -- 0/112\n",
      "working on: PORTO/USDT -->maxi custum expend : PORTO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.402%\n",
      "######################  max expend ZEC/USDT - shape (455104, 499)  buy mean : 3.402 ############################\n",
      "df original shape (455104, 499)\n",
      "df original shape buy mean : 3.401640064688511\n",
      "ADA/USDT is processed -- 0/112\n",
      "working on: JASMY/USDT -->maxi custum expend : JASMY/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "ZEC/USDT is processed -- 0/112\n",
      "working on: ALGO/USDT -->maxi custum expend : ALGO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.891%\n",
      "######################  max expend MBOX/USDT - shape (455111, 499)  buy mean : 3.891 ############################\n",
      "df original shape (455111, 499)\n",
      "df original shape buy mean : 3.8913583719136646\n",
      "MBOX/USDT is processed -- 0/112\n",
      "working on: OGN/USDT -->maxi custum expend : OGN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.091%\n",
      "######################  max expend PORTO/USDT - shape (449670, 499)  buy mean : 5.091 ############################\n",
      "df original shape (449670, 499)\n",
      "df original shape buy mean : 5.091289167611804\n",
      "PORTO/USDT is processed -- 0/112\n",
      "working on: ICP/USDT -->maxi custum expend : ICP/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.185%\n",
      "######################  max expend JASMY/USDT - shape (455092, 499)  buy mean : 3.185 ############################\n",
      "df original shape (455092, 499)\n",
      "df original shape buy mean : 3.185290007295228\n",
      "Precent Mean: 2.943%\n",
      "######################  max expend ALGO/USDT - shape (455105, 499)  buy mean : 2.943 ############################\n",
      "df original shape (455105, 499)\n",
      "df original shape buy mean : 2.942617637688006\n",
      "JASMY/USDT is processed -- 0/112\n",
      "working on: MTL/USDT -->maxi custum expend : MTL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "ALGO/USDT is processed -- 0/112\n",
      "working on: GRT/USDT -->maxi custum expend : GRT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.384%\n",
      "######################  max expend OGN/USDT - shape (455112, 499)  buy mean : 3.384 ############################\n",
      "df original shape (455112, 499)\n",
      "df original shape buy mean : 3.3835627274165483\n",
      "OGN/USDT is processed -- 0/112\n",
      "working on: AR/USDT -->maxi custum expend : AR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.384%\n",
      "######################  max expend ICP/USDT - shape (455108, 499)  buy mean : 3.384 ############################\n",
      "df original shape (455108, 499)\n",
      "df original shape buy mean : 3.384251650157765\n",
      "ICP/USDT is processed -- 0/112\n",
      "working on: EGLD/USDT -->maxi custum expend : EGLD/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.005%\n",
      "######################  max expend MTL/USDT - shape (455115, 499)  buy mean : 3.005 ############################\n",
      "df original shape (455115, 499)\n",
      "df original shape buy mean : 3.005394241016007\n",
      "Precent Mean: 3.194%\n",
      "######################  max expend GRT/USDT - shape (455105, 499)  buy mean : 3.194 ############################\n",
      "df original shape (455105, 499)\n",
      "df original shape buy mean : 3.1935487414992143\n",
      "MTL/USDT is processed -- 0/112\n",
      "working on: SNX/USDT -->maxi custum expend : SNX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "GRT/USDT is processed -- 0/112\n",
      "working on: PSG/USDT -->maxi custum expend : PSG/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.471%\n",
      "######################  max expend AR/USDT - shape (455112, 499)  buy mean : 3.471 ############################\n",
      "df original shape (455112, 499)\n",
      "df original shape buy mean : 3.4707940023554644\n",
      "AR/USDT is processed -- 0/112\n",
      "working on: GLMR/USDT -->maxi custum expend : GLMR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.628%\n",
      "######################  max expend SNX/USDT - shape (455116, 499)  buy mean : 3.628 ############################\n",
      "df original shape (455116, 499)\n",
      "df original shape buy mean : 3.6280860264196377\n",
      "Precent Mean: 4.289%\n",
      "######################  max expend PSG/USDT - shape (455106, 499)  buy mean : 4.289 ############################\n",
      "df original shape (455106, 499)\n",
      "df original shape buy mean : 4.28911066872333\n",
      "SNX/USDT is processed -- 0/112\n",
      "working on: PYR/USDT -->maxi custum expend : PYR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "PSG/USDT is processed -- 0/112\n",
      "working on: ACM/USDT -->maxi custum expend : ACM/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.294%\n",
      "######################  max expend GLMR/USDT - shape (440713, 499)  buy mean : 4.294 ############################\n",
      "df original shape (440713, 499)\n",
      "df original shape buy mean : 4.293724033554716\n",
      "GLMR/USDT is processed -- 0/112\n",
      "working on: LOKA/USDT -->maxi custum expend : LOKA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.260%\n",
      "######################  max expend EGLD/USDT - shape (968529, 499)  buy mean : 3.26 ############################\n",
      "df original shape (968529, 499)\n",
      "df original shape buy mean : 3.259582315036514\n",
      "EGLD/USDT is processed -- 0/112\n",
      "working on: XMR/USDT -->maxi custum expend : XMR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.903%\n",
      "######################  max expend PYR/USDT - shape (455116, 499)  buy mean : 3.903 ############################\n",
      "df original shape (455116, 499)\n",
      "df original shape buy mean : 3.9025215549442342\n",
      "Precent Mean: 4.862%\n",
      "######################  max expend ACM/USDT - shape (445170, 499)  buy mean : 4.862 ############################\n",
      "df original shape (445170, 499)\n",
      "df original shape buy mean : 4.86241211222679\n",
      "PYR/USDT is processed -- 0/112\n",
      "working on: DASH/USDT -->maxi custum expend : DASH/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "ACM/USDT is processed -- 0/112\n",
      "working on: BAR/USDT -->maxi custum expend : BAR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.737%\n",
      "######################  max expend LOKA/USDT - shape (427754, 499)  buy mean : 4.737 ############################\n",
      "df original shape (427754, 499)\n",
      "df original shape buy mean : 4.7368347227612135\n",
      "LOKA/USDT is processed -- 0/112\n",
      "working on: XLM/USDT -->maxi custum expend : XLM/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.863%\n",
      "######################  max expend XMR/USDT - shape (455109, 499)  buy mean : 2.863 ############################\n",
      "df original shape (455109, 499)\n",
      "df original shape buy mean : 2.8634898452898097\n",
      "XMR/USDT is processed -- 0/112\n",
      "working on: KDA/USDT -->maxi custum expend : KDA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.762%\n",
      "######################  max expend DASH/USDT - shape (455117, 499)  buy mean : 2.762 ############################\n",
      "df original shape (455117, 499)\n",
      "df original shape buy mean : 2.7617074290786765\n",
      "Precent Mean: 4.130%\n",
      "######################  max expend BAR/USDT - shape (455120, 499)  buy mean : 4.13 ############################\n",
      "df original shape (455120, 499)\n",
      "df original shape buy mean : 4.129899806644401\n",
      "DASH/USDT is processed -- 0/112\n",
      "working on: CITY/USDT -->maxi custum expend : CITY/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "BAR/USDT is processed -- 0/112\n",
      "working on: JUV/USDT -->maxi custum expend : JUV/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.353%\n",
      "######################  max expend XLM/USDT - shape (455114, 499)  buy mean : 2.353 ############################\n",
      "df original shape (455114, 499)\n",
      "df original shape buy mean : 2.352817096375853\n",
      "XLM/USDT is processed -- 0/112\n",
      "working on: JST/USDT -->maxi custum expend : JST/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.219%\n",
      "######################  max expend KDA/USDT - shape (355750, 499)  buy mean : 4.219 ############################\n",
      "df original shape (355750, 499)\n",
      "df original shape buy mean : 4.218973998594518\n",
      "KDA/USDT is processed -- 0/112\n",
      "working on: ICX/USDT -->maxi custum expend : ICX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.107%\n",
      "######################  max expend JUV/USDT - shape (455121, 499)  buy mean : 4.107 ############################\n",
      "df original shape (455121, 499)\n",
      "df original shape buy mean : 4.107259388162708\n",
      "Precent Mean: 4.221%\n",
      "######################  max expend CITY/USDT - shape (433170, 499)  buy mean : 4.221 ############################\n",
      "df original shape (433170, 499)\n",
      "df original shape buy mean : 4.221206454740633\n",
      "JUV/USDT is processed -- 0/112\n",
      "working on: SYS/USDT -->maxi custum expend : SYS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "CITY/USDT is processed -- 0/112\n",
      "working on: ASTR/USDT -->maxi custum expend : ASTR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.243%\n",
      "######################  max expend JST/USDT - shape (455124, 499)  buy mean : 3.243 ############################\n",
      "df original shape (455124, 499)\n",
      "df original shape buy mean : 3.2432919380212866\n",
      "JST/USDT is processed -- 0/112\n",
      "working on: OMG/USDT -->maxi custum expend : OMG/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.000%\n",
      "######################  max expend ICX/USDT - shape (455128, 499)  buy mean : 3.0 ############################\n",
      "df original shape (455128, 499)\n",
      "df original shape buy mean : 3.0002548733543093\n",
      "ICX/USDT is processed -- 0/112\n",
      "working on: API3/USDT -->maxi custum expend : API3/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.139%\n",
      "######################  max expend ASTR/USDT - shape (371598, 499)  buy mean : 4.139 ############################\n",
      "df original shape (371598, 499)\n",
      "df original shape buy mean : 4.139150372176384\n",
      "ASTR/USDT is processed -- 0/112\n",
      "working on: IOTA/USDT -->maxi custum expend : IOTA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.319%\n",
      "######################  max expend SYS/USDT - shape (455121, 499)  buy mean : 4.319 ############################\n",
      "df original shape (455121, 499)\n",
      "df original shape buy mean : 4.3188514702683465\n",
      "SYS/USDT is processed -- 0/112\n",
      "working on: RVN/USDT -->maxi custum expend : RVN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.438%\n",
      "######################  max expend OMG/USDT - shape (455124, 499)  buy mean : 3.438 ############################\n",
      "df original shape (455124, 499)\n",
      "df original shape buy mean : 3.4384035998980496\n",
      "OMG/USDT is processed -- 0/112\n",
      "working on: ATM/USDT -->maxi custum expend : ATM/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.504%\n",
      "######################  max expend API3/USDT - shape (426329, 499)  buy mean : 3.504 ############################\n",
      "df original shape (426329, 499)\n",
      "df original shape buy mean : 3.503632171398145\n",
      "API3/USDT is processed -- 0/112\n",
      "working on: ONT/USDT -->maxi custum expend : ONT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.806%\n",
      "######################  max expend IOTA/USDT - shape (455119, 499)  buy mean : 2.806 ############################\n",
      "df original shape (455119, 499)\n",
      "df original shape buy mean : 2.8056398436452885\n",
      "IOTA/USDT is processed -- 0/112\n",
      "working on: VOXEL/USDT -->maxi custum expend : VOXEL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.357%\n",
      "######################  max expend RVN/USDT - shape (455122, 499)  buy mean : 3.357 ############################\n",
      "df original shape (455122, 499)\n",
      "df original shape buy mean : 3.3573415479805413\n",
      "RVN/USDT is processed -- 0/112\n",
      "working on: MBL/USDT -->maxi custum expend : MBL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.489%\n",
      "######################  max expend ATM/USDT - shape (455125, 499)  buy mean : 4.489 ############################\n",
      "df original shape (455125, 499)\n",
      "df original shape buy mean : 4.488876682230156\n",
      "ATM/USDT is processed -- 0/112\n",
      "working on: XEC/USDT -->maxi custum expend : XEC/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 2.728%\n",
      "######################  max expend ONT/USDT - shape (455129, 499)  buy mean : 2.728 ############################\n",
      "df original shape (455129, 499)\n",
      "df original shape buy mean : 2.7282374887119913\n",
      "ONT/USDT is processed -- 0/112\n",
      "working on: SKL/USDT -->maxi custum expend : SKL/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.297%\n",
      "######################  max expend VOXEL/USDT - shape (444420, 499)  buy mean : 4.297 ############################\n",
      "df original shape (444420, 499)\n",
      "df original shape buy mean : 4.297286350749291\n",
      "VOXEL/USDT is processed -- 0/112\n",
      "working on: HIVE/USDT -->maxi custum expend : HIVE/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.057%\n",
      "######################  max expend MBL/USDT - shape (455123, 499)  buy mean : 4.057 ############################\n",
      "df original shape (455123, 499)\n",
      "df original shape buy mean : 4.056925270750984\n",
      "MBL/USDT is processed -- 0/112\n",
      "working on: REN/USDT -->maxi custum expend : REN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.159%\n",
      "######################  max expend XEC/USDT - shape (455126, 499)  buy mean : 3.159 ############################\n",
      "df original shape (455126, 499)\n",
      "df original shape buy mean : 3.1593448847132444\n",
      "XEC/USDT is processed -- 0/112\n",
      "working on: STORJ/USDT -->maxi custum expend : STORJ/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.163%\n",
      "######################  max expend SKL/USDT - shape (455130, 499)  buy mean : 3.163 ############################\n",
      "df original shape (455130, 499)\n",
      "df original shape buy mean : 3.1630523147232656\n",
      "SKL/USDT is processed -- 0/112\n",
      "working on: MULTI/USDT -->maxi custum expend : MULTI/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.718%\n",
      "######################  max expend HIVE/USDT - shape (455133, 499)  buy mean : 3.718 ############################\n",
      "df original shape (455133, 499)\n",
      "df original shape buy mean : 3.7184735011524106\n",
      "HIVE/USDT is processed -- 0/112\n",
      "working on: KP3R/USDT -->maxi custum expend : KP3R/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.589%\n",
      "######################  max expend REN/USDT - shape (455123, 499)  buy mean : 3.589 ############################\n",
      "df original shape (455123, 499)\n",
      "df original shape buy mean : 3.5891396391744648\n",
      "REN/USDT is processed -- 0/112\n",
      "Precent Mean: 3.305%\n",
      "######################  max expend STORJ/USDT - shape (455126, 499)  buy mean : 3.305 ############################\n",
      "df original shape (455126, 499)df original shape buy mean : 3.3050188299503875\n",
      "\n",
      "working on: NULS/USDT -->maxi custum expend : NULS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "STORJ/USDT is processed -- 0/112\n",
      "working on: ZRX/USDT -->maxi custum expend : ZRX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.859%\n",
      "######################  max expend MULTI/USDT - shape (318330, 499)  buy mean : 3.859 ############################\n",
      "df original shape (318330, 499)\n",
      "df original shape buy mean : 3.859202714164546\n",
      "MULTI/USDT is processed -- 0/112\n",
      "working on: QTUM/USDT -->maxi custum expend : QTUM/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.821%\n",
      "######################  max expend KP3R/USDT - shape (455134, 499)  buy mean : 4.821 ############################\n",
      "df original shape (455134, 499)\n",
      "df original shape buy mean : 4.82077805657235\n",
      "KP3R/USDT is processed -- 0/112\n",
      "working on: ATA/USDT -->maxi custum expend : ATA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.367%\n",
      "######################  max expend NULS/USDT - shape (455137, 499)  buy mean : 4.367 ############################\n",
      "df original shape (455137, 499)\n",
      "df original shape buy mean : 4.366817024324544\n",
      "Precent Mean: 3.072%\n",
      "######################  max expend ZRX/USDT - shape (455127, 499)  buy mean : 3.072 ############################\n",
      "df original shape (455127, 499)\n",
      "df original shape buy mean : 3.0716701052673208\n",
      "NULS/USDT is processed -- 0/112\n",
      "working on: MLN/USDT -->maxi custum expend : MLN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "ZRX/USDT is processed -- 0/112\n",
      "working on: SRM/USDT -->maxi custum expend : SRM/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.231%\n",
      "######################  max expend QTUM/USDT - shape (455131, 499)  buy mean : 3.231 ############################\n",
      "df original shape (455131, 499)\n",
      "df original shape buy mean : 3.23071818882915\n",
      "QTUM/USDT is processed -- 0/112\n",
      "working on: COCOS/USDT -->maxi custum expend : COCOS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.277%\n",
      "######################  max expend ATA/USDT - shape (455134, 499)  buy mean : 3.277 ############################\n",
      "df original shape (455134, 499)\n",
      "df original shape buy mean : 3.276617435744198\n",
      "ATA/USDT is processed -- 0/112\n",
      "working on: STMX/USDT -->maxi custum expend : STMX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.788%\n",
      "######################  max expend MLN/USDT - shape (455137, 499)  buy mean : 4.788 ############################\n",
      "df original shape (455137, 499)\n",
      "df original shape buy mean : 4.78822859930087\n",
      "Precent Mean: 3.131%\n",
      "######################  max expend SRM/USDT - shape (455127, 499)  buy mean : 3.131 ############################\n",
      "df original shape (455127, 499)\n",
      "df original shape buy mean : 3.130554768229527\n",
      "MLN/USDT is processed -- 0/112\n",
      "working on: YGG/USDT -->maxi custum expend : YGG/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "SRM/USDT is processed -- 0/112\n",
      "working on: FLUX/USDT -->maxi custum expend : FLUX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.260%\n",
      "######################  max expend COCOS/USDT - shape (455132, 499)  buy mean : 5.26 ############################\n",
      "df original shape (455132, 499)\n",
      "df original shape buy mean : 5.260232196373799\n",
      "COCOS/USDT is processed -- 0/112\n",
      "working on: LPT/USDT -->maxi custum expend : LPT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.265%\n",
      "######################  max expend STMX/USDT - shape (455135, 499)  buy mean : 3.265 ############################\n",
      "df original shape (455135, 499)\n",
      "df original shape buy mean : 3.265404770013293\n",
      "STMX/USDT is processed -- 0/112\n",
      "working on: ADX/USDT -->maxi custum expend : ADX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.134%\n",
      "######################  max expend YGG/USDT - shape (455138, 499)  buy mean : 4.134 ############################\n",
      "df original shape (455138, 499)\n",
      "df original shape buy mean : 4.13391103357663\n",
      "Precent Mean: 4.151%\n",
      "######################  max expend FLUX/USDT - shape (455141, 499)  buy mean : 4.151 ############################\n",
      "df original shape (455141, 499)\n",
      "df original shape buy mean : 4.150581907584683\n",
      "YGG/USDT is processed -- 0/112\n",
      "working on: SC/USDT -->maxi custum expend : SC/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "FLUX/USDT is processed -- 0/112\n",
      "working on: DNT/USDT -->maxi custum expend : DNT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.290%\n",
      "######################  max expend LPT/USDT - shape (455145, 499)  buy mean : 3.29 ############################\n",
      "df original shape (455145, 499)\n",
      "df original shape buy mean : 3.2901602785925363\n",
      "LPT/USDT is processed -- 0/112\n",
      "working on: SCRT/USDT -->maxi custum expend : SCRT/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.222%\n",
      "######################  max expend ADX/USDT - shape (455136, 499)  buy mean : 5.222 ############################\n",
      "df original shape (455136, 499)\n",
      "df original shape buy mean : 5.221955635238698\n",
      "ADX/USDT is processed -- 0/112\n",
      "working on: HIGH/USDT -->maxi custum expend : HIGH/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.212%\n",
      "######################  max expend DNT/USDT - shape (416700, 499)  buy mean : 4.212 ############################\n",
      "df original shape (416700, 499)\n",
      "df original shape buy mean : 4.2121430285577155\n",
      "Precent Mean: 3.122%\n",
      "######################  max expend SC/USDT - shape (455139, 499)  buy mean : 3.122 ############################\n",
      "df original shape (455139, 499)\n",
      "df original shape buy mean : 3.1221231316147375\n",
      "DNT/USDT is processed -- 0/112\n",
      "working on: ORN/USDT -->maxi custum expend : ORN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "SC/USDT is processed -- 0/112\n",
      "working on: CKB/USDT -->maxi custum expend : CKB/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.658%\n",
      "######################  max expend SCRT/USDT - shape (426346, 499)  buy mean : 3.658 ############################\n",
      "df original shape (426346, 499)\n",
      "df original shape buy mean : 3.657592659483143\n",
      "SCRT/USDT is processed -- 0/112\n",
      "working on: RAD/USDT -->maxi custum expend : RAD/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.959%\n",
      "######################  max expend HIGH/USDT - shape (455136, 499)  buy mean : 4.959 ############################\n",
      "df original shape (455136, 499)\n",
      "df original shape buy mean : 4.9587376080995575\n",
      "HIGH/USDT is processed -- 0/112\n",
      "working on: QUICK/USDT -->maxi custum expend : QUICK/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.653%\n",
      "######################  max expend ORN/USDT - shape (455142, 499)  buy mean : 4.653 ############################\n",
      "df original shape (455142, 499)\n",
      "df original shape buy mean : 4.653273044456455\n",
      "Precent Mean: 3.524%\n",
      "######################  max expend CKB/USDT - shape (455139, 499)  buy mean : 3.524 ############################\n",
      "df original shape (455139, 499)\n",
      "df original shape buy mean : 3.5235389628223466\n",
      "ORN/USDT is processed -- 0/112\n",
      "working on: PLA/USDT -->maxi custum expend : PLA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "CKB/USDT is processed -- 0/112\n",
      "working on: TOMO/USDT -->maxi custum expend : TOMO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.874%\n",
      "######################  max expend RAD/USDT - shape (621579, 499)  buy mean : 5.874 ############################\n",
      "df original shape (621579, 499)\n",
      "df original shape buy mean : 5.87407232226314\n",
      "RAD/USDT is processed -- 0/112\n",
      "working on: NMR/USDT -->maxi custum expend : NMR/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.812%\n",
      "######################  max expend QUICK/USDT - shape (455150, 499)  buy mean : 4.812 ############################\n",
      "df original shape (455150, 499)\n",
      "df original shape buy mean : 4.8122596946061735\n",
      "QUICK/USDT is processed -- 0/112\n",
      "working on: LSK/USDT -->maxi custum expend : LSK/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.217%\n",
      "######################  max expend PLA/USDT - shape (455143, 499)  buy mean : 3.217 ############################\n",
      "df original shape (455143, 499)\n",
      "df original shape buy mean : 3.217450339783321\n",
      "Precent Mean: 3.277%\n",
      "######################  max expend TOMO/USDT - shape (455140, 499)  buy mean : 3.277 ############################\n",
      "df original shape (455140, 499)\n",
      "df original shape buy mean : 3.277013666124709\n",
      "PLA/USDT is processed -- 0/112\n",
      "working on: BADGER/USDT -->maxi custum expend : BADGER/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "TOMO/USDT is processed -- 0/112\n",
      "working on: STX/USDT -->maxi custum expend : STX/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.781%\n",
      "######################  max expend NMR/USDT - shape (455147, 499)  buy mean : 4.781 ############################\n",
      "df original shape (455147, 499)\n",
      "df original shape buy mean : 4.781092701918281\n",
      "NMR/USDT is processed -- 0/112\n",
      "working on: ELF/USDT -->maxi custum expend : ELF/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.542%\n",
      "######################  max expend LSK/USDT - shape (455150, 499)  buy mean : 4.542 ############################\n",
      "df original shape (455150, 499)\n",
      "df original shape buy mean : 4.542238822366253\n",
      "LSK/USDT is processed -- 0/112\n",
      "working on: FIDA/USDT -->maxi custum expend : FIDA/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 3.910%\n",
      "######################  max expend BADGER/USDT - shape (455143, 499)  buy mean : 3.91 ############################\n",
      "df original shape (455143, 499)\n",
      "df original shape buy mean : 3.910419362705787\n",
      "Precent Mean: 3.791%\n",
      "######################  max expend STX/USDT - shape (455140, 499)  buy mean : 3.791 ############################\n",
      "df original shape (455140, 499)\n",
      "df original shape buy mean : 3.7911411873269762\n",
      "BADGER/USDT is processed -- 0/112\n",
      "working on: DF/USDT -->maxi custum expend : DF/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "STX/USDT is processed -- 0/112\n",
      "Precent Mean: 3.362%\n",
      "######################  max expend ELF/USDT - shape (455148, 499)  buy mean : 3.362 ############################\n",
      "df original shape (455148, 499)\n",
      "df original shape buy mean : 3.3619833548647913\n",
      "ELF/USDT is processed -- 0/112\n",
      "working on: TORN/USDT -->maxi custum expend : TORN/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.926%\n",
      "######################  max expend FIDA/USDT - shape (455151, 499)  buy mean : 4.926 ############################\n",
      "df original shape (455151, 499)\n",
      "df original shape buy mean : 4.925618091578399\n",
      "FIDA/USDT is processed -- 0/112\n",
      "working on: XNO/USDT -->maxi custum expend : XNO/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.884%\n",
      "######################  max expend DF/USDT - shape (455144, 499)  buy mean : 4.884 ############################\n",
      "df original shape (455144, 499)\n",
      "df original shape buy mean : 4.883948816198829\n",
      "DF/USDT is processed -- 0/112\n",
      "working on: MOB/USDT -->maxi custum expend : MOB/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.631%\n",
      "######################  max expend TORN/USDT - shape (444420, 499)  buy mean : 4.631 ############################\n",
      "df original shape (444420, 499)\n",
      "df original shape buy mean : 4.630754691508033\n",
      "TORN/USDT is processed -- 0/112\n",
      "working on: T/USDT -->maxi custum expend : T/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 5.110%\n",
      "######################  max expend XNO/USDT - shape (416272, 499)  buy mean : 5.11 ############################\n",
      "df original shape (416272, 499)\n",
      "df original shape buy mean : 5.109639850866741\n",
      "Precent Mean: 3.190%\n",
      "######################  max expend MOB/USDT - shape (285225, 499)  buy mean : 3.19 ############################\n",
      "df original shape (285225, 499)\n",
      "df original shape buy mean : 3.1897624682268386\n",
      "XNO/USDT is processed -- 0/112\n",
      "working on: BTG/USDT -->maxi custum expend : BTG/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "MOB/USDT is processed -- 0/112\n",
      "Precent Mean: 3.744%\n",
      "######################  max expend T/USDT - shape (375949, 499)  buy mean : 3.744 ############################\n",
      "df original shape (375949, 499)\n",
      "df original shape buy mean : 3.744124868000713\n",
      "T/USDT is processed -- 0/112\n",
      "Precent Mean: 4.985%\n",
      "######################  max expend BTG/USDT - shape (416700, 499)  buy mean : 4.985 ############################\n",
      "df original shape (416700, 499)\n",
      "df original shape buy mean : 4.985121190304776\n",
      "BTG/USDT is processed -- 0/112\n",
      "working on: GHST/USDT -->maxi custum expend : GHST/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 1.222%\n",
      "######################  max expend GHST/USDT - shape (455153, 499)  buy mean : 1.222 ############################\n",
      "df original shape (455153, 499)\n",
      "df original shape buy mean : 1.2217869595498656\n",
      "GHST/USDT is processed -- 0/112\n",
      "working on: EPS/USDT -->maxi custum expend : EPS/USDT with those parameters: w1m=6,w5m=10,w15m=50,w1h=8,w1d=7 btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15\n",
      "Precent Mean: 4.574%\n",
      "######################  max expend EPS/USDT - shape (174480, 499)  buy mean : 4.574 ############################\n",
      "df original shape (174480, 499)\n",
      "df original shape buy mean : 4.57416322787712\n",
      "EPS/USDT is processed -- 0/112\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "def process_pair(pair):\n",
    "    if pair != \"BTC/USDT\" and pair != \"EUR/USDT\" and pair != \"ETH/USDT\" :\n",
    "        print(\"working on: \"+pair ,end=\" -->\")\n",
    "        try:\n",
    "            \n",
    "            df=maxi_expand(pair=pair,i=0,j=len(df_list1m[pair]),window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,buy_function=buy_function,\n",
    "                           w1m=6,w5m=10,w15m=50,w1h=8,w1d=7,\n",
    "                           btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15)\n",
    "            print(\"df original shape \"+str(df.shape))\n",
    "            print(f\"df original shape buy mean : {df.buy.mean()*100}\")\n",
    "            df=df.reset_index()\n",
    "            try:df.pop(\"num_index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"index\")\n",
    "            except: pass\n",
    "            try:df.pop(\"date\")\n",
    "            except: pass\n",
    "            df=data_shufler(df)            \n",
    "            #df=data_chooser(df,weight=50,row_numbers=df.buy.sum()*2)\n",
    "            df=data_chooser50(df,row_numbers=row_numbers)\n",
    "            gc.collect()\n",
    "            df=data_cleanup(df)\n",
    "            df=df.dropna()\n",
    "            print(pair+f\" is processed -- {count}/{len(pair_list)}\")\n",
    "            # print(df.iloc[0:1])\n",
    "        except Exception as e:\n",
    "            print(f\"error while processing {pair} {count}/{len(pair_list)}\")\n",
    "            print(e)\n",
    "            df = None\n",
    "        return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    xdf=pd.DataFrame()\n",
    "    count=0\n",
    "    row_numbers=10000\n",
    "    with mp.Pool(processes=4) as pool: #mp.cpu_count()\n",
    "        results = pool.map(process_pair, pair_list)\n",
    "        for df in results:\n",
    "            if df is not None:\n",
    "                xdf=pd.concat([xdf,df],axis=0)\n",
    "                count+=1\n",
    "                del(df)\n",
    "                gc.collect()\n",
    "    df=xdf\n",
    "    del xdf\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-3_5min</th>\n",
       "      <th>BTC_high-4_5min</th>\n",
       "      <th>BTC_low-4_5min</th>\n",
       "      <th>BTC_close-4_5min</th>\n",
       "      <th>BTC_volume-4_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>739.418910</td>\n",
       "      <td>-0.006969</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>605.360330</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>-758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>542.518750</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>579.284000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.915675</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>3567.87</td>\n",
       "      <td>-0.005960</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>15473.960</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>...</td>\n",
       "      <td>221.160582</td>\n",
       "      <td>-0.005449</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>200.687935</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>-223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.815000</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>85.610</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>...</td>\n",
       "      <td>127.609940</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>94.250820</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>-679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>128.378030</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>96.998120</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>-314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089995</th>\n",
       "      <td>0.110050</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>5950.000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>...</td>\n",
       "      <td>205.213660</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>123.195440</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>-335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089996</th>\n",
       "      <td>0.163467</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>241500.10</td>\n",
       "      <td>-0.001789</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>127066.200</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>...</td>\n",
       "      <td>86.199610</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>118.047930</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089997</th>\n",
       "      <td>5.860500</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>7094.09</td>\n",
       "      <td>-0.009470</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>-0.007593</td>\n",
       "      <td>8057.994</td>\n",
       "      <td>-0.008446</td>\n",
       "      <td>...</td>\n",
       "      <td>118.296080</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>129.590440</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089998</th>\n",
       "      <td>3.393800</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109.33</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>5.680</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>...</td>\n",
       "      <td>593.171030</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>773.236620</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>-685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089999</th>\n",
       "      <td>0.117760</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>669.50</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>5644.800</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>...</td>\n",
       "      <td>628.509790</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>-0.008415</td>\n",
       "      <td>-0.009881</td>\n",
       "      <td>95.214170</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>-617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1090000 rows  499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             price    high-1     low-1   close-1   volume-1    high-2  \\\n",
       "0         0.572000  0.000000  0.000000  0.000000       0.00  0.000000   \n",
       "1         3.820000  0.000000  0.000000  0.000000       3.56  0.000000   \n",
       "2        33.915675 -0.002221  0.004676  0.001300    3567.87 -0.005960   \n",
       "3        14.815000  0.000337  0.000337  0.000337       0.00 -0.001012   \n",
       "4         0.072900  0.000000  0.000000  0.000000       0.00  0.000000   \n",
       "...            ...       ...       ...       ...        ...       ...   \n",
       "1089995   0.110050  0.001363  0.001363  0.001363       0.00  0.001363   \n",
       "1089996   0.163467 -0.000260  0.002554  0.002371  241500.10 -0.001789   \n",
       "1089997   5.860500 -0.008276 -0.005546 -0.005546    7094.09 -0.009470   \n",
       "1089998   3.393800 -0.000413  0.001296  0.000000     109.33  0.000177   \n",
       "1089999   0.117760 -0.001274  0.000000  0.000000     669.50 -0.001529   \n",
       "\n",
       "            low-2   close-2    volume-2    high-3  ...  BTC_volume-3_5min  \\\n",
       "0        0.000000  0.000000       0.000  0.000000  ...         739.418910   \n",
       "1        0.000000  0.000000      12.350  0.000000  ...         542.518750   \n",
       "2        0.005501 -0.001342   15473.960  0.004325  ...         221.160582   \n",
       "3        0.000337  0.000337      85.610 -0.001012  ...         127.609940   \n",
       "4        0.000000  0.000000       0.000  0.000000  ...         128.378030   \n",
       "...           ...       ...         ...       ...  ...                ...   \n",
       "1089995  0.003180  0.001363    5950.000  0.000454  ...         205.213660   \n",
       "1089996  0.000474 -0.000015  127066.200 -0.002401  ...          86.199610   \n",
       "1089997 -0.005546 -0.007593    8057.994 -0.008446  ...         118.296080   \n",
       "1089998  0.000177  0.000177       5.680 -0.000324  ...         593.171030   \n",
       "1089999 -0.001444 -0.001529    5644.800 -0.002802  ...         628.509790   \n",
       "\n",
       "         BTC_high-4_5min  BTC_low-4_5min  BTC_close-4_5min  BTC_volume-4_5min  \\\n",
       "0              -0.006969       -0.005141         -0.005663         605.360330   \n",
       "1              -0.006736       -0.003985         -0.005581         579.284000   \n",
       "2              -0.005449       -0.002277         -0.005378         200.687935   \n",
       "3              -0.000746        0.000915          0.000379          94.250820   \n",
       "4               0.000604        0.001454          0.000783          96.998120   \n",
       "...                  ...             ...               ...                ...   \n",
       "1089995        -0.001845        0.001947          0.001696         123.195440   \n",
       "1089996        -0.002866       -0.000300         -0.000647         118.047930   \n",
       "1089997        -0.002173        0.001198         -0.000688         129.590440   \n",
       "1089998        -0.001501        0.001733          0.001660         773.236620   \n",
       "1089999        -0.010186       -0.008415         -0.009881          95.214170   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          1     5      41       -758    1  \n",
       "1          4     0       1       -355    1  \n",
       "2          1    12      45       -223    0  \n",
       "3          3    20      18       -679    1  \n",
       "4          2     3      53       -314    1  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "1089995    5    12      59       -335    1  \n",
       "1089996    7     1       1       -323    0  \n",
       "1089997    6    20       0       -855    0  \n",
       "1089998    4    16      23       -685    0  \n",
       "1089999    5    14      45       -617    1  \n",
       "\n",
       "[1090000 rows x 499 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.reset_index().drop(columns=\"num_index\")\n",
    "gc.collect()\n",
    "for i in range(1):\n",
    "    df = df.reindex(np.random.permutation(df.index)).reset_index().drop(columns=\"index\")\n",
    "    gc.collect()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=pd.read_feather(\"../BigFiles/fea/w240_buy4.5_forcasr240min_IS_GOOD.fea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>volume-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>high-3</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_volume-3_5min</th>\n",
       "      <th>BTC_high-4_5min</th>\n",
       "      <th>BTC_low-4_5min</th>\n",
       "      <th>BTC_close-4_5min</th>\n",
       "      <th>BTC_volume-4_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.572000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>739.418910</td>\n",
       "      <td>-0.006969</td>\n",
       "      <td>-0.005141</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>605.360330</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>-758</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.820000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>542.518750</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>-0.003985</td>\n",
       "      <td>-0.005581</td>\n",
       "      <td>579.284000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.915675</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>0.004676</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>3567.87</td>\n",
       "      <td>-0.005960</td>\n",
       "      <td>0.005501</td>\n",
       "      <td>-0.001342</td>\n",
       "      <td>15473.960</td>\n",
       "      <td>0.004325</td>\n",
       "      <td>...</td>\n",
       "      <td>221.160582</td>\n",
       "      <td>-0.005449</td>\n",
       "      <td>-0.002277</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>200.687935</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>-223</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.815000</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>0.000337</td>\n",
       "      <td>85.610</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>...</td>\n",
       "      <td>127.609940</td>\n",
       "      <td>-0.000746</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>94.250820</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>-679</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.072900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>128.378030</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>96.998120</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>-314</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089995</th>\n",
       "      <td>0.110050</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.003180</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>5950.000</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>...</td>\n",
       "      <td>205.213660</td>\n",
       "      <td>-0.001845</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>123.195440</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>-335</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089996</th>\n",
       "      <td>0.163467</td>\n",
       "      <td>-0.000260</td>\n",
       "      <td>0.002554</td>\n",
       "      <td>0.002371</td>\n",
       "      <td>241500.10</td>\n",
       "      <td>-0.001789</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>127066.200</td>\n",
       "      <td>-0.002401</td>\n",
       "      <td>...</td>\n",
       "      <td>86.199610</td>\n",
       "      <td>-0.002866</td>\n",
       "      <td>-0.000300</td>\n",
       "      <td>-0.000647</td>\n",
       "      <td>118.047930</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-323</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089997</th>\n",
       "      <td>5.860500</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>7094.09</td>\n",
       "      <td>-0.009470</td>\n",
       "      <td>-0.005546</td>\n",
       "      <td>-0.007593</td>\n",
       "      <td>8057.994</td>\n",
       "      <td>-0.008446</td>\n",
       "      <td>...</td>\n",
       "      <td>118.296080</td>\n",
       "      <td>-0.002173</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>-0.000688</td>\n",
       "      <td>129.590440</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>-855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089998</th>\n",
       "      <td>3.393800</td>\n",
       "      <td>-0.000413</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>109.33</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>5.680</td>\n",
       "      <td>-0.000324</td>\n",
       "      <td>...</td>\n",
       "      <td>593.171030</td>\n",
       "      <td>-0.001501</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.001660</td>\n",
       "      <td>773.236620</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>-685</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1089999</th>\n",
       "      <td>0.117760</td>\n",
       "      <td>-0.001274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>669.50</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>-0.001444</td>\n",
       "      <td>-0.001529</td>\n",
       "      <td>5644.800</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>...</td>\n",
       "      <td>628.509790</td>\n",
       "      <td>-0.010186</td>\n",
       "      <td>-0.008415</td>\n",
       "      <td>-0.009881</td>\n",
       "      <td>95.214170</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>-617</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1090000 rows  499 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             price    high-1     low-1   close-1   volume-1    high-2  \\\n",
       "0         0.572000  0.000000  0.000000  0.000000       0.00  0.000000   \n",
       "1         3.820000  0.000000  0.000000  0.000000       3.56  0.000000   \n",
       "2        33.915675 -0.002221  0.004676  0.001300    3567.87 -0.005960   \n",
       "3        14.815000  0.000337  0.000337  0.000337       0.00 -0.001012   \n",
       "4         0.072900  0.000000  0.000000  0.000000       0.00  0.000000   \n",
       "...            ...       ...       ...       ...        ...       ...   \n",
       "1089995   0.110050  0.001363  0.001363  0.001363       0.00  0.001363   \n",
       "1089996   0.163467 -0.000260  0.002554  0.002371  241500.10 -0.001789   \n",
       "1089997   5.860500 -0.008276 -0.005546 -0.005546    7094.09 -0.009470   \n",
       "1089998   3.393800 -0.000413  0.001296  0.000000     109.33  0.000177   \n",
       "1089999   0.117760 -0.001274  0.000000  0.000000     669.50 -0.001529   \n",
       "\n",
       "            low-2   close-2    volume-2    high-3  ...  BTC_volume-3_5min  \\\n",
       "0        0.000000  0.000000       0.000  0.000000  ...         739.418910   \n",
       "1        0.000000  0.000000      12.350  0.000000  ...         542.518750   \n",
       "2        0.005501 -0.001342   15473.960  0.004325  ...         221.160582   \n",
       "3        0.000337  0.000337      85.610 -0.001012  ...         127.609940   \n",
       "4        0.000000  0.000000       0.000  0.000000  ...         128.378030   \n",
       "...           ...       ...         ...       ...  ...                ...   \n",
       "1089995  0.003180  0.001363    5950.000  0.000454  ...         205.213660   \n",
       "1089996  0.000474 -0.000015  127066.200 -0.002401  ...          86.199610   \n",
       "1089997 -0.005546 -0.007593    8057.994 -0.008446  ...         118.296080   \n",
       "1089998  0.000177  0.000177       5.680 -0.000324  ...         593.171030   \n",
       "1089999 -0.001444 -0.001529    5644.800 -0.002802  ...         628.509790   \n",
       "\n",
       "         BTC_high-4_5min  BTC_low-4_5min  BTC_close-4_5min  BTC_volume-4_5min  \\\n",
       "0              -0.006969       -0.005141         -0.005663         605.360330   \n",
       "1              -0.006736       -0.003985         -0.005581         579.284000   \n",
       "2              -0.005449       -0.002277         -0.005378         200.687935   \n",
       "3              -0.000746        0.000915          0.000379          94.250820   \n",
       "4               0.000604        0.001454          0.000783          96.998120   \n",
       "...                  ...             ...               ...                ...   \n",
       "1089995        -0.001845        0.001947          0.001696         123.195440   \n",
       "1089996        -0.002866       -0.000300         -0.000647         118.047930   \n",
       "1089997        -0.002173        0.001198         -0.000688         129.590440   \n",
       "1089998        -0.001501        0.001733          0.001660         773.236620   \n",
       "1089999        -0.010186       -0.008415         -0.009881          95.214170   \n",
       "\n",
       "         day  hour  minute  lunch_day  buy  \n",
       "0          1     5      41       -758    1  \n",
       "1          4     0       1       -355    1  \n",
       "2          1    12      45       -223    0  \n",
       "3          3    20      18       -679    1  \n",
       "4          2     3      53       -314    1  \n",
       "...      ...   ...     ...        ...  ...  \n",
       "1089995    5    12      59       -335    1  \n",
       "1089996    7     1       1       -323    0  \n",
       "1089997    6    20       0       -855    0  \n",
       "1089998    4    16      23       -685    0  \n",
       "1089999    5    14      45       -617    1  \n",
       "\n",
       "[1090000 rows x 499 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_feather(f\"../BigFiles/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df choosen data shape(1090000, 499)\n",
      "pair: True\n",
      "218000\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "print(\"df choosen data shape\"+str(df.shape))\n",
    "print(f\"pair: {(df.shape[0]/2)==df.buy.sum()}\")\n",
    "dt=df.to_numpy(dtype=np.float32)\n",
    "#dt=df.to_numpy()\n",
    "dt=np.nan_to_num(dt,nan=0)\n",
    "#dt=dt.astype(np.float32)\n",
    "dt=np.nan_to_num(dt, neginf=0) \n",
    "dt=np.nan_to_num(dt, posinf=0) \n",
    "\n",
    "index_20pct= int(0.2*len(dt[:,0]))\n",
    "print(index_20pct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # feather loading\n",
    "# df=pd.read_feather(f\"../Data/fea/w{WINDOW_SIZE}_buy{BUY_PCT}_forcasr{MAX_FORCAST_SIZE}min_{BUY_MODE}.fea\")\n",
    "# dt=df.to_numpy(dtype=np.float32)\n",
    "# dt=fixdt(dt)\n",
    "# index_20pct= int(0.2*len(dt[:,0]))\n",
    "# gc.collect()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-Normalized Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Model Plus - Very deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# from keras.layers import Dense\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# # Define the class weights\n",
    "# class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# SizeTunner = 1\n",
    "# IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "# model.add(Dense(int(300 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "# ]\n",
    "\n",
    "# print(\"saving file in: \" + Model_FileName)\n",
    "# history = model.fit(dt[index_20pct:, :-1],\n",
    "#                     dt[index_20pct:, -1],\n",
    "#                     validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "#                     epochs=500,\n",
    "#                     batch_size=256*10,\n",
    "#                     callbacks=callbacks,\n",
    "#                     class_weight=class_weights)\n",
    "\n",
    "# verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "# model.save(verydeep_model_file)\n",
    "# print(verydeep_model_file)\n",
    "# very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 03:59:07.913996: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-05-01 03:59:07.915321: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-05-01 03:59:07.915897: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: abj-K93SV\n",
      "2023-05-01 03:59:07.915917: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: abj-K93SV\n",
      "2023-05-01 03:59:07.916988: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: NOT_FOUND: was unable to find libcuda.so DSO loaded into this program\n",
      "2023-05-01 03:59:07.918814: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.154.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 498)              1992      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 300)              1200      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 200)               60200     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 200)              800       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 80)                16080     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 80)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 80)                6480      \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 80)               320       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                1620      \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 20)               80        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 259,213\n",
      "Trainable params: 256,377\n",
      "Non-trainable params: 2,836\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 03:59:14.940819: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1737024000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3407/3407 [==============================] - 97s 27ms/step - loss: 0.4451 - accuracy: 0.7818 - val_loss: 0.4057 - val_accuracy: 0.8057\n",
      "Epoch 2/500\n",
      "3407/3407 [==============================] - 95s 28ms/step - loss: 0.4073 - accuracy: 0.8066 - val_loss: 0.3955 - val_accuracy: 0.8118\n",
      "Epoch 3/500\n",
      "3407/3407 [==============================] - 95s 28ms/step - loss: 0.3876 - accuracy: 0.8182 - val_loss: 0.3685 - val_accuracy: 0.8253\n",
      "Epoch 4/500\n",
      "3407/3407 [==============================] - 95s 28ms/step - loss: 0.3722 - accuracy: 0.8268 - val_loss: 0.3626 - val_accuracy: 0.8264\n",
      "Epoch 5/500\n",
      "3407/3407 [==============================] - 111s 33ms/step - loss: 0.3612 - accuracy: 0.8334 - val_loss: 0.3614 - val_accuracy: 0.8303\n",
      "Epoch 6/500\n",
      "3407/3407 [==============================] - 124s 37ms/step - loss: 0.3524 - accuracy: 0.8380 - val_loss: 0.3415 - val_accuracy: 0.8446\n",
      "Epoch 7/500\n",
      "3407/3407 [==============================] - 86s 25ms/step - loss: 0.3454 - accuracy: 0.8418 - val_loss: 0.3204 - val_accuracy: 0.8558\n",
      "Epoch 8/500\n",
      "3407/3407 [==============================] - 93s 27ms/step - loss: 0.3399 - accuracy: 0.8450 - val_loss: 0.3396 - val_accuracy: 0.8402\n",
      "Epoch 9/500\n",
      "3407/3407 [==============================] - 93s 27ms/step - loss: 0.3354 - accuracy: 0.8473 - val_loss: 0.3447 - val_accuracy: 0.8412\n",
      "Epoch 10/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3311 - accuracy: 0.8500 - val_loss: 0.3309 - val_accuracy: 0.8499\n",
      "Epoch 11/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3272 - accuracy: 0.8528 - val_loss: 0.3811 - val_accuracy: 0.8177\n",
      "Epoch 12/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3240 - accuracy: 0.8540 - val_loss: 0.3101 - val_accuracy: 0.8625\n",
      "Epoch 13/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3207 - accuracy: 0.8560 - val_loss: 0.3280 - val_accuracy: 0.8504\n",
      "Epoch 14/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3184 - accuracy: 0.8574 - val_loss: 0.3337 - val_accuracy: 0.8470\n",
      "Epoch 15/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3154 - accuracy: 0.8590 - val_loss: 0.3224 - val_accuracy: 0.8528\n",
      "Epoch 16/500\n",
      "3407/3407 [==============================] - 94s 27ms/step - loss: 0.3141 - accuracy: 0.8599 - val_loss: 0.3285 - val_accuracy: 0.8479\n",
      "Epoch 17/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3113 - accuracy: 0.8616 - val_loss: 0.2996 - val_accuracy: 0.8670\n",
      "Epoch 18/500\n",
      "3407/3407 [==============================] - 94s 27ms/step - loss: 0.3100 - accuracy: 0.8622 - val_loss: 0.3339 - val_accuracy: 0.8465\n",
      "Epoch 19/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3077 - accuracy: 0.8636 - val_loss: 0.3133 - val_accuracy: 0.8631\n",
      "Epoch 20/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3067 - accuracy: 0.8642 - val_loss: 0.3133 - val_accuracy: 0.8595\n",
      "Epoch 21/500\n",
      "3407/3407 [==============================] - 95s 28ms/step - loss: 0.3047 - accuracy: 0.8653 - val_loss: 0.3146 - val_accuracy: 0.8582\n",
      "Epoch 22/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3028 - accuracy: 0.8665 - val_loss: 0.3118 - val_accuracy: 0.8596\n",
      "Epoch 23/500\n",
      "3407/3407 [==============================] - 94s 28ms/step - loss: 0.3018 - accuracy: 0.8669 - val_loss: 0.3274 - val_accuracy: 0.8502\n",
      "Epoch 23: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_VeryDeep.h5\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 1\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=6, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256,  # Reduced batch size\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights,\n",
    "                    workers=8)  # Use 8 workers for multi-core systems\n",
    "\n",
    "verydeep_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "verydeep_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\"\n",
    "\n",
    "very_deep_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-  Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stats plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 04:36:15.398639: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 147s 4ms/step\n",
      "Normal Prediction ratio: 41.658%\n",
      "ModelAccuracy: 74.016%\n",
      "True Win Predictions Mean of all: 32.837%\n",
      "XXX Loss Buy Mean of all: 8.821%\n",
      "Missed good deal off all: 17.163%\n",
      "Good Zero prediction Mean: 41.179%\n",
      "good fiability\n",
      "========= Win Ratio:78.82519564069327 ====================\n"
     ]
    }
   ],
   "source": [
    "USED_MODEL=very_deep_model\n",
    "USED_MODEL=load_model(\"/UltimeTradingBot/Data/IS_GOOD/tp450_w240_max240min_Model_minicnn.h5\")\n",
    "#model_init=model\n",
    "#USED_MODEL=model_init#load_model(\"/UltimeTradingBot/Data/BUY_UP_CLOSE/tp60_w6_max3min_Model_GoodVeryDeep.h5\")\n",
    "Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "prediction2=Prediction_Note.round()\n",
    "hp(prediction2[:,0].mean(),\"Normal Prediction ratio\")\n",
    "\n",
    "Y=dt[:,-1].copy()\n",
    "Pred01=prediction2[:,-1]\n",
    "Original_Traget_Data=Y\n",
    "\n",
    "Predicted_Data=Pred01\n",
    "\n",
    "TruePred=((Original_Traget_Data==Predicted_Data)).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PesemisticPrediction ratio: 19.497%\n",
      "$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ for Precision : -0.25 $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\n",
      "ModelAccuracy: 64.402%\n",
      "True Win Predictions Mean of all: 16.949%\n",
      "XXX Loss Buy Mean of all: 2.548%\n",
      "Missed good deal off all: 33.051%\n",
      "Good Zero prediction Mean: 47.452%\n",
      "good fiability\n",
      "========= Win Ratio:86.93132276760528 ====================\n"
     ]
    }
   ],
   "source": [
    "PRECISION=-0.25\n",
    "PesemisticPrediction=(Prediction_Note[:,0]+PRECISION).round()\n",
    "hp(PesemisticPrediction.mean(),\"PesemisticPrediction ratio\")\n",
    "Pred02=PesemisticPrediction\n",
    "\n",
    "print(f\"$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ for Precision : {PRECISION} $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$\" )\n",
    "Predicted_Data=Pred02\n",
    "\n",
    "TruePred=((Original_Traget_Data==Predicted_Data)).copy()\n",
    "ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "if( fiability == 100):print(\"good fiability\")\n",
    "else: print(f\"check the fiability {fiability}\")\n",
    "\n",
    "winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "# plot_data(\"Original\", pair_to_test, winratio, OnePair_DF, i_start, 600, OnePair_DF.buy,dot_color=\"r\",fig_width=20, fig_height=7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# True PredONly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_9 (Batc  (None, 498)              1992      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 04:39:17.335537: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171082792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852/852 [==============================] - 82s 94ms/step - loss: 0.0967 - accuracy: 0.8493 - val_loss: 0.2988 - val_accuracy: 0.8687\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0728 - accuracy: 0.8904 - val_loss: 0.2592 - val_accuracy: 0.8990\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0647 - accuracy: 0.9029 - val_loss: 0.2309 - val_accuracy: 0.9293\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0599 - accuracy: 0.9097 - val_loss: 0.2191 - val_accuracy: 0.9394\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0567 - accuracy: 0.9147 - val_loss: 0.2486 - val_accuracy: 0.9192\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0544 - accuracy: 0.9186 - val_loss: 0.1978 - val_accuracy: 0.9192\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0524 - accuracy: 0.9213 - val_loss: 0.2418 - val_accuracy: 0.9293\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0511 - accuracy: 0.9232 - val_loss: 0.1872 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 0.0498 - accuracy: 0.9262 - val_loss: 0.1552 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 81s 96ms/step - loss: 0.0485 - accuracy: 0.9277 - val_loss: 0.2102 - val_accuracy: 0.9394\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0477 - accuracy: 0.9290 - val_loss: 0.2127 - val_accuracy: 0.9192\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0467 - accuracy: 0.9305 - val_loss: 0.2022 - val_accuracy: 0.9394\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0460 - accuracy: 0.9316 - val_loss: 0.1477 - val_accuracy: 0.9293\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0451 - accuracy: 0.9327 - val_loss: 0.1518 - val_accuracy: 0.9596\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0445 - accuracy: 0.9342 - val_loss: 0.1954 - val_accuracy: 0.9394\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0440 - accuracy: 0.9349 - val_loss: 0.1887 - val_accuracy: 0.9293\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 0.0435 - accuracy: 0.9357 - val_loss: 0.1639 - val_accuracy: 0.9293\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 83s 98ms/step - loss: 0.0431 - accuracy: 0.9360 - val_loss: 0.2060 - val_accuracy: 0.9192\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0424 - accuracy: 0.9369 - val_loss: 0.1861 - val_accuracy: 0.9192\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0421 - accuracy: 0.9378 - val_loss: 0.1290 - val_accuracy: 0.9293\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0417 - accuracy: 0.9384 - val_loss: 0.1927 - val_accuracy: 0.9394\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0413 - accuracy: 0.9393 - val_loss: 0.1601 - val_accuracy: 0.9495\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0410 - accuracy: 0.9394 - val_loss: 0.1995 - val_accuracy: 0.9394\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0407 - accuracy: 0.9404 - val_loss: 0.1932 - val_accuracy: 0.9495\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 0.0404 - accuracy: 0.9406 - val_loss: 0.2250 - val_accuracy: 0.9192\n",
      "Epoch 25: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re1.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 05:12:48.527439: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171280000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34063/34063 [==============================] - 127s 4ms/step\n",
      "ModelAccuracy: 60.636%\n",
      "True Win Predictions Mean of all: 12.017%\n",
      "XXX Loss Buy Mean of all: 1.380%\n",
      "Missed good deal off all: 37.983%\n",
      "Good Zero prediction Mean: 48.620%\n",
      "good fiability\n",
      "========= Win Ratio:89.69918638501157 ====================\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_14 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 300)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 120)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 05:15:17.362894: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 2171082792 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "852/852 [==============================] - 76s 88ms/step - loss: 0.0474 - accuracy: 0.8956 - val_loss: 0.2321 - val_accuracy: 0.8990\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 75s 88ms/step - loss: 0.0347 - accuracy: 0.9195 - val_loss: 0.1447 - val_accuracy: 0.9394\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 0.0306 - accuracy: 0.9301 - val_loss: 0.1134 - val_accuracy: 0.9192\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0282 - accuracy: 0.9361 - val_loss: 0.0982 - val_accuracy: 0.9495\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 81s 94ms/step - loss: 0.0266 - accuracy: 0.9399 - val_loss: 0.1291 - val_accuracy: 0.9293\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0253 - accuracy: 0.9430 - val_loss: 0.0868 - val_accuracy: 0.9495\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0245 - accuracy: 0.9448 - val_loss: 0.1286 - val_accuracy: 0.9091\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0236 - accuracy: 0.9467 - val_loss: 0.1285 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0230 - accuracy: 0.9483 - val_loss: 0.1547 - val_accuracy: 0.9091\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0224 - accuracy: 0.9495 - val_loss: 0.1133 - val_accuracy: 0.9293\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0220 - accuracy: 0.9504 - val_loss: 0.1769 - val_accuracy: 0.9192\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0215 - accuracy: 0.9514 - val_loss: 0.1011 - val_accuracy: 0.9596\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0211 - accuracy: 0.9526 - val_loss: 0.0733 - val_accuracy: 0.9697\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0208 - accuracy: 0.9533 - val_loss: 0.1101 - val_accuracy: 0.9394\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0205 - accuracy: 0.9541 - val_loss: 0.0989 - val_accuracy: 0.9495\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 88s 103ms/step - loss: 0.0202 - accuracy: 0.9547 - val_loss: 0.0925 - val_accuracy: 0.9495\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 90s 106ms/step - loss: 0.0200 - accuracy: 0.9553 - val_loss: 0.0851 - val_accuracy: 0.9495\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 92s 108ms/step - loss: 0.0197 - accuracy: 0.9561 - val_loss: 0.0889 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 92s 108ms/step - loss: 0.0194 - accuracy: 0.9568 - val_loss: 0.0885 - val_accuracy: 0.9394\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 92s 108ms/step - loss: 0.0192 - accuracy: 0.9577 - val_loss: 0.0992 - val_accuracy: 0.9495\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 95s 111ms/step - loss: 0.0190 - accuracy: 0.9577 - val_loss: 0.0915 - val_accuracy: 0.9596\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0189 - accuracy: 0.9587 - val_loss: 0.0840 - val_accuracy: 0.9697\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0187 - accuracy: 0.9591 - val_loss: 0.0796 - val_accuracy: 0.9798\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0186 - accuracy: 0.9591 - val_loss: 0.0689 - val_accuracy: 0.9596\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0184 - accuracy: 0.9599 - val_loss: 0.0504 - val_accuracy: 0.9697\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 93s 110ms/step - loss: 0.0183 - accuracy: 0.9599 - val_loss: 0.0660 - val_accuracy: 0.9596\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 95s 111ms/step - loss: 0.0180 - accuracy: 0.9608 - val_loss: 0.0646 - val_accuracy: 0.9697\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 90s 106ms/step - loss: 0.0180 - accuracy: 0.9607 - val_loss: 0.0859 - val_accuracy: 0.9596\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0178 - accuracy: 0.9612 - val_loss: 0.1104 - val_accuracy: 0.9394\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0178 - accuracy: 0.9610 - val_loss: 0.0732 - val_accuracy: 0.9697\n",
      "Epoch 31/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0175 - accuracy: 0.9619 - val_loss: 0.0750 - val_accuracy: 0.9495\n",
      "Epoch 32/500\n",
      "852/852 [==============================] - 96s 113ms/step - loss: 0.0174 - accuracy: 0.9614 - val_loss: 0.0602 - val_accuracy: 0.9798\n",
      "Epoch 33/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0174 - accuracy: 0.9618 - val_loss: 0.0792 - val_accuracy: 0.9798\n",
      "Epoch 34/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0173 - accuracy: 0.9619 - val_loss: 0.0678 - val_accuracy: 0.9798\n",
      "Epoch 35/500\n",
      "852/852 [==============================] - 95s 112ms/step - loss: 0.0171 - accuracy: 0.9625 - val_loss: 0.0740 - val_accuracy: 0.9596\n",
      "Epoch 36/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0171 - accuracy: 0.9622 - val_loss: 0.0530 - val_accuracy: 0.9798\n",
      "Epoch 37/500\n",
      "852/852 [==============================] - 93s 110ms/step - loss: 0.0169 - accuracy: 0.9629 - val_loss: 0.0474 - val_accuracy: 1.0000\n",
      "Epoch 38/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0168 - accuracy: 0.9633 - val_loss: 0.0727 - val_accuracy: 0.9697\n",
      "Epoch 39/500\n",
      "852/852 [==============================] - 93s 109ms/step - loss: 0.0167 - accuracy: 0.9639 - val_loss: 0.0685 - val_accuracy: 0.9899\n",
      "Epoch 40/500\n",
      "852/852 [==============================] - 94s 110ms/step - loss: 0.0166 - accuracy: 0.9637 - val_loss: 0.0607 - val_accuracy: 0.9697\n",
      "Epoch 41/500\n",
      "852/852 [==============================] - 94s 111ms/step - loss: 0.0167 - accuracy: 0.9634 - val_loss: 0.0599 - val_accuracy: 0.9798\n",
      "Epoch 42/500\n",
      "852/852 [==============================] - 95s 112ms/step - loss: 0.0165 - accuracy: 0.9639 - val_loss: 0.0474 - val_accuracy: 0.9899\n",
      "Epoch 43/500\n",
      "852/852 [==============================] - 90s 106ms/step - loss: 0.0165 - accuracy: 0.9642 - val_loss: 0.0649 - val_accuracy: 0.9798\n",
      "Epoch 44/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0163 - accuracy: 0.9647 - val_loss: 0.0941 - val_accuracy: 0.9697\n",
      "Epoch 45/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0163 - accuracy: 0.9644 - val_loss: 0.0771 - val_accuracy: 0.9798\n",
      "Epoch 46/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0162 - accuracy: 0.9648 - val_loss: 0.0516 - val_accuracy: 0.9899\n",
      "Epoch 47/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0161 - accuracy: 0.9647 - val_loss: 0.0633 - val_accuracy: 0.9697\n",
      "Epoch 48/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0161 - accuracy: 0.9648 - val_loss: 0.0570 - val_accuracy: 0.9798\n",
      "Epoch 49/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0160 - accuracy: 0.9651 - val_loss: 0.0490 - val_accuracy: 0.9899\n",
      "Epoch 50/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 0.0161 - accuracy: 0.9652 - val_loss: 0.0397 - val_accuracy: 0.9899\n",
      "Epoch 51/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0158 - accuracy: 0.9655 - val_loss: 0.0532 - val_accuracy: 0.9798\n",
      "Epoch 52/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0158 - accuracy: 0.9656 - val_loss: 0.0432 - val_accuracy: 0.9798\n",
      "Epoch 53/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0158 - accuracy: 0.9655 - val_loss: 0.0660 - val_accuracy: 0.9697\n",
      "Epoch 53: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re2.h5\n",
      "34063/34063 [==============================] - 129s 4ms/step\n",
      "ModelAccuracy: 59.141%\n",
      "True Win Predictions Mean of all: 9.315%\n",
      "XXX Loss Buy Mean of all: 0.174%\n",
      "Missed good deal off all: 40.685%\n",
      "Good Zero prediction Mean: 49.826%\n",
      "good fiability\n",
      "========= Win Ratio:98.16629781852671 ====================\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_19 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 74s 85ms/step - loss: 0.0345 - accuracy: 0.9145 - val_loss: 0.2072 - val_accuracy: 0.9192\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 77s 91ms/step - loss: 0.0236 - accuracy: 0.9316 - val_loss: 0.1670 - val_accuracy: 0.9394\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0207 - accuracy: 0.9404 - val_loss: 0.1587 - val_accuracy: 0.9495\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 0.0189 - accuracy: 0.9456 - val_loss: 0.1275 - val_accuracy: 0.9495\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0177 - accuracy: 0.9490 - val_loss: 0.1444 - val_accuracy: 0.9596\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0167 - accuracy: 0.9521 - val_loss: 0.1075 - val_accuracy: 0.9697\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0159 - accuracy: 0.9540 - val_loss: 0.0766 - val_accuracy: 0.9596\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0152 - accuracy: 0.9565 - val_loss: 0.0924 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0148 - accuracy: 0.9576 - val_loss: 0.1728 - val_accuracy: 0.9495\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0143 - accuracy: 0.9594 - val_loss: 0.1500 - val_accuracy: 0.9596\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0140 - accuracy: 0.9606 - val_loss: 0.1122 - val_accuracy: 0.9495\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0135 - accuracy: 0.9615 - val_loss: 0.0898 - val_accuracy: 0.9495\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0133 - accuracy: 0.9620 - val_loss: 0.1313 - val_accuracy: 0.9596\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0131 - accuracy: 0.9630 - val_loss: 0.0986 - val_accuracy: 0.9495\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0128 - accuracy: 0.9638 - val_loss: 0.1064 - val_accuracy: 0.9495\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0126 - accuracy: 0.9648 - val_loss: 0.0846 - val_accuracy: 0.9495\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0124 - accuracy: 0.9653 - val_loss: 0.0904 - val_accuracy: 0.9697\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 81s 96ms/step - loss: 0.0122 - accuracy: 0.9665 - val_loss: 0.0816 - val_accuracy: 0.9596\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0121 - accuracy: 0.9665 - val_loss: 0.1397 - val_accuracy: 0.9394\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0120 - accuracy: 0.9665 - val_loss: 0.0962 - val_accuracy: 0.9394\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0118 - accuracy: 0.9672 - val_loss: 0.0757 - val_accuracy: 0.9697\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0117 - accuracy: 0.9673 - val_loss: 0.0763 - val_accuracy: 0.9495\n",
      "Epoch 22: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re3.h5\n",
      "34063/34063 [==============================] - 127s 4ms/step\n",
      "ModelAccuracy: 55.952%\n",
      "True Win Predictions Mean of all: 6.030%\n",
      "XXX Loss Buy Mean of all: 0.077%\n",
      "Missed good deal off all: 43.970%\n",
      "Good Zero prediction Mean: 49.923%\n",
      "good fiability\n",
      "========= Win Ratio:98.7391517930244 ====================\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_24 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_14 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 75s 86ms/step - loss: 0.0192 - accuracy: 0.9420 - val_loss: 0.1232 - val_accuracy: 0.9394\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 77s 91ms/step - loss: 0.0124 - accuracy: 0.9491 - val_loss: 0.1380 - val_accuracy: 0.9697\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 0.0108 - accuracy: 0.9534 - val_loss: 0.0464 - val_accuracy: 0.9697\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 76s 90ms/step - loss: 0.0099 - accuracy: 0.9567 - val_loss: 0.0430 - val_accuracy: 0.9697\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0091 - accuracy: 0.9595 - val_loss: 0.1012 - val_accuracy: 0.9798\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0086 - accuracy: 0.9622 - val_loss: 0.0563 - val_accuracy: 0.9798\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0082 - accuracy: 0.9641 - val_loss: 0.0488 - val_accuracy: 0.9697\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0077 - accuracy: 0.9660 - val_loss: 0.0390 - val_accuracy: 0.9798\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0075 - accuracy: 0.9670 - val_loss: 0.0243 - val_accuracy: 0.9899\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0072 - accuracy: 0.9680 - val_loss: 0.0188 - val_accuracy: 0.9899\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0071 - accuracy: 0.9693 - val_loss: 0.0876 - val_accuracy: 0.9798\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0068 - accuracy: 0.9711 - val_loss: 0.0498 - val_accuracy: 0.9798\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0066 - accuracy: 0.9714 - val_loss: 0.0316 - val_accuracy: 0.9899\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0065 - accuracy: 0.9718 - val_loss: 0.0216 - val_accuracy: 0.9899\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0064 - accuracy: 0.9726 - val_loss: 0.0333 - val_accuracy: 0.9899\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 81s 96ms/step - loss: 0.0063 - accuracy: 0.9732 - val_loss: 0.0241 - val_accuracy: 0.9899\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0061 - accuracy: 0.9737 - val_loss: 0.0175 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0060 - accuracy: 0.9742 - val_loss: 0.0869 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0060 - accuracy: 0.9743 - val_loss: 0.0357 - val_accuracy: 0.9697\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0058 - accuracy: 0.9754 - val_loss: 0.0427 - val_accuracy: 0.9899\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0057 - accuracy: 0.9760 - val_loss: 0.0207 - val_accuracy: 0.9899\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0057 - accuracy: 0.9762 - val_loss: 0.0250 - val_accuracy: 0.9899\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0057 - accuracy: 0.9758 - val_loss: 0.0549 - val_accuracy: 0.9798\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0055 - accuracy: 0.9767 - val_loss: 0.0441 - val_accuracy: 0.9899\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0055 - accuracy: 0.9767 - val_loss: 0.0249 - val_accuracy: 0.9899\n",
      "Epoch 25: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re4.h5\n",
      "34063/34063 [==============================] - 133s 4ms/step\n",
      "ModelAccuracy: 54.017%\n",
      "True Win Predictions Mean of all: 4.039%\n",
      "XXX Loss Buy Mean of all: 0.021%\n",
      "Missed good deal off all: 45.961%\n",
      "Good Zero prediction Mean: 49.979%\n",
      "good fiability\n",
      "========= Win Ratio:99.48275862068967 ====================\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_29 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_46 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 73s 84ms/step - loss: 0.0083 - accuracy: 0.9628 - val_loss: 0.1737 - val_accuracy: 0.9596\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0062 - accuracy: 0.9653 - val_loss: 0.0628 - val_accuracy: 0.9596\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0055 - accuracy: 0.9676 - val_loss: 0.0348 - val_accuracy: 0.9798\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0051 - accuracy: 0.9692 - val_loss: 0.0813 - val_accuracy: 0.9798\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0047 - accuracy: 0.9714 - val_loss: 0.0424 - val_accuracy: 0.9596\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0045 - accuracy: 0.9730 - val_loss: 0.0556 - val_accuracy: 0.9798\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0042 - accuracy: 0.9743 - val_loss: 0.0744 - val_accuracy: 0.9798\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0041 - accuracy: 0.9750 - val_loss: 0.0289 - val_accuracy: 0.9899\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0039 - accuracy: 0.9760 - val_loss: 0.0392 - val_accuracy: 0.9899\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0038 - accuracy: 0.9770 - val_loss: 0.0525 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0037 - accuracy: 0.9771 - val_loss: 0.0823 - val_accuracy: 0.9798\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0036 - accuracy: 0.9775 - val_loss: 0.0318 - val_accuracy: 0.9798\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 0.0035 - accuracy: 0.9780 - val_loss: 0.0243 - val_accuracy: 0.9899\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0034 - accuracy: 0.9788 - val_loss: 0.0633 - val_accuracy: 0.9798\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 0.0033 - accuracy: 0.9794 - val_loss: 0.0476 - val_accuracy: 0.9798\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0033 - accuracy: 0.9795 - val_loss: 0.0163 - val_accuracy: 0.9899\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0032 - accuracy: 0.9803 - val_loss: 0.0210 - val_accuracy: 0.9798\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0032 - accuracy: 0.9808 - val_loss: 0.0215 - val_accuracy: 0.9899\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 0.0031 - accuracy: 0.9809 - val_loss: 0.0251 - val_accuracy: 0.9899\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0031 - accuracy: 0.9807 - val_loss: 0.0458 - val_accuracy: 0.9798\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0030 - accuracy: 0.9812 - val_loss: 0.0092 - val_accuracy: 0.9899\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0030 - accuracy: 0.9820 - val_loss: 0.0233 - val_accuracy: 0.9899\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 84s 98ms/step - loss: 0.0029 - accuracy: 0.9824 - val_loss: 0.0180 - val_accuracy: 0.9798\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0030 - accuracy: 0.9818 - val_loss: 0.0491 - val_accuracy: 0.9798\n",
      "Epoch 24: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re5.h5\n",
      "34063/34063 [==============================] - 129s 4ms/step\n",
      "ModelAccuracy: 52.411%\n",
      "True Win Predictions Mean of all: 2.422%\n",
      "XXX Loss Buy Mean of all: 0.011%\n",
      "Missed good deal off all: 47.578%\n",
      "Good Zero prediction Mean: 49.989%\n",
      "good fiability\n",
      "========= Win Ratio:99.54788327168104 ====================\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_34 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_50 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_20 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_51 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_21 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_52 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_53 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_56 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 73s 84ms/step - loss: 0.0041 - accuracy: 0.9764 - val_loss: 0.0815 - val_accuracy: 0.9798\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0026 - accuracy: 0.9796 - val_loss: 0.0443 - val_accuracy: 0.9798\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0023 - accuracy: 0.9808 - val_loss: 0.0367 - val_accuracy: 0.9798\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 0.0022 - accuracy: 0.9812 - val_loss: 0.0245 - val_accuracy: 0.9798\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0021 - accuracy: 0.9819 - val_loss: 0.0177 - val_accuracy: 0.9899\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0020 - accuracy: 0.9822 - val_loss: 0.0205 - val_accuracy: 0.9798\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0019 - accuracy: 0.9828 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 0.0018 - accuracy: 0.9833 - val_loss: 0.0198 - val_accuracy: 0.9899\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0017 - accuracy: 0.9837 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0017 - accuracy: 0.9843 - val_loss: 0.0059 - val_accuracy: 1.0000\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0016 - accuracy: 0.9841 - val_loss: 0.0189 - val_accuracy: 0.9899\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0016 - accuracy: 0.9846 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0015 - accuracy: 0.9853 - val_loss: 0.0124 - val_accuracy: 0.9899\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0015 - accuracy: 0.9851 - val_loss: 0.0080 - val_accuracy: 0.9899\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0015 - accuracy: 0.9849 - val_loss: 0.0153 - val_accuracy: 0.9899\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0015 - accuracy: 0.9855 - val_loss: 0.0091 - val_accuracy: 1.0000\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0014 - accuracy: 0.9861 - val_loss: 0.0103 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 0.0014 - accuracy: 0.9861 - val_loss: 0.0068 - val_accuracy: 1.0000\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0014 - accuracy: 0.9867 - val_loss: 0.0072 - val_accuracy: 1.0000\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0013 - accuracy: 0.9873 - val_loss: 0.0054 - val_accuracy: 1.0000\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 0.0013 - accuracy: 0.9872 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 0.0014 - accuracy: 0.9870 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 0.0013 - accuracy: 0.9870 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 23: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re6.h5\n",
      "34063/34063 [==============================] - 127s 4ms/step\n",
      "ModelAccuracy: 51.288%\n",
      "True Win Predictions Mean of all: 1.289%\n",
      "XXX Loss Buy Mean of all: 0.001%\n",
      "Missed good deal off all: 48.711%\n",
      "Good Zero prediction Mean: 49.999%\n",
      "good fiability\n",
      "========= Win Ratio:99.92248062015506 ====================\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_39 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_57 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_22 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_58 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_23 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_59 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_42 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_24 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_60 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_43 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_61 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_62 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_63 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_64 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 74s 84ms/step - loss: 0.0046 - accuracy: 0.9855 - val_loss: 0.2034 - val_accuracy: 0.9596\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 9.0588e-04 - accuracy: 0.9897 - val_loss: 0.1721 - val_accuracy: 0.9798\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 7.9068e-04 - accuracy: 0.9902 - val_loss: 0.1380 - val_accuracy: 0.9798\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 7.2653e-04 - accuracy: 0.9905 - val_loss: 0.0948 - val_accuracy: 0.9798\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 7.2054e-04 - accuracy: 0.9903 - val_loss: 0.0717 - val_accuracy: 0.9798\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 6.9071e-04 - accuracy: 0.9908 - val_loss: 0.1456 - val_accuracy: 0.9798\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 6.8246e-04 - accuracy: 0.9906 - val_loss: 0.0758 - val_accuracy: 0.9798\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 6.5220e-04 - accuracy: 0.9910 - val_loss: 0.0696 - val_accuracy: 0.9798\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 6.6104e-04 - accuracy: 0.9908 - val_loss: 0.2035 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 6.5504e-04 - accuracy: 0.9909 - val_loss: 0.1010 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 6.2786e-04 - accuracy: 0.9910 - val_loss: 0.1375 - val_accuracy: 0.9697\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 5.9661e-04 - accuracy: 0.9913 - val_loss: 0.0579 - val_accuracy: 0.9798\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 6.0882e-04 - accuracy: 0.9910 - val_loss: 0.1101 - val_accuracy: 0.9798\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 5.7911e-04 - accuracy: 0.9914 - val_loss: 0.0340 - val_accuracy: 0.9798\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 5.8826e-04 - accuracy: 0.9912 - val_loss: 0.0383 - val_accuracy: 0.9798\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 5.5529e-04 - accuracy: 0.9920 - val_loss: 0.0439 - val_accuracy: 0.9798\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 83s 97ms/step - loss: 5.2386e-04 - accuracy: 0.9920 - val_loss: 0.0379 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 77s 91ms/step - loss: 5.4289e-04 - accuracy: 0.9916 - val_loss: 0.1043 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 5.3275e-04 - accuracy: 0.9916 - val_loss: 0.0189 - val_accuracy: 0.9899\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 5.1177e-04 - accuracy: 0.9917 - val_loss: 0.0183 - val_accuracy: 0.9899\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 5.0423e-04 - accuracy: 0.9922 - val_loss: 0.0680 - val_accuracy: 0.9798\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 4.9328e-04 - accuracy: 0.9921 - val_loss: 0.0325 - val_accuracy: 0.9899\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 4.7485e-04 - accuracy: 0.9923 - val_loss: 0.0651 - val_accuracy: 0.9798\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 4.4992e-04 - accuracy: 0.9929 - val_loss: 0.0474 - val_accuracy: 0.9798\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 4.4452e-04 - accuracy: 0.9929 - val_loss: 0.0152 - val_accuracy: 0.9899\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 4.3985e-04 - accuracy: 0.9932 - val_loss: 0.0459 - val_accuracy: 0.9798\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 4.6181e-04 - accuracy: 0.9926 - val_loss: 0.0532 - val_accuracy: 0.9798\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 4.6705e-04 - accuracy: 0.9926 - val_loss: 0.0139 - val_accuracy: 0.9899\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 81s 96ms/step - loss: 4.1487e-04 - accuracy: 0.9933 - val_loss: 0.0301 - val_accuracy: 0.9798\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 4.3034e-04 - accuracy: 0.9931 - val_loss: 0.0910 - val_accuracy: 0.9798\n",
      "Epoch 31/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 4.1999e-04 - accuracy: 0.9934 - val_loss: 0.0141 - val_accuracy: 0.9899\n",
      "Epoch 32/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 4.0851e-04 - accuracy: 0.9935 - val_loss: 0.0435 - val_accuracy: 0.9899\n",
      "Epoch 33/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 3.8704e-04 - accuracy: 0.9938 - val_loss: 0.0142 - val_accuracy: 0.9899\n",
      "Epoch 33: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re7.h5\n",
      "34063/34063 [==============================] - 130s 4ms/step\n",
      "ModelAccuracy: 50.543%\n",
      "True Win Predictions Mean of all: 0.543%\n",
      "XXX Loss Buy Mean of all: 0.000%\n",
      "Missed good deal off all: 49.457%\n",
      "Good Zero prediction Mean: 50.000%\n",
      "good fiability\n",
      "========= Win Ratio:100.0 ====================\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_44 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_65 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_45 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_25 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_66 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_46 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_67 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_68 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_69 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_70 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_71 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_72 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 75s 86ms/step - loss: 0.0013 - accuracy: 0.9944 - val_loss: 0.1442 - val_accuracy: 0.9697\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 77s 90ms/step - loss: 1.5500e-04 - accuracy: 0.9953 - val_loss: 0.1043 - val_accuracy: 0.9899\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 1.5824e-04 - accuracy: 0.9955 - val_loss: 0.1786 - val_accuracy: 0.9697\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 1.4302e-04 - accuracy: 0.9961 - val_loss: 0.1218 - val_accuracy: 0.9697\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 1.3892e-04 - accuracy: 0.9958 - val_loss: 0.0940 - val_accuracy: 0.9899\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 1.4168e-04 - accuracy: 0.9962 - val_loss: 0.0839 - val_accuracy: 0.9798\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 1.3966e-04 - accuracy: 0.9962 - val_loss: 0.1005 - val_accuracy: 0.9899\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 1.3969e-04 - accuracy: 0.9962 - val_loss: 0.1571 - val_accuracy: 0.9697\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 1.4315e-04 - accuracy: 0.9958 - val_loss: 0.1293 - val_accuracy: 0.9697\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 78s 91ms/step - loss: 1.3554e-04 - accuracy: 0.9961 - val_loss: 0.0572 - val_accuracy: 0.9899\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 1.3694e-04 - accuracy: 0.9962 - val_loss: 0.0516 - val_accuracy: 0.9899\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 1.3656e-04 - accuracy: 0.9961 - val_loss: 0.0936 - val_accuracy: 0.9899\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 1.1366e-04 - accuracy: 0.9966 - val_loss: 0.0924 - val_accuracy: 0.9899\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 1.2279e-04 - accuracy: 0.9965 - val_loss: 0.0559 - val_accuracy: 0.9899\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 1.2836e-04 - accuracy: 0.9958 - val_loss: 0.0677 - val_accuracy: 0.9899\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 1.1982e-04 - accuracy: 0.9966 - val_loss: 0.1513 - val_accuracy: 0.9899\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 1.1194e-04 - accuracy: 0.9965 - val_loss: 0.0671 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 81s 95ms/step - loss: 1.2454e-04 - accuracy: 0.9962 - val_loss: 0.0735 - val_accuracy: 0.9899\n",
      "Epoch 18: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re8.h5\n",
      "34063/34063 [==============================] - 129s 4ms/step\n",
      "ModelAccuracy: 50.231%\n",
      "True Win Predictions Mean of all: 0.231%\n",
      "XXX Loss Buy Mean of all: 0.000%\n",
      "Missed good deal off all: 49.769%\n",
      "Good Zero prediction Mean: 50.000%\n",
      "good fiability\n",
      "========= Win Ratio:100.0 ====================\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization_49 (Bat  (None, 498)              1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_73 (Dense)            (None, 300)               149700    \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 300)              1200      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_28 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_74 (Dense)            (None, 120)               36120     \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 120)               0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 120)              480       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 120)               14520     \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 30)                3630      \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 1)                 31        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 252,193\n",
      "Trainable params: 249,877\n",
      "Non-trainable params: 2,316\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_v3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 75s 85ms/step - loss: 0.0025 - accuracy: 0.9965 - val_loss: 0.0884 - val_accuracy: 0.9798\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 76s 90ms/step - loss: 2.4577e-05 - accuracy: 0.9977 - val_loss: 0.0338 - val_accuracy: 0.9798\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 1.8636e-05 - accuracy: 0.9978 - val_loss: 0.0341 - val_accuracy: 0.9798\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 1.2552e-05 - accuracy: 0.9988 - val_loss: 0.0153 - val_accuracy: 0.9899\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 1.7017e-05 - accuracy: 0.9988 - val_loss: 0.0098 - val_accuracy: 1.0000\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 1.8281e-05 - accuracy: 0.9986 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 2.0929e-05 - accuracy: 0.9985 - val_loss: 0.0047 - val_accuracy: 1.0000\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 80s 94ms/step - loss: 3.0529e-05 - accuracy: 0.9984 - val_loss: 0.0698 - val_accuracy: 0.9798\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 2.3396e-05 - accuracy: 0.9983 - val_loss: 0.2768 - val_accuracy: 0.9798\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 2.9078e-05 - accuracy: 0.9982 - val_loss: 0.0157 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 81s 96ms/step - loss: 2.0180e-05 - accuracy: 0.9989 - val_loss: 0.0116 - val_accuracy: 1.0000\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 77s 91ms/step - loss: 2.7954e-05 - accuracy: 0.9987 - val_loss: 0.0312 - val_accuracy: 0.9798\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 2.7792e-05 - accuracy: 0.9987 - val_loss: 0.1722 - val_accuracy: 0.9798\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 2.1769e-05 - accuracy: 0.9989 - val_loss: 0.1007 - val_accuracy: 0.9798\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 82s 96ms/step - loss: 1.7237e-05 - accuracy: 0.9992 - val_loss: 0.3431 - val_accuracy: 0.9798\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 79s 92ms/step - loss: 4.0558e-05 - accuracy: 0.9982 - val_loss: 0.0182 - val_accuracy: 0.9798\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 9.3459e-06 - accuracy: 0.9993 - val_loss: 1.0747e-04 - val_accuracy: 1.0000\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 78s 92ms/step - loss: 4.8262e-05 - accuracy: 0.9981 - val_loss: 0.0561 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 4.4287e-05 - accuracy: 0.9981 - val_loss: 0.2511 - val_accuracy: 0.9798\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 79s 93ms/step - loss: 3.3950e-05 - accuracy: 0.9983 - val_loss: 0.0171 - val_accuracy: 0.9798\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 80s 93ms/step - loss: 1.9931e-05 - accuracy: 0.9990 - val_loss: 0.0691 - val_accuracy: 0.9798\n",
      "Epoch 21: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_true_win_model_Re9.h5\n",
      "34063/34063 [==============================] - 128s 4ms/step\n",
      "ModelAccuracy: 50.000%\n",
      "True Win Predictions Mean of all: 0.000%\n",
      "XXX Loss Buy Mean of all: 0.000%\n",
      "Missed good deal off all: 50.000%\n",
      "Good Zero prediction Mean: 50.000%\n",
      "good fiability\n",
      "========= Win Ratio:nan ====================\n"
     ]
    }
   ],
   "source": [
    "# Change retaindt\n",
    "# PRECISION=-0.46\n",
    "# init_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "# predictions = init_model.predict(dt[:, :-1])\n",
    "# TrueWinPred = predictions.round()\n",
    "TrueWinPred = PesemisticPrediction\n",
    "for rrr in range(1, 10):\n",
    "    retrain_dt = dt\n",
    "    class_1_weight = TrueWinPred.mean()\n",
    "\n",
    "    import gc\n",
    "    from keras.layers import Dense, BatchNormalization, Dropout\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "    class_weights = {0: 1 - class_1_weight, 1: class_1_weight}\n",
    "    gc.collect()\n",
    "\n",
    "    SizeTunner = 1.5\n",
    "    IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "    model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(int(20 * SizeTunner), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"saving file in: \" + Model_FileName)\n",
    "    history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                        TrueWinPred[index_20pct:],\n",
    "                        validation_data=(retrain_dt[:index_20pct, :-1], TrueWinPred[:index_20pct]),\n",
    "                        epochs=500,\n",
    "                        batch_size=256 * 5,  # Reduced batch size\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weights,\n",
    "                        workers=8) \n",
    "\n",
    "    true_win_model=model\n",
    "    wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re{rrr}.h5\"\n",
    "    true_win_model.save(wheretosave)\n",
    "    print(wheretosave)\n",
    "    USED_MODEL=true_win_model\n",
    "    bad_Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "    Pred02=bad_Prediction_Note.round()\n",
    "    Original_Traget_Data=Y\n",
    "    Predicted_Data=Pred02[:,0]\n",
    "\n",
    "    BadTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    BadModelAccuracy=hp(BadTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "    BadTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    BadTrueWinPred_Mean=hp(BadTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "    BadLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    BadLossPred_Mean=hp(BadLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "    BadMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    BadMissedDeal_Mean=hp(BadMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "    BadGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    BadGoodZero_Mean=hp(BadGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "    fiability=BadTrueWinPred_Mean + BadLossPred_Mean + BadMissedDeal_Mean + BadGoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=BadTrueWinPred_Mean/(BadLossPred_Mean+BadTrueWinPred_Mean)\n",
    "    print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "    TrueWinPred=BadTrueWinPred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteratif model v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# from keras.layers import Conv1D, Flatten, Dense, BatchNormalization, Dropout, LeakyReLU,MaxPooling1D,GlobalMaxPooling1D\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.layers import Reshape\n",
    "\n",
    "# # Define the class weights\n",
    "# class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# SizeTunner = 0.2\n",
    "# IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Reshape((1, IN_DIM), input_shape=(IN_DIM,)))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv1D(256, kernel_size=3, padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(128, kernel_size=3, padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(64, kernel_size=3, padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(32, kernel_size=3, padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# model.add(Dense(128))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(64))\n",
    "# model.add(LeakyReLU(alpha=0.2))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# optimizer = SGD(lr=0.0001, momentum=0.9)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "#                     monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=15, verbose=1)\n",
    "# ]\n",
    "\n",
    "# print(f\"saving file in: {DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5\")\n",
    "\n",
    "# # Reshape the input data to have a single channel\n",
    "# X_train = dt[index_20pct:, :-1]\n",
    "# y_train = dt[index_20pct:, -1]\n",
    "# X_val = dt[:index_20pct, :-1]\n",
    "# y_val = dt[:index_20pct, -1]\n",
    "\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     epochs=30,\n",
    "#                     batch_size=256*5,\n",
    "#                     callbacks=callbacks,\n",
    "#                     class_weight=class_weights)\n",
    "\n",
    "# minicnn_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5'\n",
    "# model.save(minicnn_model_file)\n",
    "# print(minicnn_model_file)\n",
    "# minicnn_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, Flatten, Dense, BatchNormalization, Dropout, Activation, MaxPooling1D, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 498)]             0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 498)            0         \n",
      "                                                                 \n",
      " batch_normalization_54 (Bat  (None, 1, 498)           1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 256)            382720    \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1, 256)            0         \n",
      "                                                                 \n",
      " batch_normalization_55 (Bat  (None, 1, 256)           1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 1, 256)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 1, 128)            98432     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1, 128)            0         \n",
      "                                                                 \n",
      " batch_normalization_56 (Bat  (None, 1, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 1, 64)             24640     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 1, 64)             0         \n",
      "                                                                 \n",
      " batch_normalization_57 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1, 32)             6176      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " batch_normalization_58 (Bat  (None, 1, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 1, 32)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_59 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_60 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_61 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,369\n",
      "Trainable params: 528,965\n",
      "Non-trainable params: 2,404\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_minicnn.h5\n",
      "Epoch 1/300\n",
      "852/852 [==============================] - 160s 185ms/step - loss: 0.4588 - accuracy: 0.7717 - val_loss: 0.4244 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 2/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.4079 - accuracy: 0.8050 - val_loss: 0.3935 - val_accuracy: 0.7879 - lr: 0.0010\n",
      "Epoch 3/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.3860 - accuracy: 0.8174 - val_loss: 0.3700 - val_accuracy: 0.8384 - lr: 0.0010\n",
      "Epoch 4/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.3700 - accuracy: 0.8267 - val_loss: 0.3562 - val_accuracy: 0.8283 - lr: 0.0010\n",
      "Epoch 5/300\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.3585 - accuracy: 0.8339 - val_loss: 0.3480 - val_accuracy: 0.8283 - lr: 0.0010\n",
      "Epoch 6/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.3494 - accuracy: 0.8393 - val_loss: 0.3601 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 7/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.3421 - accuracy: 0.8436 - val_loss: 0.3424 - val_accuracy: 0.8384 - lr: 0.0010\n",
      "Epoch 8/300\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.3359 - accuracy: 0.8472 - val_loss: 0.3735 - val_accuracy: 0.8182 - lr: 0.0010\n",
      "Epoch 9/300\n",
      "852/852 [==============================] - 167s 195ms/step - loss: 0.3294 - accuracy: 0.8509 - val_loss: 0.3288 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 10/300\n",
      "852/852 [==============================] - 165s 193ms/step - loss: 0.3252 - accuracy: 0.8533 - val_loss: 0.3464 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 11/300\n",
      "852/852 [==============================] - 164s 193ms/step - loss: 0.3214 - accuracy: 0.8556 - val_loss: 0.3504 - val_accuracy: 0.8384 - lr: 0.0010\n",
      "Epoch 12/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.3178 - accuracy: 0.8580 - val_loss: 0.3289 - val_accuracy: 0.8889 - lr: 0.0010\n",
      "Epoch 13/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.3142 - accuracy: 0.8601 - val_loss: 0.3394 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 14/300\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.3114 - accuracy: 0.8615 - val_loss: 0.3660 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 15/300\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.3088 - accuracy: 0.8629 - val_loss: 0.3444 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 16/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.3071 - accuracy: 0.8644 - val_loss: 0.3427 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 17/300\n",
      "852/852 [==============================] - 163s 192ms/step - loss: 0.3042 - accuracy: 0.8657 - val_loss: 0.3234 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 18/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.3020 - accuracy: 0.8670 - val_loss: 0.3414 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 19/300\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.3005 - accuracy: 0.8682 - val_loss: 0.3160 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 20/300\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.2985 - accuracy: 0.8691 - val_loss: 0.3130 - val_accuracy: 0.8990 - lr: 0.0010\n",
      "Epoch 21/300\n",
      "852/852 [==============================] - 168s 198ms/step - loss: 0.2973 - accuracy: 0.8696 - val_loss: 0.3348 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 22/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.2954 - accuracy: 0.8709 - val_loss: 0.3260 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 23/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.2938 - accuracy: 0.8716 - val_loss: 0.3221 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 24/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.2926 - accuracy: 0.8724 - val_loss: 0.3309 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 25/300\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.2911 - accuracy: 0.8730 - val_loss: 0.3331 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 26/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.2901 - accuracy: 0.8738 - val_loss: 0.3148 - val_accuracy: 0.8586 - lr: 0.0010\n",
      "Epoch 27/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.2893 - accuracy: 0.8743 - val_loss: 0.3278 - val_accuracy: 0.8283 - lr: 0.0010\n",
      "Epoch 28/300\n",
      "852/852 [==============================] - 164s 193ms/step - loss: 0.2872 - accuracy: 0.8754 - val_loss: 0.3636 - val_accuracy: 0.8384 - lr: 0.0010\n",
      "Epoch 29/300\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.2864 - accuracy: 0.8761 - val_loss: 0.3231 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 30/300\n",
      "852/852 [==============================] - 165s 193ms/step - loss: 0.2858 - accuracy: 0.8764 - val_loss: 0.3473 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 31/300\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.2851 - accuracy: 0.8769 - val_loss: 0.3288 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 32/300\n",
      "852/852 [==============================] - 168s 198ms/step - loss: 0.2837 - accuracy: 0.8773 - val_loss: 0.3458 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 33/300\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.2827 - accuracy: 0.8779 - val_loss: 0.3288 - val_accuracy: 0.8485 - lr: 0.0010\n",
      "Epoch 34/300\n",
      "852/852 [==============================] - 163s 192ms/step - loss: 0.2822 - accuracy: 0.8782 - val_loss: 0.3590 - val_accuracy: 0.8687 - lr: 0.0010\n",
      "Epoch 35/300\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.2814 - accuracy: 0.8784 - val_loss: 0.3164 - val_accuracy: 0.8788 - lr: 0.0010\n",
      "Epoch 35: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_Model_minicnn.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the class weights\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 0.2\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "inputs = Input(shape=(IN_DIM,))\n",
    "\n",
    "x = Reshape((1, IN_DIM))(inputs)\n",
    "\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "x = Conv1D(256, kernel_size=3, padding='same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "x = Activation('relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "                    monitor='val_accuracy', save_best_only=True, save_weights_only=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=15, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=15, min_lr=0.0001)\n",
    "]\n",
    "\n",
    "print(f\"saving file in: {DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5\")\n",
    "\n",
    "# Reshape the input data to have a single channel\n",
    "X_train = dt[index_20pct:, :-1]\n",
    "y_train = dt[index_20pct:, -1]\n",
    "X_val = dt[:index_20pct, :-1]\n",
    "y_val = dt[:index_20pct, -1]\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    epochs=300,\n",
    "                    batch_size=256*5,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "minicnn_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5'\n",
    "model.save(minicnn_model_file)\n",
    "print(minicnn_model_file)\n",
    "minicnn_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 498)]             0         \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 1, 498)            0         \n",
      "                                                                 \n",
      " batch_normalization_62 (Bat  (None, 1, 498)           1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 1, 256)            382720    \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1, 256)            0         \n",
      "                                                                 \n",
      " batch_normalization_63 (Bat  (None, 1, 256)           1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 1, 256)            0         \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 1, 128)            98432     \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1, 128)            0         \n",
      "                                                                 \n",
      " batch_normalization_64 (Bat  (None, 1, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_6 (Conv1D)           (None, 1, 64)             24640     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 1, 64)             0         \n",
      "                                                                 \n",
      " batch_normalization_65 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_7 (Conv1D)           (None, 1, 32)             6176      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1, 32)             0         \n",
      "                                                                 \n",
      " batch_normalization_66 (Bat  (None, 1, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 1, 32)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_67 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_68 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_69 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_88 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,369\n",
      "Trainable params: 528,965\n",
      "Non-trainable params: 2,404\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN1.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 172s 199ms/step - loss: 0.1228 - accuracy: 0.8388 - val_loss: 0.4170 - val_accuracy: 0.8182\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 165s 193ms/step - loss: 0.0741 - accuracy: 0.8872 - val_loss: 0.3759 - val_accuracy: 0.8485\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0648 - accuracy: 0.9021 - val_loss: 0.2962 - val_accuracy: 0.8586\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0599 - accuracy: 0.9100 - val_loss: 0.2676 - val_accuracy: 0.9091\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0566 - accuracy: 0.9152 - val_loss: 0.2451 - val_accuracy: 0.8990\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0544 - accuracy: 0.9184 - val_loss: 0.2588 - val_accuracy: 0.8788\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.0528 - accuracy: 0.9213 - val_loss: 0.2802 - val_accuracy: 0.8788\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0510 - accuracy: 0.9237 - val_loss: 0.2056 - val_accuracy: 0.9192\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0501 - accuracy: 0.9252 - val_loss: 0.2421 - val_accuracy: 0.9091\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 167s 195ms/step - loss: 0.0490 - accuracy: 0.9270 - val_loss: 0.2812 - val_accuracy: 0.8889\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.0481 - accuracy: 0.9283 - val_loss: 0.2382 - val_accuracy: 0.8990\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0472 - accuracy: 0.9298 - val_loss: 0.2175 - val_accuracy: 0.9192\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0465 - accuracy: 0.9312 - val_loss: 0.2063 - val_accuracy: 0.9192\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0460 - accuracy: 0.9318 - val_loss: 0.1982 - val_accuracy: 0.9192\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0454 - accuracy: 0.9325 - val_loss: 0.1699 - val_accuracy: 0.9394\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0450 - accuracy: 0.9335 - val_loss: 0.1830 - val_accuracy: 0.9192\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0442 - accuracy: 0.9345 - val_loss: 0.2089 - val_accuracy: 0.9192\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0440 - accuracy: 0.9348 - val_loss: 0.2081 - val_accuracy: 0.9394\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0436 - accuracy: 0.9355 - val_loss: 0.1667 - val_accuracy: 0.9394\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0432 - accuracy: 0.9363 - val_loss: 0.2034 - val_accuracy: 0.9293\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0431 - accuracy: 0.9363 - val_loss: 0.2107 - val_accuracy: 0.9293\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0427 - accuracy: 0.9370 - val_loss: 0.1860 - val_accuracy: 0.9394\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.0423 - accuracy: 0.9377 - val_loss: 0.1815 - val_accuracy: 0.9293\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0420 - accuracy: 0.9382 - val_loss: 0.1985 - val_accuracy: 0.9192\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0417 - accuracy: 0.9389 - val_loss: 0.2004 - val_accuracy: 0.9293\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0414 - accuracy: 0.9394 - val_loss: 0.1514 - val_accuracy: 0.9293\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 164s 193ms/step - loss: 0.0412 - accuracy: 0.9394 - val_loss: 0.2058 - val_accuracy: 0.9293\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 165s 193ms/step - loss: 0.0409 - accuracy: 0.9401 - val_loss: 0.2324 - val_accuracy: 0.9091\n",
      "Epoch 29/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0406 - accuracy: 0.9407 - val_loss: 0.1955 - val_accuracy: 0.9091\n",
      "Epoch 30/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0406 - accuracy: 0.9405 - val_loss: 0.2220 - val_accuracy: 0.9293\n",
      "Epoch 31/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0406 - accuracy: 0.9404 - val_loss: 0.1944 - val_accuracy: 0.9394\n",
      "Epoch 31: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN1.h5\n",
      "34063/34063 [==============================] - 138s 4ms/step\n",
      "ModelAccuracy: 61.768%\n",
      "True Win Predictions Mean of all: 13.131%\n",
      "XXX Loss Buy Mean of all: 1.363%\n",
      "Missed good deal off all: 36.869%\n",
      "Good Zero prediction Mean: 48.637%\n",
      "good fiability\n",
      "========= Win Ratio:90.59610873464882 ====================\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 498)]             0         \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 1, 498)            0         \n",
      "                                                                 \n",
      " batch_normalization_70 (Bat  (None, 1, 498)           1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_8 (Conv1D)           (None, 1, 256)            382720    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 1, 256)            0         \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 1, 256)           1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 1, 256)            0         \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, 1, 128)            98432     \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 1, 128)            0         \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 1, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, 1, 64)             24640     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_11 (Conv1D)          (None, 1, 32)             6176      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 1, 32)             0         \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 1, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 1, 32)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_76 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_77 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,369\n",
      "Trainable params: 528,965\n",
      "Non-trainable params: 2,404\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN2.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 154s 176ms/step - loss: 0.0904 - accuracy: 0.8732 - val_loss: 0.2383 - val_accuracy: 0.8990\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 164s 193ms/step - loss: 0.0439 - accuracy: 0.9046 - val_loss: 0.1782 - val_accuracy: 0.9293\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 152s 178ms/step - loss: 0.0378 - accuracy: 0.9181 - val_loss: 0.1685 - val_accuracy: 0.9293\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0346 - accuracy: 0.9255 - val_loss: 0.1974 - val_accuracy: 0.9091\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0325 - accuracy: 0.9303 - val_loss: 0.1467 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 163s 192ms/step - loss: 0.0308 - accuracy: 0.9342 - val_loss: 0.1538 - val_accuracy: 0.9293\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0296 - accuracy: 0.9371 - val_loss: 0.1217 - val_accuracy: 0.9394\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0288 - accuracy: 0.9388 - val_loss: 0.1515 - val_accuracy: 0.9394\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0279 - accuracy: 0.9407 - val_loss: 0.1551 - val_accuracy: 0.9091\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0273 - accuracy: 0.9426 - val_loss: 0.1423 - val_accuracy: 0.9192\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0268 - accuracy: 0.9436 - val_loss: 0.1215 - val_accuracy: 0.9192\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0263 - accuracy: 0.9450 - val_loss: 0.0808 - val_accuracy: 0.9697\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0259 - accuracy: 0.9460 - val_loss: 0.1493 - val_accuracy: 0.9091\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0254 - accuracy: 0.9472 - val_loss: 0.1148 - val_accuracy: 0.9293\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0250 - accuracy: 0.9477 - val_loss: 0.1552 - val_accuracy: 0.9293\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0248 - accuracy: 0.9484 - val_loss: 0.1092 - val_accuracy: 0.9495\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.0245 - accuracy: 0.9495 - val_loss: 0.1369 - val_accuracy: 0.9192\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.0242 - accuracy: 0.9496 - val_loss: 0.1156 - val_accuracy: 0.9293\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0238 - accuracy: 0.9506 - val_loss: 0.1591 - val_accuracy: 0.9293\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0237 - accuracy: 0.9511 - val_loss: 0.1153 - val_accuracy: 0.9394\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0235 - accuracy: 0.9515 - val_loss: 0.1361 - val_accuracy: 0.9293\n",
      "Epoch 22/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0232 - accuracy: 0.9522 - val_loss: 0.1147 - val_accuracy: 0.9394\n",
      "Epoch 23/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0231 - accuracy: 0.9528 - val_loss: 0.1337 - val_accuracy: 0.9293\n",
      "Epoch 24/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0229 - accuracy: 0.9528 - val_loss: 0.1156 - val_accuracy: 0.9495\n",
      "Epoch 25/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0228 - accuracy: 0.9529 - val_loss: 0.1093 - val_accuracy: 0.9495\n",
      "Epoch 26/500\n",
      "852/852 [==============================] - 165s 193ms/step - loss: 0.0225 - accuracy: 0.9540 - val_loss: 0.1081 - val_accuracy: 0.9192\n",
      "Epoch 27/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0225 - accuracy: 0.9540 - val_loss: 0.1565 - val_accuracy: 0.9192\n",
      "Epoch 28/500\n",
      "852/852 [==============================] - 165s 194ms/step - loss: 0.0222 - accuracy: 0.9547 - val_loss: 0.1458 - val_accuracy: 0.9394\n",
      "Epoch 28: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN2.h5\n",
      "34063/34063 [==============================] - 139s 4ms/step\n",
      "ModelAccuracy: 57.570%\n",
      "True Win Predictions Mean of all: 7.755%\n",
      "XXX Loss Buy Mean of all: 0.186%\n",
      "Missed good deal off all: 42.245%\n",
      "Good Zero prediction Mean: 49.814%\n",
      "good fiability\n",
      "========= Win Ratio:97.65772572723839 ====================\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 498)]             0         \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 1, 498)            0         \n",
      "                                                                 \n",
      " batch_normalization_78 (Bat  (None, 1, 498)           1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_12 (Conv1D)          (None, 1, 256)            382720    \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 1, 256)            0         \n",
      "                                                                 \n",
      " batch_normalization_79 (Bat  (None, 1, 256)           1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 1, 256)            0         \n",
      "                                                                 \n",
      " conv1d_13 (Conv1D)          (None, 1, 128)            98432     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 1, 128)            0         \n",
      "                                                                 \n",
      " batch_normalization_80 (Bat  (None, 1, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_14 (Conv1D)          (None, 1, 64)             24640     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " batch_normalization_81 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_15 (Conv1D)          (None, 1, 32)             6176      \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 1, 32)             0         \n",
      "                                                                 \n",
      " batch_normalization_82 (Bat  (None, 1, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 1, 32)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_83 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_84 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_85 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_96 (Dense)            (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,369\n",
      "Trainable params: 528,965\n",
      "Non-trainable params: 2,404\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN3.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 168s 194ms/step - loss: 0.0677 - accuracy: 0.9198 - val_loss: 0.1997 - val_accuracy: 0.9293\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 161s 189ms/step - loss: 0.0194 - accuracy: 0.9367 - val_loss: 0.1510 - val_accuracy: 0.9293\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 162s 190ms/step - loss: 0.0163 - accuracy: 0.9444 - val_loss: 0.0856 - val_accuracy: 0.9596\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 157s 185ms/step - loss: 0.0147 - accuracy: 0.9493 - val_loss: 0.0401 - val_accuracy: 0.9899\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 164s 193ms/step - loss: 0.0137 - accuracy: 0.9532 - val_loss: 0.0977 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 152s 179ms/step - loss: 0.0130 - accuracy: 0.9558 - val_loss: 0.0764 - val_accuracy: 0.9596\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 161s 189ms/step - loss: 0.0125 - accuracy: 0.9582 - val_loss: 0.1014 - val_accuracy: 0.9495\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0119 - accuracy: 0.9600 - val_loss: 0.0838 - val_accuracy: 0.9697\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0115 - accuracy: 0.9616 - val_loss: 0.0974 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 166s 194ms/step - loss: 0.0112 - accuracy: 0.9629 - val_loss: 0.0544 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0108 - accuracy: 0.9641 - val_loss: 0.0492 - val_accuracy: 0.9899\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.0106 - accuracy: 0.9651 - val_loss: 0.0782 - val_accuracy: 0.9697\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0104 - accuracy: 0.9658 - val_loss: 0.0751 - val_accuracy: 0.9899\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0102 - accuracy: 0.9666 - val_loss: 0.0664 - val_accuracy: 0.9798\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 168s 197ms/step - loss: 0.0100 - accuracy: 0.9675 - val_loss: 0.0541 - val_accuracy: 0.9697\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0098 - accuracy: 0.9680 - val_loss: 0.0559 - val_accuracy: 0.9899\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0096 - accuracy: 0.9685 - val_loss: 0.0597 - val_accuracy: 0.9899\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 169s 199ms/step - loss: 0.0095 - accuracy: 0.9686 - val_loss: 0.0400 - val_accuracy: 0.9899\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 168s 198ms/step - loss: 0.0094 - accuracy: 0.9692 - val_loss: 0.1082 - val_accuracy: 0.9697\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 169s 199ms/step - loss: 0.0092 - accuracy: 0.9698 - val_loss: 0.0408 - val_accuracy: 0.9899\n",
      "Epoch 20: early stopping\n",
      "/UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN3.h5\n",
      "34063/34063 [==============================] - 139s 4ms/step\n",
      "ModelAccuracy: 54.858%\n",
      "True Win Predictions Mean of all: 4.910%\n",
      "XXX Loss Buy Mean of all: 0.052%\n",
      "Missed good deal off all: 45.090%\n",
      "Good Zero prediction Mean: 49.948%\n",
      "good fiability\n",
      "========= Win Ratio:98.95203546956873 ====================\n",
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 498)]             0         \n",
      "                                                                 \n",
      " reshape_4 (Reshape)         (None, 1, 498)            0         \n",
      "                                                                 \n",
      " batch_normalization_86 (Bat  (None, 1, 498)           1992      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_16 (Conv1D)          (None, 1, 256)            382720    \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 1, 256)            0         \n",
      "                                                                 \n",
      " batch_normalization_87 (Bat  (None, 1, 256)           1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 1, 256)            0         \n",
      "                                                                 \n",
      " conv1d_17 (Conv1D)          (None, 1, 128)            98432     \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 1, 128)            0         \n",
      "                                                                 \n",
      " batch_normalization_88 (Bat  (None, 1, 128)           512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 1, 128)            0         \n",
      "                                                                 \n",
      " conv1d_18 (Conv1D)          (None, 1, 64)             24640     \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 1, 64)             0         \n",
      "                                                                 \n",
      " batch_normalization_89 (Bat  (None, 1, 64)            256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 1, 64)             0         \n",
      "                                                                 \n",
      " conv1d_19 (Conv1D)          (None, 1, 32)             6176      \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 1, 32)             0         \n",
      "                                                                 \n",
      " batch_normalization_90 (Bat  (None, 1, 32)            128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 1, 32)             0         \n",
      "                                                                 \n",
      " global_max_pooling1d_4 (Glo  (None, 32)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 128)               4224      \n",
      "                                                                 \n",
      " batch_normalization_91 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_92 (Bat  (None, 64)               256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " batch_normalization_93 (Bat  (None, 32)               128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 531,369\n",
      "Trainable params: 528,965\n",
      "Non-trainable params: 2,404\n",
      "_________________________________________________________________\n",
      "None\n",
      "saving file in: /UltimeTradingBot/Data/IS_GOOD/tp270_w240_max240min_CNN__ReCNN4.h5\n",
      "Epoch 1/500\n",
      "852/852 [==============================] - 164s 188ms/step - loss: 0.0586 - accuracy: 0.9449 - val_loss: 0.2322 - val_accuracy: 0.9293\n",
      "Epoch 2/500\n",
      "852/852 [==============================] - 162s 191ms/step - loss: 0.0106 - accuracy: 0.9562 - val_loss: 0.1826 - val_accuracy: 0.9394\n",
      "Epoch 3/500\n",
      "852/852 [==============================] - 154s 180ms/step - loss: 0.0088 - accuracy: 0.9588 - val_loss: 0.0846 - val_accuracy: 0.9495\n",
      "Epoch 4/500\n",
      "852/852 [==============================] - 163s 192ms/step - loss: 0.0078 - accuracy: 0.9613 - val_loss: 0.1101 - val_accuracy: 0.9394\n",
      "Epoch 5/500\n",
      "852/852 [==============================] - 154s 181ms/step - loss: 0.0071 - accuracy: 0.9633 - val_loss: 0.0680 - val_accuracy: 0.9697\n",
      "Epoch 6/500\n",
      "852/852 [==============================] - 164s 192ms/step - loss: 0.0067 - accuracy: 0.9653 - val_loss: 0.0433 - val_accuracy: 0.9697\n",
      "Epoch 7/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0062 - accuracy: 0.9671 - val_loss: 0.0646 - val_accuracy: 0.9697\n",
      "Epoch 8/500\n",
      "852/852 [==============================] - 170s 200ms/step - loss: 0.0060 - accuracy: 0.9686 - val_loss: 0.0728 - val_accuracy: 0.9697\n",
      "Epoch 9/500\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.0057 - accuracy: 0.9700 - val_loss: 0.0504 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0054 - accuracy: 0.9718 - val_loss: 0.0421 - val_accuracy: 0.9798\n",
      "Epoch 11/500\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.0053 - accuracy: 0.9725 - val_loss: 0.0435 - val_accuracy: 0.9798\n",
      "Epoch 12/500\n",
      "852/852 [==============================] - 169s 199ms/step - loss: 0.0052 - accuracy: 0.9732 - val_loss: 0.0663 - val_accuracy: 0.9697\n",
      "Epoch 13/500\n",
      "852/852 [==============================] - 171s 200ms/step - loss: 0.0050 - accuracy: 0.9742 - val_loss: 0.0454 - val_accuracy: 0.9798\n",
      "Epoch 14/500\n",
      "852/852 [==============================] - 170s 199ms/step - loss: 0.0049 - accuracy: 0.9749 - val_loss: 0.0708 - val_accuracy: 0.9798\n",
      "Epoch 15/500\n",
      "852/852 [==============================] - 169s 198ms/step - loss: 0.0048 - accuracy: 0.9752 - val_loss: 0.0910 - val_accuracy: 0.9697\n",
      "Epoch 16/500\n",
      "852/852 [==============================] - 171s 200ms/step - loss: 0.0047 - accuracy: 0.9757 - val_loss: 0.0689 - val_accuracy: 0.9798\n",
      "Epoch 17/500\n",
      "852/852 [==============================] - 169s 199ms/step - loss: 0.0045 - accuracy: 0.9767 - val_loss: 0.0513 - val_accuracy: 0.9798\n",
      "Epoch 18/500\n",
      "852/852 [==============================] - 166s 195ms/step - loss: 0.0045 - accuracy: 0.9769 - val_loss: 0.0475 - val_accuracy: 0.9798\n",
      "Epoch 19/500\n",
      "852/852 [==============================] - 171s 201ms/step - loss: 0.0044 - accuracy: 0.9773 - val_loss: 0.0489 - val_accuracy: 0.9798\n",
      "Epoch 20/500\n",
      "852/852 [==============================] - 167s 196ms/step - loss: 0.0043 - accuracy: 0.9779 - val_loss: 0.0648 - val_accuracy: 0.9798\n",
      "Epoch 21/500\n",
      "852/852 [==============================] - 170s 200ms/step - loss: 0.0043 - accuracy: 0.9779 - val_loss: 0.0843 - val_accuracy: 0.9697\n",
      "Epoch 22/500\n",
      "747/852 [=========================>....] - ETA: 26s - loss: 0.0043 - accuracy: 0.9783"
     ]
    }
   ],
   "source": [
    "# Change retaindt\n",
    "# PRECISION=-0.46\n",
    "# init_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\")\n",
    "# predictions = init_model.predict(dt[:, :-1])\n",
    "# TrueWinPred = predictions.round()\n",
    "\n",
    "\n",
    "TrueWinPred = PesemisticPrediction\n",
    "MODEL_FILE_SUFFIX=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_CNN_'\n",
    "for rrr in range(1, 10):\n",
    "    retrain_dt = dt\n",
    "    class_1_weight = TrueWinPred.mean()\n",
    "\n",
    "    import gc\n",
    "    from keras.layers import Dense, BatchNormalization, Dropout\n",
    "    from keras.models import Sequential\n",
    "    from keras.optimizers import Adam\n",
    "    from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "    index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "    class_weights = {0: 1 - class_1_weight, 1: class_1_weight}\n",
    "    gc.collect()\n",
    "\n",
    "    SizeTunner = 1.5\n",
    "    IN_DIM = retrain_dt.shape[1] - 1\n",
    "\n",
    "    inputs = Input(shape=(IN_DIM,))\n",
    "\n",
    "    x = Reshape((1, IN_DIM))(inputs)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    optimizer = Adam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=f\"{MODEL_FILE_SUFFIX}_ReCNN{rrr}.h5\", monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"saving file in: \" + f\"{MODEL_FILE_SUFFIX}_ReCNN{rrr}.h5\")\n",
    "    history = model.fit(retrain_dt[index_20pct:, :-1],\n",
    "                        TrueWinPred[index_20pct:],\n",
    "                        validation_data=(retrain_dt[:index_20pct, :-1], TrueWinPred[:index_20pct]),\n",
    "                        epochs=500,\n",
    "                        batch_size=256 * 5,  # Reduced batch size\n",
    "                        callbacks=callbacks,\n",
    "                        class_weight=class_weights,\n",
    "                        workers=8) \n",
    "\n",
    "    true_win_model=model\n",
    "    wheretosave=f\"{MODEL_FILE_SUFFIX}_ReCNN{rrr}.h5\"\n",
    "    true_win_model.save(wheretosave)\n",
    "    print(wheretosave)\n",
    "    USED_MODEL=true_win_model\n",
    "    bad_Prediction_Note=USED_MODEL.predict( dt[:, 0:-1])\n",
    "    Pred02=bad_Prediction_Note.round()\n",
    "    Original_Traget_Data=Y\n",
    "    Predicted_Data=Pred02[:,0]\n",
    "\n",
    "    BadTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "    BadModelAccuracy=hp(BadTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "    BadTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "    BadTrueWinPred_Mean=hp(BadTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "    BadLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "    BadLossPred_Mean=hp(BadLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "    BadMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "    BadMissedDeal_Mean=hp(BadMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "    BadGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "    BadGoodZero_Mean=hp(BadGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "    fiability=BadTrueWinPred_Mean + BadLossPred_Mean + BadMissedDeal_Mean + BadGoodZero_Mean\n",
    "    if( fiability == 100):print(\"good fiability\")\n",
    "    else: print(f\"check the fiability {fiability}\")\n",
    "    winratio=BadTrueWinPred_Mean/(BadLossPred_Mean+BadTrueWinPred_Mean)\n",
    "    print(f\"========= Win Ratio:{winratio*100} ====================\")\n",
    "    TrueWinPred=BadTrueWinPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install -U imbalanced-learn\n",
    "\n",
    "import gc\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Define constants\n",
    "RETRAIN_ITERATIONS = 5\n",
    "INDEX_20PCT = int(dt.shape[1] * 0.2)\n",
    "SIZE_TUNER = 3\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "CLASS_1_WEIGHT = TrueWinPred.mean()\n",
    "CLASS_WEIGHTS = {0: 1 - CLASS_1_WEIGHT, 1: CLASS_1_WEIGHT}\n",
    "BATCH_SIZE = 256 * 10\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.002\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "PATIENCE = 16\n",
    "TAKE_PROFIT = BUY_PCT\n",
    "tp100 = TAKE_PROFIT * 100\n",
    "MODEL_FILE_SUFFIX = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'\n",
    "\n",
    "### Init\n",
    "Y = dt[:, -1]\n",
    "\n",
    "class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "init_model = minicnn_model\n",
    "predictions = init_model.predict(dt[:, :-1])\n",
    "predicted_data = predictions.round().flatten()\n",
    "original_target_data = Y\n",
    "TrueWinPred = ((predicted_data == 1) & (original_target_data == 1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    SizeTunner = 0.2\n",
    "    IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "    inputs = Input(shape=(IN_DIM,))\n",
    "\n",
    "    x = Reshape((1, IN_DIM))(inputs)\n",
    "\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Conv1D(256, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(128, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(64, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = Conv1D(32, kernel_size=3, padding='same')(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model):\n",
    "    predictions = model.predict(dt[:, :-1])\n",
    "    predicted_data = predictions.round().flatten()\n",
    "\n",
    "    accuracy = (original_target_data == predicted_data).mean()\n",
    "    true_win_predictions = ((predicted_data == 1) & (original_target_data == 1)).mean()\n",
    "    loss_buy_mean = ((predicted_data == 1) & (original_target_data == 0)).mean()\n",
    "    missed_good_deal_mean = ((predicted_data == 0) & (original_target_data == 1)).mean()\n",
    "    good_zero_prediction_mean = ((predicted_data == 0) & (original_target_data == 0)).mean()\n",
    "\n",
    "    fiability = true_win_predictions + loss_buy_mean + missed_good_deal_mean + good_zero_prediction_mean\n",
    "    if fiability == 100:\n",
    "        print(\"Good fiability\")\n",
    "    else:\n",
    "        print(f\"Check the fiability: {fiability}\")\n",
    "\n",
    "    win_ratio = true_win_predictions / (loss_buy_mean + true_win_predictions)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(MODEL_FILE_SUFFIX+f'ReCNN{rrr}.h5')\n",
    "    print(f\"accuracy : {accuracy100}%\")\n",
    "    print(f\"True win prediction: {true_win_predictions100}%\")\n",
    "    print(f\"Win ratio: {win_ratio*100}%\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    return predicted_data\n",
    "\n",
    "\n",
    "for rrr in range(RETRAIN_ITERATIONS):\n",
    "    # Balance the dataset using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_resampled, y_resampled = smote.fit_resample(dt[INDEX_20PCT:, :-1], TrueWinPred[INDEX_20PCT:])\n",
    "\n",
    "    model = create_model()\n",
    "    \n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=MODEL_FILE_SUFFIX+f'ReCNN{rrr}.h5', monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=PATIENCE, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(X_resampled, y_resampled,\n",
    "                        validation_data=(dt[:INDEX_20PCT, :-1], TrueWinPred[:INDEX_20PCT]),\n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks)\n",
    "\n",
    "    best_model = create_model()\n",
    "    best_model.load_weights(MODEL_FILE_SUFFIX+f'ReCNN{rrr}.h5')\n",
    "    # Evaluate the model on the hold-out dataset\n",
    "    predictions = best_model.predict(dt[:, :-1])\n",
    "    predictions_rounded = predictions.round()\n",
    "    original_target_data = Y\n",
    "    predicted_data = predictions_rounded[:, 0]\n",
    "\n",
    "    # Compute various metrics\n",
    "    accuracy = (original_target_data == predicted_data).mean()\n",
    "    true_win_predictions = ((predicted_data == 1) & (original_target_data == 1)).mean()\n",
    "    loss_buy_mean = ((predicted_data == 1) & (original_target_data == 0)).mean()\n",
    "    missed_good_deal_mean = ((predicted_data == 0) & (original_target_data == 1)).mean()\n",
    "    good_zero_prediction_mean = ((predicted_data == 0) & (original_target_data == 0)).mean()\n",
    "\n",
    "    fiability = true_win_predictions + loss_buy_mean + missed_good_deal_mean + good_zero_prediction_mean\n",
    "    if fiability == 100:\n",
    "        print(\"Good fiability\")\n",
    "    else:\n",
    "        print(f\"Check the fiability: {fiability}\")\n",
    "\n",
    "    win_ratio = true_win_predictions / (loss_buy_mean + true_win_predictions)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(MODEL_FILE_SUFFIX+f'ReCNN{rrr}.h5')\n",
    "    print(f\"accuracy : {accuracy*100}%\")\n",
    "    print(f\"True win prediction: {true_win_predictions*100}%\")\n",
    "    print(f\"Win ratio: {win_ratio*100}%\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    # Use the true win predictions for retraining\n",
    "    TrueWinPred = ((predicted_data == 1) & (original_target_data == 1)).copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# from keras.layers import Conv1D, Flatten, Dense, BatchNormalization, Dropout\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import SGD\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.layers import Reshape\n",
    "\n",
    "# # Define the class weights\n",
    "# class_weights = {0: 1., 1: 1.}\n",
    "\n",
    "# gc.collect()\n",
    "\n",
    "# SizeTunner = 0.2\n",
    "# IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Reshape((1, IN_DIM), input_shape=(IN_DIM,)))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# model.add(Conv1D(128, kernel_size=3, activation='elu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Conv1D(64, kernel_size=3, activation='elu', padding='same'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(64, activation='elu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(32, activation='relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# print(model.summary())\n",
    "\n",
    "# optimizer = SGD(lr=0.0001, momentum=0.9)\n",
    "# model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "#                     monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=15, verbose=1)\n",
    "# ]\n",
    "\n",
    "# print(f\"saving file in: {DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5\")\n",
    "\n",
    "# # Reshape the input data to have a single channel\n",
    "# X_train = dt[index_20pct:, :-1]\n",
    "# y_train = dt[index_20pct:, -1]\n",
    "# X_val = dt[:index_20pct, :-1]\n",
    "# y_val = dt[:index_20pct, -1]\n",
    "\n",
    "\n",
    "\n",
    "# history = model.fit(X_train, y_train,\n",
    "#                     validation_data=(X_val, y_val),\n",
    "#                     epochs=30,\n",
    "#                     batch_size=256*5,\n",
    "#                     callbacks=callbacks,\n",
    "#                     class_weight=class_weights)\n",
    "\n",
    "# minicnn_model_file = f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5'\n",
    "# model.save(minicnn_model_file)\n",
    "# print(minicnn_model_file)\n",
    "# minicnn_model = load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# Define the class weights\n",
    "class_weights = {0: 3., 1: 1.}\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "SizeTunner = 3\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "model.add(Dense(int(300 * SizeTunner), activation='softplus'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(int(200 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(int(80 * SizeTunner), activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(80 * SizeTunner), activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(int(20 * SizeTunner), activation='sigmoid'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "#optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "optimizer = Nadam(lr=0.005, beta_1=0.8, beta_2=0.99, clipnorm=1.0) #faster\n",
    "#optimizer = Nadam(lr=0.0005, beta_1=0.95, beta_2=0.999, epsilon=1e-08) #optimizer more accurate\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=20, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"saving file in: \" + Model_FileName)\n",
    "history = model.fit(dt[index_20pct:, :-1],\n",
    "                    dt[index_20pct:, -1],\n",
    "                    validation_data=(dt[:index_20pct, :-1], dt[:index_20pct, -1]),\n",
    "                    epochs=500,\n",
    "                    batch_size=256*10,\n",
    "                    callbacks=callbacks,\n",
    "                    class_weight=class_weights)\n",
    "\n",
    "verydeep_model_file=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\"\n",
    "model.save(verydeep_model_file)\n",
    "print(verydeep_model_file)\n",
    "very_deep_model=load_model(f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\")\n",
    "#868/868 [==============================] - 30s 35ms/step - loss: 0.6078 - accuracy: 0.6664 - val_loss: 0.6107 - val_accuracy: 0.6639 >0.6646\n",
    "#Results after 380 min\n",
    "# Epoch 133/500\n",
    "# 347/347 [==============================] - 138s 398ms/step - loss: 0.5867 - accuracy: 0.6842 - val_loss: 0.5839 - val_accuracy: 0.6863\n",
    "# Epoch 134/500\n",
    "# 347/347 [==============================] - 137s 395ms/step - loss: 0.5865 - accuracy: 0.6843 - val_loss: 0.5845 - val_accuracy: 0.6861\n",
    "# Epoch 134: early stopping\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Define constants\n",
    "RETRAIN_ITERATIONS = 5\n",
    "INDEX_20PCT = int(dt.shape[1] * 0.2)\n",
    "SIZE_TUNER = 3\n",
    "IN_DIM = dt.shape[1] - 1\n",
    "CLASS_1_WEIGHT = TrueWinPred.mean()\n",
    "CLASS_WEIGHTS = {0: 1 - CLASS_1_WEIGHT, 1: CLASS_1_WEIGHT}\n",
    "BATCH_SIZE = 256 * 10\n",
    "EPOCHS = 500\n",
    "LEARNING_RATE = 0.002\n",
    "BETA_1 = 0.9\n",
    "BETA_2 = 0.999\n",
    "PATIENCE = 16\n",
    "TAKE_PROFIT=BUY_PCT\n",
    "tp100=TAKE_PROFIT*100\n",
    "MODEL_FILE_SUFFIX=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_Big'\n",
    "\n",
    "### Init\n",
    "Y=dt[:, -1]\n",
    "\n",
    "init_model=very_deep_model\n",
    "predictions = init_model.predict(dt[:, :-1])\n",
    "predictions_rounded = predictions.round()\n",
    "original_target_data = Y\n",
    "predicted_data = predictions_rounded[:, 0]\n",
    "TrueWinPred=((predicted_data == 1) & (original_target_data == 1))\n",
    "\n",
    "for rrr in range(RETRAIN_ITERATIONS):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    model.add(Dense(int(200 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    model.add(Dense(int(80 * SIZE_TUNER), activation='relu'))\n",
    "    model.add(Dense(int(20 * SIZE_TUNER), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Nadam(lr=LEARNING_RATE, beta_1=BETA_1, beta_2=BETA_2)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5', monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=PATIENCE, verbose=1)\n",
    "    ]\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(dt[INDEX_20PCT:, :-1], TrueWinPred[INDEX_20PCT:],\n",
    "                        validation_data=(dt[:INDEX_20PCT, :-1], TrueWinPred[:INDEX_20PCT]),\n",
    "                        epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=callbacks, class_weight=CLASS_WEIGHTS)\n",
    "\n",
    "    # Load the best model\n",
    "    best_model = Sequential()\n",
    "    best_model.add(BatchNormalization(input_shape=(IN_DIM,)))\n",
    "    best_model.add(Dense(int(200 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dropout(0.2))\n",
    "\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dropout(0.2))\n",
    "\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(BatchNormalization())\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='elu'))\n",
    "    best_model.add(Dense(int(80 * SIZE_TUNER), activation='relu'))\n",
    "    best_model.add(Dense(int(20 * SIZE_TUNER), activation='relu'))\n",
    "    best_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    best_model.load_weights(MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5')\n",
    "\n",
    "    # Evaluate the model on the hold-out dataset\n",
    "    predictions = best_model.predict(dt[:, :-1])\n",
    "    predictions_rounded = predictions.round()\n",
    "    original_target_data = Y\n",
    "    predicted_data = predictions_rounded[:, 0]\n",
    "\n",
    "    # Compute various metrics\n",
    "    accuracy = (original_target_data == predicted_data).mean()\n",
    "    true_win_predictions = ((predicted_data == 1) & (original_target_data == 1)).mean()\n",
    "    loss_buy_mean = ((predicted_data == 1) & (original_target_data == 0)).mean()\n",
    "    missed_good_deal_mean = ((predicted_data == 0) & (original_target_data == 1)).mean()\n",
    "    good_zero_prediction_mean = ((predicted_data == 0) & (original_target_data == 0)).mean()\n",
    "\n",
    "    fiability = true_win_predictions + loss_buy_mean + missed_good_deal_mean + good_zero_prediction_mean\n",
    "    if fiability == 100:\n",
    "        print(\"Good fiability\")\n",
    "    else:\n",
    "        print(f\"Check the fiability: {fiability}\")\n",
    "\n",
    "    win_ratio = true_win_predictions / (loss_buy_mean + true_win_predictions)\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    print(MODEL_FILE_SUFFIX+f'Model_Re{rrr}.h5')\n",
    "    print(f\"accuracy : {accuracy*100}%\")\n",
    "    print(f\"True win prediction: {true_win_predictions*100}%\")\n",
    "    print(f\"Win ratio: {win_ratio*100}%\")\n",
    "    print(\"---------------------------------------------------------------\")\n",
    "    # Use the true win predictions for retraining\n",
    "    TrueWinPred = ((predicted_data == 1) & (original_target_data == 1)).copy()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Win-Loss Double Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deep neural network to predict a binary outcome (win or loss) and applying class weights to the loss function. To implement cost-sensitive learning or ensemble methods, you can make the following modifications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow as tf\n",
    "# from keras.layers import Dense, Dropout, BatchNormalization\n",
    "# from keras.models import Sequential\n",
    "# from keras.optimizers import Nadam\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# # Custom cost-sensitive loss function\n",
    "# def cost_sensitive_loss(y_true, y_pred):\n",
    "#     cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    \n",
    "#     y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "#     y_pred_1_probs = 1 - y_pred_probs\n",
    "    \n",
    "#     y_true_int = tf.cast(y_true, tf.int32)\n",
    "#     cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "#     loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "\n",
    "# # Function to create the model\n",
    "# def create_model(input_dim):\n",
    "#     model = Sequential()\n",
    "#     model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "#     model.add(Dense(200, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "#     model.add(Dropout(0.2))\n",
    "\n",
    "#     model.add(Dense(80, activation='elu'))\n",
    "#     model.add(BatchNormalization())\n",
    "\n",
    "#     model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "#     return model\n",
    "\n",
    "# # Prepare the data\n",
    "# retrain_dt = dt\n",
    "# index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "# X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "# y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "# # Define the optimizer and callbacks\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the model for class 1 (win)\n",
    "# model_win = create_model(input_dim=IN_DIM)\n",
    "# model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "# model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Train the model for class 0 (loss)\n",
    "# model_loss = create_model(input_dim=IN_DIM)\n",
    "# model_loss.compile(optimizer=optimizer, loss=cost_sensitive_loss,metrics=['accuracy'])\n",
    "# model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Combine the predictions from both models\n",
    "# win_preds = model_win.predict(dt[:, 0:-1])\n",
    "# loss_preds = model_loss.predict(dt[:, 0:-1])\n",
    "\n",
    "# # Use a strategy such as averaging, voting, or another combination method\n",
    "# combined_preds = (win_preds + loss_preds) / 2\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# # Additional metrics\n",
    "# true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "# false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "# true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "# false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "# print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "# print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "# print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "# print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, concatenate\n",
    "# from keras.models import Model\n",
    "\n",
    "# # 1. Freeze the weights of model_win and model_loss\n",
    "# for layer in model_win.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# for layer in model_loss.layers:\n",
    "#     layer.trainable = False\n",
    "\n",
    "# # 2. Add an additional layer to each model to obtain the intermediate features\n",
    "# model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "# model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# # 3. Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "# combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "# combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# # 4. Train the new model to make predictions using the intermediate features from both models\n",
    "# optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "# combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# callbacks = [\n",
    "#     ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "#     EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "# ]\n",
    "\n",
    "# # Train the combined model\n",
    "# combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# # Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "# combined_preds_rounded = np.round(combined_preds)\n",
    "\n",
    "# # Calculate the accuracy and other metrics\n",
    "# y_true = Y\n",
    "# accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "# print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, Input, concatenate\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.losses import BinaryCrossentropy\n",
    "\n",
    "# Custom cost-sensitive loss function\n",
    "def cost_sensitive_loss(y_true, y_pred):\n",
    "    cost_matrix = tf.constant([[0, 1], [10, 0]], dtype=tf.float32)\n",
    "    y_pred_probs = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
    "    y_pred_1_probs = 1 - y_pred_probs\n",
    "    y_true_int = tf.cast(y_true, tf.int32)\n",
    "    cost_weights = tf.gather(cost_matrix, y_true_int[:, 0])\n",
    "    loss = -tf.reduce_mean(cost_weights * (y_true * tf.math.log(y_pred_probs) + (1 - y_true) * tf.math.log(y_pred_1_probs)))\n",
    "\n",
    "    return loss\n",
    "\n",
    "# Function to create the model\n",
    "def create_model(input_dim):\n",
    "    model = Sequential()\n",
    "    model.add(BatchNormalization(input_shape=(input_dim,)))\n",
    "    model.add(Dense(200, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(80, activation='elu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, y_train, X_val, y_val, loss_function):\n",
    "    optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "        EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "    ]\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=loss_function, metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Prepare the data\n",
    "retrain_dt = dt\n",
    "index_20pct = int(retrain_dt.shape[1] * 0.2)\n",
    "X_train, X_val = retrain_dt[index_20pct:, :-1], retrain_dt[:index_20pct, :-1]\n",
    "y_train, y_val = retrain_dt[index_20pct:,-1], retrain_dt[:index_20pct,-1]\n",
    "\n",
    "\n",
    "# Train the model for class 1 (win)\n",
    "model_win = create_model(input_dim=IN_DIM)\n",
    "model_win.compile(optimizer=optimizer, loss=cost_sensitive_loss, metrics=['accuracy'])\n",
    "model_win.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Train the model for class 0 (loss)\n",
    "model_loss = create_model(input_dim=IN_DIM)\n",
    "model_loss.compile(optimizer=optimizer, loss=BinaryCrossentropy(), metrics=['accuracy'])\n",
    "model_loss.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "\n",
    "# Freeze the weights of model_win and model_loss\n",
    "for layer in model_win.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "for layer in model_loss.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add an additional layer to each model\n",
    "# Add an additional layer to each model to obtain the intermediate features\n",
    "model_win_intermediate = Dense(64, activation='relu')(model_win.output)\n",
    "model_loss_intermediate = Dense(64, activation='relu')(model_loss.output)\n",
    "\n",
    "# Create a new model that takes the outputs of the intermediate layers from both models and combines them\n",
    "combined_input = concatenate([model_win_intermediate, model_loss_intermediate])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "combined_model = Model(inputs=[model_win.input, model_loss.input], outputs=combined_output)\n",
    "\n",
    "# Train the new model to make predictions using the intermediate features from both models\n",
    "optimizer = Nadam(lr=0.002, beta_1=0.9, beta_2=0.999)\n",
    "combined_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=Model_FileName, monitor='val_accuracy', save_best_only=True, save_weights=True),\n",
    "    EarlyStopping(monitor='val_accuracy', mode='auto', patience=16, verbose=1)\n",
    "]\n",
    "\n",
    "# Train the combined model\n",
    "combined_model.fit([X_train, X_train], y_train, validation_data=([X_val, X_val], y_val), epochs=500, batch_size=256 * 5, callbacks=callbacks)\n",
    "\n",
    "# Make predictions using the combined model\n",
    "combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Lambda\n",
    "\n",
    "# Create a new input layer for the single input\n",
    "single_input = Input(shape=(IN_DIM,))\n",
    "\n",
    "# Duplicate the input for the two original models\n",
    "input_duplication = Lambda(lambda x: tf.tile(tf.expand_dims(x, axis=1), [1, 2, 1]))(single_input)\n",
    "input_for_model_win = Lambda(lambda x: x[:, 0])(input_duplication)\n",
    "input_for_model_loss = Lambda(lambda x: x[:, 1])(input_duplication)\n",
    "\n",
    "# Feed the duplicated input into the original models\n",
    "model_win_output = model_win(input_for_model_win)\n",
    "model_loss_output = model_loss(input_for_model_loss)\n",
    "\n",
    "# Combine the outputs of the original models\n",
    "combined_input = concatenate([model_win_output, model_loss_output])\n",
    "combined_output = Dense(1, activation='sigmoid')(combined_input)\n",
    "\n",
    "# Create the new model\n",
    "single_input_combined_model = Model(inputs=single_input, outputs=combined_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_preds = single_input_combined_model.predict(dt[:, 0:-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions using the combined model\n",
    "# combined_preds = combined_model.predict([dt[:, 0:-1], dt[:, 0:-1]])\n",
    "\n",
    "combined_preds_rounded = np.round(combined_preds).reshape(-1)\n",
    "\n",
    "# Calculate the accuracy and other metrics\n",
    "y_true = Y\n",
    "accuracy = np.mean(combined_preds_rounded == y_true)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Additional metrics\n",
    "true_positive = np.mean((combined_preds_rounded == 1) & (y_true == 1))\n",
    "false_positive = np.mean((combined_preds_rounded == 1) & (y_true == 0))\n",
    "true_negative = np.mean((combined_preds_rounded == 0) & (y_true == 0))\n",
    "false_negative = np.mean((combined_preds_rounded == 0) & (y_true == 1))\n",
    "\n",
    "print(f\"True Positive Rate: {true_positive * 100:.2f}%\")\n",
    "print(f\"False Positive Rate: {false_positive * 100:.2f}%\")\n",
    "print(f\"True Negative Rate: {true_negative * 100:.2f}%\")\n",
    "print(f\"False Negative Rate: {false_negative * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very_deep_good_model=model\n",
    "wheretosave=f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_CombinedLOssWin.h5\"\n",
    "combined_model.save(wheretosave)\n",
    "print(wheretosave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def find_matching_files(path=\"/UltimeTradingBot/Data\", pattern=f\"**/*w{WINDOW_SIZE}*.h5\"):\n",
    "    pattern = os.path.join(path, pattern)\n",
    "    matching_files = []\n",
    "    \n",
    "    for file_path in glob.iglob(pattern, recursive=True):\n",
    "        if os.path.isfile(file_path):\n",
    "            matching_files.append(file_path)\n",
    "    \n",
    "    return matching_files\n",
    "\n",
    "\n",
    "pattern = f\"*/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_*\"\n",
    "path=\"/UltimeTradingBot/Data\"\n",
    "Model_lists = find_matching_files(path, pattern)\n",
    "print(Model_lists)\n",
    "import random\n",
    "random.shuffle(Model_lists)\n",
    "print(Model_lists)\n",
    "\n",
    "#####################################################################\n",
    "\n",
    "\n",
    "# try: \n",
    "#     if tests_history:\n",
    "#         print(tests_history.head)\n",
    "# except:\n",
    "#         tests_history = pd.DataFrame(columns=['Model-FileName', 'TAKE_PROFIT', 'STOP_LOSS', 'MAX_HOLDING_TIME', 'FinalValue', 'Profit', 'ROI', 'Nbr_wins', 'best_win', 'worst_loss'])\n",
    "# tests_history_filename=f\"workdir/backtest/test_all_models_sl{STOP_LOSS}_between_{start_period.date()}--{end_period.date()}.csv\"\n",
    "# pdebug(f\"saving Test in {tests_history_filename}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test On special coin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate Data\n",
    "Mlist=Model_lists\n",
    "# MAX_FORCAST_SIZE=40\n",
    "# Mlist=[              f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryDeep.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re2.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re4.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re5.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re6.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re7.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re8.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+f\"_true_win_model_Re9.h5\",\n",
    "#                        f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model'+\"_VeryBig.h5\",\n",
    "    \n",
    "#                         ]\n",
    "\n",
    "Mlist=[              \n",
    "                       '/UltimeTradingBot/Data/IS_GOOD/tp210_w240_max120min_Model_minicnn.h5'\n",
    "                                                ]\n",
    "PRECISION=-0.0\n",
    "BAD_PERIOD_START=\"2022-08-30\"\n",
    "BAD_PERIOD_END=\"2022-11-22\"\n",
    "pair_to_test=\"SOL/USDT\"\n",
    "MAX_FORCAST_SIZE=120\n",
    "\n",
    "BUY_PCT_TEST=1.5\n",
    "loc_start=0\n",
    "loc_end=1000000\n",
    "\n",
    "\n",
    "i_start=71000\n",
    "i_end=i_start+200\n",
    "\n",
    "# loc_start=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_START))\n",
    "# loc_end=df_list1m[pair_to_test].index.get_loc(pd.to_datetime(BAD_PERIOD_END))\n",
    "\n",
    "pair=pair_to_test\n",
    "OnePair_DF=maxi_expand(pair=pair,i=loc_start,j=loc_end,window=WINDOW_SIZE,metadata=MetaData,BUY_PCT=BUY_PCT_TEST,SELL_PCT=SELL_PCT,buy_function=is_high_win,\n",
    "                           w1m=6,w5m=10,w15m=50,w1h=8,w1d=7,\n",
    "                           btc_w1m=6,btc_w5m=4,btc_w15m=5,btc_w1h=12,btc_w1d=15)\n",
    "\n",
    "\n",
    "\n",
    "OnePair_DT=OnePair_DF.to_numpy()\n",
    "gc.collect()\n",
    "OnePair_DT=fixdt(OnePair_DT)\n",
    "print(OnePair_DT[0,0] == OnePair_DF.iloc[0,0])\n",
    "print(OnePair_DT[5,5] == OnePair_DF.iloc[5,5])\n",
    "hp(OnePair_DF.buy.mean(),\"Buy mean pct\")\n",
    "\n",
    "\n",
    "\n",
    "plot_data(\"Original\", pair_to_test, winratio, OnePair_DF, i_start, 600, OnePair_DF.buy,dot_color=\"r\",fig_width=20, fig_height=7)\n",
    "\n",
    "for Model_FileName in Mlist:\n",
    "    try:\n",
    "        USED_MODEL=load_model(Model_FileName)\n",
    "\n",
    "\n",
    "        OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "        for PRECISION in [-0.0,-0.1,-0.2,-0.3,-0.4,-0.41,-0.42,-0.43,-0.44,-0.45,-0.46,-0.47,-0.48,-0.49,-0.495,-0.499]:\n",
    "            print(f'################################################### Precision: {PRECISION} #######################################################')\n",
    "            OnePair_Pred=(OnePair_PredNote+PRECISION).round()\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            Original_Traget_Data=OnePair_DT[:,-1]\n",
    "            Predicted_Data=OnePair_Pred[:,0]\n",
    "            gc.collect()\n",
    "            TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "            ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "            gc.collect()\n",
    "            TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "            TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "            gc.collect()\n",
    "            LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "            LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "            gc.collect()\n",
    "\n",
    "            MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "            MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "            gc.collect()\n",
    "\n",
    "            GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "            GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "            gc.collect()\n",
    "\n",
    "            fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "            if( fiability == 100):print(\"good fiability\")\n",
    "            else: print(f\"check the fiability {fiability}\")\n",
    "            winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "            print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "\n",
    "\n",
    "            plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 600, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)\n",
    "    except:\n",
    "        print(f\"error in plotting:{Model_FileName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, 75000, 2000, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## External and other Models\n",
    "\n",
    "\n",
    "# # Mlist=[ f'{DATA_DIR}/tp{int(BUY_PCT*100)}_w{WINDOW_SIZE}_max{MAX_FORCAST_SIZE}min_Model_minicnn.h5',\n",
    "    \n",
    "# #                         ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot_data(\"Original\", pair_to_test, winratio, OnePair_DF, i_start, 600, OnePair_DF.buy,dot_color=\"r\",fig_width=20, fig_height=7)\n",
    "\n",
    "# for Model_FileName in Mlist:\n",
    "\n",
    "#     USED_MODEL=load_model(Model_FileName)\n",
    "\n",
    "\n",
    "#     OnePair_PredNote=USED_MODEL.predict( OnePair_DT[:, 0:-1])\n",
    "#     OnePair_Pred=OnePair_PredNote.round()\n",
    "\n",
    "#     gc.collect()\n",
    "\n",
    "#     Original_Traget_Data=OnePair_DT[:,-1]\n",
    "#     Predicted_Data=OnePair_Pred[:,0]\n",
    "#     gc.collect()\n",
    "#     TruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "#     ModelAccuracy=hp(TruePred.mean(),\"ModelAccuracy\")\n",
    "#     gc.collect()\n",
    "#     TrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "#     TrueWinPred_Mean=hp(TrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "#     gc.collect()\n",
    "#     LossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "#     LossPred_Mean=hp(LossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "#     gc.collect()\n",
    "\n",
    "#     MissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "#     MissedDeal_Mean=hp(MissedDealPred.mean(),\"Missed good deal off all\")\n",
    "#     gc.collect()\n",
    "\n",
    "#     GoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "#     GoodZero_Mean=hp(GoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "#     gc.collect()\n",
    "\n",
    "#     fiability=TrueWinPred_Mean + LossPred_Mean + MissedDeal_Mean + GoodZero_Mean\n",
    "#     if( fiability == 100):print(\"good fiability\")\n",
    "#     else: print(f\"check the fiability {fiability}\")\n",
    "#     winratio=TrueWinPred_Mean/(LossPred_Mean+TrueWinPred_Mean)\n",
    "\n",
    "#     print(f\"========= Win Ratio:{winratio*100} %====================\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     PREDICTION_TO_TEST=Predicted_Data\n",
    "\n",
    "\n",
    "\n",
    "#     plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 600, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start+15000, 1000, PREDICTION_TO_TEST,dot_color=\"g\",fig_width=20, fig_height=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, window_size, PREDICTION_TO_TEST, dot_color=\"g\", fig_width=20, fig_height=5):\n",
    "#     i_end = i_start + window_size\n",
    "#     mname = Model_FileName.replace(\"/UltimeTradingBot/Data\",\"\")\n",
    "#     coin = pair_to_test.replace('/', '-')\n",
    "#     mtitle = f\"{coin} WinRatio:{hp(winratio)}% - {mname}\".replace(\"/\", \"-\")\n",
    "\n",
    "#     x = np.linspace(0, 10, 500)\n",
    "#     dashes = []  # 10 points on, 5 off, 100 on, 5 off\n",
    "#     fig, ax = plt.subplots(figsize=(fig_width, fig_height))\n",
    "#     line1, = ax.plot(OnePair_DF.index[i_start:i_end], OnePair_DF.price[i_start:i_end], '-', linewidth=1,\n",
    "#                  label='price',c=\"w\")\n",
    "#     line1.set_dashes(dashes)\n",
    "#     plt.plot(OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].index, OnePair_DF[i_start:i_end][PREDICTION_TO_TEST[i_start:i_end]==1].price, 'ro',c=dot_color,markersize=5)\n",
    "#     plt.title(mtitle)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # plot_data(Model_FileName, pair_to_test, winratio, OnePair_DF, i_start, 300, PREDICTION_TO_TEST,dot_color=\"g\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XX=OnePair_DT[:100000,:-1]\n",
    "# YY=OnePair_DT[:100000,-1]\n",
    "# precision=0.0\n",
    "# # Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# # Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# # Initial_Pred_Note=model_init.predict( XX)\n",
    "# Predicted_Data=OnePair_Pred[:300000,0]\n",
    "# goodp=(Good_Prediction_Note-precision).round()\n",
    "# # badp=(Bad_Prediction_Note).round()\n",
    "# # initp=Initial_Pred_Note.round()\n",
    "\n",
    "# # Original_Traget_Data=YY\n",
    "\n",
    "# #Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "# Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "# GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "# GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "# GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "# GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "# GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "# GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "# GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "# GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "# GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "# GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "# winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "# fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "# if( fiability == 100):print(\"good fiability\")\n",
    "# else: print(f\"check the fiability {fiability}\")\n",
    "# print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_prediction(XX,YY,USEDMODEL)\n",
    "# XX=OnePair_DT[:100000,:-1]\n",
    "# YY=OnePair_DT[:100000,-1]\n",
    "\n",
    "# # Good_Prediction_Note=very_deep_good_model.predict( XX)\n",
    "# # Bad_Prediction_Note=very_deep_bad_model.predict( XX)\n",
    "# # Initial_Pred_Note=model_init.predict( XX)\n",
    "# goodp=(Good_Prediction_Note-precision).round()\n",
    "# # badp=(Bad_Prediction_Note).round()\n",
    "# # initp=Initial_Pred_Note.round()\n",
    "\n",
    "# # Original_Traget_Data=YY\n",
    "\n",
    "# #Predicted_Data=((goodp==badp|initp==goodp))[:,0]\n",
    "# Predicted_Data=(goodp)[:,0]\n",
    "\n",
    "# GoodTruePred=(Original_Traget_Data==Predicted_Data).copy()\n",
    "# GoodModelAccuracy=hp(GoodTruePred.mean(),\"ModelAccuracy\")\n",
    "\n",
    "# GoodTrueWinPred=((Predicted_Data==1) & (Original_Traget_Data==1) ).copy()\n",
    "# GoodTrueWinPred_Mean=hp(GoodTrueWinPred.mean(),\"True Win Predictions Mean of all\")\n",
    "\n",
    "# GoodLossPred=((Predicted_Data==1) & (Original_Traget_Data==0) ).copy()\n",
    "# GoodLossPred_Mean=hp(GoodLossPred.mean(),\"XXX Loss Buy Mean of all\")\n",
    "\n",
    "# GoodMissedDealPred=((Predicted_Data==0) & (Original_Traget_Data==1) ).copy()\n",
    "# GoodMissedDeal_Mean=hp(GoodMissedDealPred.mean(),\"Missed good deal off all\")\n",
    "\n",
    "# GoodGoodZeroPred=((Predicted_Data==0) & (Original_Traget_Data==0) ).copy()\n",
    "# GoodGoodZero_Mean=hp(GoodGoodZeroPred.mean(),\"Good Zero prediction Mean\")\n",
    "\n",
    "# winratio=GoodTrueWinPred_Mean/(GoodTrueWinPred_Mean+GoodLossPred_Mean)\n",
    "# fiability=GoodTrueWinPred_Mean + GoodLossPred_Mean + GoodMissedDeal_Mean + GoodGoodZero_Mean\n",
    "# if( fiability == 100):print(\"good fiability\")\n",
    "# else: print(f\"check the fiability {fiability}\")\n",
    "# print(f\"========= Win Ratio:{winratio*100} ====================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
