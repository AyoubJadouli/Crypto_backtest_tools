{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "## importation and routings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/UltimeTradingBot/Crypto_backtest_tools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-01 21:16:26.440153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-01 21:16:27.022966: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/UltimeTradingBot/Crypto_backtest_tools')\n",
    "%cd /UltimeTradingBot/Crypto_backtest_tools\n",
    "from utilities.get_data import get_historical_from_db\n",
    "from utilities.backtesting import basic_single_asset_backtest, plot_wallet_vs_asset, get_metrics\n",
    "import pandas as pd\n",
    "import ccxt\n",
    "import matplotlib.pyplot as plt\n",
    "#import ta\n",
    "import numpy as np\n",
    "import gc\n",
    "import random\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from utilities.backtesting import plot_wallet_vs_asset, get_metrics, get_n_columns, basic_multi_asset_backtest, plot_sharpe_evolution, plot_bar_by_month\n",
    "#from utilities.custom_indicators import SuperTrend\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import gc\n",
    "gc.collect()    \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "PRERR=False\n",
    "def prerr(err):\n",
    "    if PRERR:\n",
    "        print(\"\\033[0;31m Error in \"+str(sys._getframe().f_code.co_name) +\" \\033[0;33m\"+str(err))\n",
    "\n",
    "PDEBUG=True\n",
    "def pdebug(err):\n",
    "    if PDEBUG:\n",
    "        print(\"\\033[0;31m Error in \"+str(sys._getframe().f_code.co_name) +\" \\033[0;33m\"+str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIRST_NORM_FLAG=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull Global and Config Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/UltimeTradingBot/Crypto_backtest_tools\n"
     ]
    }
   ],
   "source": [
    "%cd '/UltimeTradingBot/Crypto_backtest_tools'\n",
    "Normalization_File='/UltimeTradingBot/Data/007_w10_Norm_v1.json'\n",
    "Model_FileName='/UltimeTradingBot/Data/007_w10_Model_v1.hdf5'\n",
    "#Normalization_File='w15_NoVol_Normalization.json'\n",
    "#Model_FileName='w15_NoVol_XcryptoAi_model.hdf5'\n",
    "NORM_FILE=Normalization_File\n",
    "MODEL_FILE=Model_FileName\n",
    "ALLHIST_FILE='Results_history.json'\n",
    "DATA_DIR='/UltimeTradingBot/Data/'\n",
    "\n",
    "WINDOW_SIZE=10\n",
    "window=WINDOW_SIZE\n",
    "MAX_FORCAST_SIZE=3\n",
    "BUY_PCT=0.7\n",
    "SELL_PCT=0.40\n",
    "DATA_DIR='/UltimeTradingBot/Data/'\n",
    "DATA_FILE=DATA_DIR+'w'+str(WINDOW_SIZE)+'_CryIn_NoVol.csv'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Binance_USDT_HALAL = [\n",
    "    \"BTC/USDT\",\n",
    "    \"LUNA/USDT\",\n",
    "    \"ETH/USDT\",\n",
    "    \"GMT/USDT\",\n",
    "    \"UST/USDT\",\n",
    "    \"SOL/USDT\",\n",
    "    \"APE/USDT\",\n",
    "    \"XRP/USDT\",\n",
    "    \"IDEX/USDT\",\n",
    "    \"AVAX/USDT\",\n",
    "    \"DOT/USDT\",\n",
    "    \"ADA/USDT\",\n",
    "    \"JASMY/USDT\",\n",
    "    \"TRX/USDT\",\n",
    "    \"NEAR/USDT\",\n",
    "    \"AXS/USDT\",\n",
    "    \"GAL/USDT\",\n",
    "    \"GALA/USDT\",\n",
    "    \"SHIB/USDT\",\n",
    "    \"ZIL/USDT\",\n",
    "    \"ENS/USDT\",\n",
    "    \"DOGE/USDT\",\n",
    "    \"LTC/USDT\",\n",
    "    \"EUR/USDT\",\n",
    "    \"MANA/USDT\",\n",
    "    \"DAR/USDT\",\n",
    "    \"WAVES/USDT\",\n",
    "    \"LAZIO/USDT\",\n",
    "    \"ALICE/USDT\",\n",
    "    \"ROSE/USDT\",\n",
    "    \"ZEC/USDT\",\n",
    "    \"ALGO/USDT\",\n",
    "    \"GRT/USDT\",\n",
    "    \"PSG/USDT\",\n",
    "    \"SLP/USDT\",\n",
    "    \"EOS/USDT\",\n",
    "    \"PORTO/USDT\",\n",
    "    \"ICP/USDT\",\n",
    "    \"EGLD/USDT\",\n",
    "    \"XMR/USDT\",\n",
    "    \"KDA/USDT\",\n",
    "    \"ETC/USDT\",\n",
    "    \"MBOX/USDT\",\n",
    "    \"OGN/USDT\",\n",
    "    \"AR/USDT\",\n",
    "    \"GLMR/USDT\",\n",
    "    \"LOKA/USDT\",\n",
    "    \"XLM/USDT\",\n",
    "    \"MTL/USDT\",\n",
    "    \"SNX/USDT\",\n",
    "    \"PYR/USDT\",\n",
    "    \"DASH/USDT\",\n",
    "    \"CITY/USDT\",\n",
    "    \"ASTR/USDT\",\n",
    "    \"IOTA/USDT\",\n",
    "    \"ACM/USDT\",\n",
    "    \"BAR/USDT\",\n",
    "    \"JUV/USDT\",\n",
    "    \"SYS/USDT\",\n",
    "    \"RVN/USDT\",\n",
    "    \"MBL/USDT\",\n",
    "    \"REN/USDT\",\n",
    "    \"JST/USDT\",\n",
    "    \"OMG/USDT\",\n",
    "    \"ATM/USDT\",\n",
    "    \"XEC/USDT\",\n",
    "    \"STORJ/USDT\",\n",
    "    \"ZRX/USDT\",\n",
    "    \"SRM/USDT\",\n",
    "    \"ICX/USDT\",\n",
    "    \"API3/USDT\",\n",
    "    \"ONT/USDT\",\n",
    "    \"SKL/USDT\",\n",
    "    \"MULTI/USDT\",\n",
    "    \"QTUM/USDT\",\n",
    "    \"COCOS/USDT\",\n",
    "    \"VOXEL/USDT\",\n",
    "    \"HIVE/USDT\",\n",
    "    \"KP3R/USDT\",\n",
    "    \"ATA/USDT\",\n",
    "    \"STMX/USDT\",\n",
    "    \"ADX/USDT\",\n",
    "    \"HIGH/USDT\",\n",
    "    \"NULS/USDT\",\n",
    "    \"MLN/USDT\",\n",
    "    \"YGG/USDT\",\n",
    "    \"SC/USDT\",\n",
    "    \"CKB/USDT\",\n",
    "    \"TOMO/USDT\",\n",
    "    \"STX/USDT\",\n",
    "    \"FLUX/USDT\",\n",
    "    \"DNT/USDT\",\n",
    "    \"ORN/USDT\",\n",
    "    \"PLA/USDT\",\n",
    "    \"BADGER/USDT\",\n",
    "    \"DF/USDT\",\n",
    "    \"MOB/USDT\",\n",
    "    \"LPT/USDT\",\n",
    "    \"SCRT/USDT\",\n",
    "    \"RAD/USDT\",\n",
    "    \"NMR/USDT\",\n",
    "    \"ELF/USDT\",\n",
    "    \"TORN/USDT\",\n",
    "    \"T/USDT\",\n",
    "    \"QUICK/USDT\",\n",
    "    \"LSK/USDT\",\n",
    "    \"FIDA/USDT\",\n",
    "    \"XNO/USDT\",\n",
    "    \"BTG/USDT\",\n",
    "    \"GHST/USDT\",\n",
    "    \"EPS/USDT\"\n",
    "]\n",
    "\n",
    "pair_list = Binance_USDT_HALAL\n",
    "tf = '1m'\n",
    "oldest_pair = \"BTC/USDT\"\n",
    "df_list1m = {}\n",
    "df_list1d = {}\n",
    "df_list1h = {}\n",
    "df_list5m = {}\n",
    "df_list15m = {}\n",
    "\n",
    "\n",
    "for pair in pair_list:\n",
    "    df = get_historical_from_db(ccxt.binance(), pair, '1m', path=\"./database/\")\n",
    "    df_list1m[pair] = df.loc[:]\n",
    "\n",
    "for pair in pair_list:\n",
    "    df = get_historical_from_db(ccxt.binance(), pair, '1d', path=\"./database/\")\n",
    "    df_list1d[pair] = df.loc[:]\n",
    "\n",
    "for pair in pair_list:\n",
    "    df = get_historical_from_db(ccxt.binance(), pair, '1h', path=\"./database/\")\n",
    "    df_list1h[pair] = df.loc[:]\n",
    "\n",
    "for pair in pair_list:\n",
    "    df = get_historical_from_db(ccxt.binance(), pair, '5m', path=\"./database/\")\n",
    "    df_list5m[pair] = df.loc[:]\n",
    "\n",
    "for pair in pair_list:\n",
    "    df = get_historical_from_db(\n",
    "        ccxt.binance(), pair, '15m', path=\"./database/\")\n",
    "    df_list15m[pair] = df.loc[:]\n",
    "del(df)\n",
    "df_list = df_list1m\n",
    "prerr(\"Data load 100% use df_list1d[\\\"BTC/USDT\\\"] for exemple to access\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_list1m[\"BTC/USDT\"].iloc[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validated Fonctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Binance First Candle Finders\n",
    "Creslin\n",
    "\n",
    "Get list of all IDs on binance\n",
    "Returns the first candle / launch timestamp to the minute for each\n",
    "'''\n",
    "import urllib.request\n",
    "import json\n",
    "import ccxt\n",
    "\n",
    "def all_ids():\n",
    "    # load all markets from binance into a list\n",
    "    id = 'binance'\n",
    "    exchange_found = id in ccxt.exchanges\n",
    "    if exchange_found:\n",
    "        exchange = getattr(ccxt, id)({})\n",
    "        markets = exchange.load_markets()\n",
    "        tuples = list(ccxt.Exchange.keysort(markets).items())\n",
    "\n",
    "        ids = []\n",
    "        for (k, v) in tuples:\n",
    "            ids.append(v['id'])\n",
    "\n",
    "        return ids\n",
    "\n",
    "def give_first_kline_open_stamp(interval, symbol, start_ts=1499990400000):\n",
    "        '''\n",
    "        Returns the first kline from an interval and start timestamp and symbol\n",
    "        :param interval:  1w, 1d, 1m etc - the bar length to query\n",
    "        :param symbol:    BTCUSDT or LTCBTC etc\n",
    "        :param start_ts:  Timestamp in miliseconds to start the query from\n",
    "        :return:          The first open candle timestamp\n",
    "        '''\n",
    "\n",
    "        url_stub = \"http://api.binance.com/api/v1/klines?interval=\"\n",
    "\n",
    "        #/api/v1/klines?interval=1m&startTime=1536349500000&symbol=ETCBNB\n",
    "        addInterval   = url_stub     + str(interval) + \"&\"\n",
    "        addStarttime  = addInterval   + \"startTime=\"  + str(start_ts) + \"&\"\n",
    "        addSymbol     = addStarttime + \"symbol=\"     + str(symbol)\n",
    "        url_to_get = addSymbol\n",
    "\n",
    "        kline_data = urllib.request.urlopen(url_to_get).read().decode(\"utf-8\")\n",
    "        kline_data = json.loads(kline_data)\n",
    "\n",
    "        return kline_data[0][0]\n",
    "\n",
    "\n",
    "# Get list of all IDs on binance\n",
    "def get_crypto_metadata(pair_list):\n",
    "    Binance_USDT_HALAL=pair_list\n",
    "    ids = []\n",
    "    #ids = all_ids()\n",
    "    for halalpair in Binance_USDT_HALAL:\n",
    "    #    print( halalpair.replace('/',''))\n",
    "        ids.append(halalpair.replace('/',''))\n",
    "    #print(ids)\n",
    "    MetaData=pd.DataFrame(ids)\n",
    "    MetaData[\"Pair\"]=Binance_USDT_HALAL\n",
    "    counters=0\n",
    "    for this_id in ids:\n",
    "        '''\n",
    "        Find launch Week of symbol, start at Binance launch date 2017-07-14 (1499990400000)\n",
    "        Find launch Day of symbol in week\n",
    "        Find launch minute of symbol in day\n",
    "        '''\n",
    "\n",
    "        symbol_launch_week_stamp   = give_first_kline_open_stamp('1w', this_id, 1499990400000 )\n",
    "        symbol_launch_day_stamp    = give_first_kline_open_stamp('1d', this_id, symbol_launch_week_stamp)\n",
    "        symbol_launch_minute_stamp = give_first_kline_open_stamp('1m', this_id, symbol_launch_day_stamp)\n",
    "        MetaData.loc[counters,\"launch_week_stamp\"]=str(symbol_launch_week_stamp)\n",
    "        MetaData.loc[counters,\"launch_day_stamp\"]=str(symbol_launch_day_stamp)\n",
    "        MetaData.loc[counters,\"launch_minute\"]=pd.to_datetime(symbol_launch_minute_stamp, unit='ms')\n",
    "\n",
    "        counters += 1\n",
    "\n",
    "        #print(\"Week stamp\", symbol_launch_week_stamp)\n",
    "        #print(\"Day  stamp\", symbol_launch_day_stamp)\n",
    "        #print(\"Min  stamp\", symbol_launch_minute_stamp)\n",
    "\n",
    "        print(this_id, \"launched\", symbol_launch_minute_stamp )\n",
    "    return MetaData\n",
    "    #print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaData=get_crypto_metadata(Binance_USDT_HALAL)\n",
    "MetaData = pd.read_csv(\"../Data/MetaData.csv\",index_col=0)\n",
    "#allok = pd.read_csv('D:/+DATA+/allok_w15.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MetaData.to_csv(\"D:\\+DATA+\\MetaData.csv\")\n",
    "pair_list=Binance_USDT_HALAL\n",
    "window=WINDOW_SIZE\n",
    "buy_weight=50\n",
    "sample_size=10000\n",
    "min_days=MAX_FORCAST_SIZE\n",
    "buffer_size=100000\n",
    "#MetaData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# btc1m[\"high1\"]=btc1m.loc[1:][\"high\"]\n",
    "\n",
    "def expand_row(dataframe, window=2):\n",
    "    df = dataframe.copy()\n",
    "    for i in range(1, window+1):\n",
    "        df[\"high\"+str(i)] = df[\"high\"][i:]\n",
    "        df[\"low\"+str(i)] = df[\"low\"][i:]\n",
    "        df[\"open\"+str(i)] = df[\"open\"][i:]\n",
    "        df[\"close\"+str(i)] = df[\"close\"][i:]\n",
    "        df[\"volume\"+str(i)] = df[\"volume\"][i:]\n",
    "    return df\n",
    "\n",
    "def expand_previous(dataframe, window=10):\n",
    "    df = dataframe.copy()\n",
    "    if window >= len(df):\n",
    "        for i in range(1, window+1):\n",
    "            df.loc[window:len(df),\"high-\"+str(i)]=None\n",
    "            df.loc[window:len(df),\"low-\"+str(i)]=None\n",
    "            df.loc[window:len(df),\"open-\"+str(i)]=None            \n",
    "            df.loc[window:len(df),\"close-\"+str(i)]=None            \n",
    "            df.loc[window:len(df),\"volume-\"+str(i)]=None\n",
    "        window=len(df)\n",
    "\n",
    "    for i in range(1, window+1):\n",
    "        try:\n",
    "            df.loc[window:len(df),\"high-\"+str(i)]=None\n",
    "            df[\"high-\"+str(i)].iloc[window:len(df)]=df[\"high\"][window-i:len(df)-i]\n",
    "\n",
    "            df.loc[window:len(df),\"low-\"+str(i)]=None\n",
    "            df[\"low-\"+str(i)].iloc[window:len(df)]=df[\"low\"][window-i:len(df)-i]\n",
    "\n",
    "            df.loc[window:len(df),\"open-\"+str(i)]=None\n",
    "            df[\"open-\"+str(i)].iloc[window:len(df)]=df[\"open\"][window-i:len(df)-i]            \n",
    "            \n",
    "            df.loc[window:len(df),\"close-\"+str(i)]=None\n",
    "            df[\"close-\"+str(i)].iloc[window:len(df)]=df[\"close\"][window-i:len(df)-i]            \n",
    "            \n",
    "            df.loc[window:len(df),\"volume-\"+str(i)]=None\n",
    "            df[\"volume-\"+str(i)].iloc[window:len(df)]=df[\"volume\"][window-i:len(df)-i]\n",
    "            \n",
    "            # df[\"high-\"+str(i)][i:] = df[\"high\"][i-1:]\n",
    "            # df[\"low-\"+str(i)][i:] = df[\"low\"][i-1:]\n",
    "            # df[\"open-\"+str(i)][i:] = df[\"open\"][i-1:]\n",
    "            # df[\"close-\"+str(i)][i:] = df[\"close\"][i-1:]\n",
    "            # df[\"volume-\"+str(i)][i:] = df[\"volume\"][i-1:]\n",
    "        except:\n",
    "            prerr(\"Error in     expand_previous: \" +str(i))\n",
    "    if window >= len(df): return df       \n",
    "    return df.iloc[window:]\n",
    "\n",
    "\n",
    "def expand_previous_err(dataframe, window=10):\n",
    "    df = dataframe.copy()\n",
    "    if window >= len(df):\n",
    "        for i in range(1, window+1):\n",
    "            df.loc[window:len(df),\"high-\"+str(i)]=None\n",
    "            df.loc[window:len(df),\"low-\"+str(i)]=None\n",
    "            df.loc[window:len(df),\"open-\"+str(i)]=None            \n",
    "            df.loc[window:len(df),\"close-\"+str(i)]=None            \n",
    "            df.loc[window:len(df),\"volume-\"+str(i)]=None\n",
    "        window=len(df)\n",
    "\n",
    "    for i in range(1, window+1):\n",
    "            df.loc[window:len(df),\"high-\"+str(i)]=None\n",
    "            df[\"high-\"+str(i)].iloc[window:len(df)]=df[\"high\"][window-i:len(df)-i]\n",
    "\n",
    "            df.loc[window:len(df),\"low-\"+str(i)]=None\n",
    "            df[\"low-\"+str(i)].iloc[window:len(df)]=df[\"low\"][window-i:len(df)-i]\n",
    "\n",
    "            df.loc[window:len(df),\"open-\"+str(i)]=None\n",
    "            df[\"open-\"+str(i)].iloc[window:len(df)]=df[\"open\"][window-i:len(df)-i]            \n",
    "            \n",
    "            df.loc[window:len(df),\"close-\"+str(i)]=None\n",
    "            df[\"close-\"+str(i)].iloc[window:len(df)]=df[\"close\"][window-i:len(df)-i]            \n",
    "            \n",
    "            df.loc[window:len(df),\"volume-\"+str(i)]=None\n",
    "            df[\"volume-\"+str(i)].iloc[window:len(df)]=df[\"volume\"][window-i:len(df)-i]\n",
    "            \n",
    "            # df[\"high-\"+str(i)][i:] = df[\"high\"][i-1:]\n",
    "            # df[\"low-\"+str(i)][i:] = df[\"low\"][i-1:]\n",
    "            # df[\"open-\"+str(i)][i:] = df[\"open\"][i-1:]\n",
    "            # df[\"close-\"+str(i)][i:] = df[\"close\"][i-1:]\n",
    "            # df[\"volume-\"+str(i)][i:] = df[\"volume\"][i-1:]\n",
    "    if window >= len(df):\n",
    "        return df\n",
    "    return df.iloc[window:]\n",
    "\n",
    "def expand_timeframe(df_minutes,df_hours, window=2):\n",
    "    dfm = df_minutes.copy()\n",
    "    for j in range(1, window+1):\n",
    "        for i in df_hours[dfm.iloc[0].name:].index:\n",
    "        #prerr(str(i))\n",
    "            try:\n",
    "                dfm.loc[pd.date_range(str(i), periods=60, freq=\"min\"),\"high_1h-\"+str(j)]= df_hours[str(i-pd.Timedelta(str(j)+\" hour\"))]['high']\n",
    "                dfm.loc[pd.date_range(str(i), periods=60, freq=\"min\"),\"low_1h-\"+str(j)]= df_hours[str(i-pd.Timedelta(str(j)+\" hour\"))]['low']\n",
    "                dfm.loc[pd.date_range(str(i), periods=60, freq=\"min\"),\"open_1h-\"+str(j)]= df_hours[str(i-pd.Timedelta(str(j)+\" hour\"))]['open']\n",
    "                dfm.loc[pd.date_range(str(i), periods=60, freq=\"min\"),\"close_1h-\"+str(j)]= df_hours[str(i-pd.Timedelta(str(j)+\" hour\"))]['close']\n",
    "            except:\n",
    "                prerr(\"Error Merging: \"+str(i))\n",
    "    \n",
    "    return dfm\n",
    "\n",
    "\n",
    "def float_or_not(var):\n",
    "    try:\n",
    "        x=float(var)\n",
    "    except:\n",
    "        x=None\n",
    "    return x\n",
    "\n",
    "def expand_to_1h(df_1m,df_1h, window=2):\n",
    "    dfm = df_1m.copy()\n",
    "    index_start=df_1h.index.intersection(dfm.index.round(freq='H'))\n",
    "    for i in index_start:\n",
    "        for j in range(1, window+1):\n",
    "            # try:    \n",
    "                timefragment=dfm.index.intersection(pd.date_range(str(i), periods=60, freq=\"min\"))\n",
    "                dfm.loc[timefragment,\"high_1h-\"+str(j)]=float_or_not(df_1h.loc[str(i-pd.Timedelta(str(j)+\" hour\"))]['high'])\n",
    "                dfm.loc[timefragment,\"low_1h-\"+str(j)]=float_or_not(df_1h.loc[str(i-pd.Timedelta(str(j)+\" hour\"))]['low'])\n",
    "                dfm.loc[timefragment,\"open_1h-\"+str(j)]=float_or_not(df_1h.loc[str(i-pd.Timedelta(str(j)+\" hour\"))]['open'])\n",
    "                dfm.loc[timefragment,\"close_1h-\"+str(j)]=float_or_not(df_1h.loc[str(i-pd.Timedelta(str(j)+\" hour\"))]['close'])\n",
    "            # except:\n",
    "            #     prerr(\"error fonction \"str(i))\n",
    "    return dfm\n",
    "\n",
    "def expand_to_4h(df_1m,df_4h, window=2):\n",
    "    dfm = df_1m.copy()\n",
    "    #index_start=df_1h[str(dfm.iloc[0].name.round(freq='H')):].index.intersection(dfm.index)\n",
    "    index_start=df_4h.index.intersection(dfm.index.round(freq='4H'))\n",
    "    for i in index_start:\n",
    "        for j in range(1, window+1):\n",
    "            # try:    \n",
    "                timefragment=dfm.index.intersection(pd.date_range(str(i), periods=4*60, freq=\"min\"))\n",
    "                dfm.loc[timefragment,\"high_4h-\"+str(j)]=float_or_not(df_4h.loc[str(i-pd.Timedelta(str(j*4)+\" hour\"))]['high'])\n",
    "                dfm.loc[timefragment,\"low_4h-\"+str(j)]= float_or_not(df_4h.loc[str(i-pd.Timedelta(str(j*4)+\" hour\"))]['low'])\n",
    "                dfm.loc[timefragment,\"open_4h-\"+str(j)]= float_or_not(df_4h.loc[str(i-pd.Timedelta(str(j*4)+\" hour\"))]['open'])\n",
    "                dfm.loc[timefragment,\"close_4h-\"+str(j)]= float_or_not(df_4h.loc[str(i-pd.Timedelta(str(j*4)+\" hour\"))]['close'])\n",
    "            # except:\n",
    "            #     prerr(\"error fonction \"str(i))\n",
    "    return dfm\n",
    "\n",
    "def expand_to_1d(df_1m,df_1d, window=2,time_suffix=\"1d\"):\n",
    "    dfm = df_1m.copy()\n",
    "    index_start=df_1d.index.intersection(dfm.index.round(freq='1d'))\n",
    "    for i in index_start:\n",
    "        for j in range(1, window+1):\n",
    "            # try:    \n",
    "                prerr(i)\n",
    "                timefragment=dfm.index.intersection(pd.date_range(str(i), periods=24*60, freq=\"min\"))\n",
    "                dfm.loc[timefragment,\"high_\"+time_suffix+\"-\"+str(j)]=float_or_not(df_1d.loc[str(i-pd.Timedelta(str(j)+\" day\"))]['high'])\n",
    "                dfm.loc[timefragment,\"low_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_1d.loc[str(i-pd.Timedelta(str(j)+\" day\"))]['low'])\n",
    "                dfm.loc[timefragment,\"open_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_1d.loc[str(i-pd.Timedelta(str(j)+\" day\"))]['open'])\n",
    "                dfm.loc[timefragment,\"close_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_1d.loc[str(i-pd.Timedelta(str(j)+\" day\"))]['close'])\n",
    "            # except:\n",
    "            #     prerr(\"error fonction \"str(i))\n",
    "    return dfm\n",
    "\n",
    "def expand_to_5m(df_1m,df_5m, window=2,time_suffix=\"5m\"):\n",
    "    dfm = df_1m.copy()\n",
    "    index_start=df_5m.index.intersection(dfm.index.round(freq='5 min'))\n",
    "    for i in index_start:\n",
    "        for j in range(1, window+1):\n",
    "            # try:    \n",
    "                \n",
    "                timefragment=dfm.index.intersection(pd.date_range(str(i), periods=5, freq=\"min\"))\n",
    "                dfm.loc[timefragment,\"high_\"+time_suffix+\"-\"+str(j)]=float_or_not(df_5m.loc[str(i-pd.Timedelta(str(j*5)+\" min\"))]['high'])\n",
    "                dfm.loc[timefragment,\"low_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_5m.loc[str(i-pd.Timedelta(str(j*5)+\" min\"))]['low'])\n",
    "                dfm.loc[timefragment,\"open_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_5m.loc[str(i-pd.Timedelta(str(j*5)+\" min\"))]['open'])\n",
    "                dfm.loc[timefragment,\"close_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_5m.loc[str(i-pd.Timedelta(str(j*5)+\" min\"))]['close'])\n",
    "            # except:\n",
    "            #     prerr(\"error fonction \"str(i))\n",
    "    return dfm\n",
    "\n",
    "\n",
    "def expand_to_15m(df_1m,df_15m, window=2,time_suffix=\"15m\"):\n",
    "    dfm = df_1m.copy()\n",
    "    index_start=df_15m.index.intersection(dfm.index.round(freq='5 min'))\n",
    "    for i in index_start:\n",
    "        for j in range(1, window+1):    \n",
    "            # try:    \n",
    "                timefragment=dfm.index.intersection(pd.date_range(str(i), periods=15, freq=\"min\"))\n",
    "                dfm.loc[timefragment,\"high_\"+time_suffix+\"-\"+str(j)]=float_or_not(df_15m.loc[str(i-pd.Timedelta(str(j*15)+\" min\"))]['high'])\n",
    "                dfm.loc[timefragment,\"low_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_15m.loc[str(i-pd.Timedelta(str(j*15)+\" min\"))]['low'])\n",
    "                dfm.loc[timefragment,\"open_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_15m.loc[str(i-pd.Timedelta(str(j*15)+\" min\"))]['open'])\n",
    "                dfm.loc[timefragment,\"close_\"+time_suffix+\"-\"+str(j)]= float_or_not(df_15m.loc[str(i-pd.Timedelta(str(j*15)+\" min\"))]['close'])\n",
    "            # except:\n",
    "            #     prerr(\"error fonction \"str(i))\n",
    "    return dfm\n",
    "\n",
    "def rapid1d_expand(df1m,df1d,window=2):\n",
    "    d1min=df1m.copy()\n",
    "    d1day=df1d.loc[\n",
    "    d1min.index[0].round(freq='1d'):\n",
    "    d1min.index[len(d1min)-1].round(freq='1d')+pd.Timedelta('1 day')\n",
    "    ].copy()\n",
    "    d1day_pre=expand_previous(d1day,window)\n",
    "    d1day_pre=d1day_pre.drop(columns=['open', 'low','close','high','volume'])\n",
    "    d1day_pre=d1day_pre.add_suffix(\"_day\")\n",
    "    d1min=pd.merge_asof(\n",
    "    d1min, d1day_pre, on=None, left_on=None, right_on=None, left_index=True, \n",
    "    right_index=True, by=None, left_by=None, right_by=None, \n",
    "    suffixes=('', '_day'),\n",
    "    tolerance=pd.Timedelta('1 day'), allow_exact_matches=True, direction='backward')\n",
    "    return d1min\n",
    "\n",
    "def rapid1h_expand(df1m,df1h,window=2):\n",
    "    d1min=df1m.copy()\n",
    "    d1hour=df1h.loc[\n",
    "    d1min.index[0].round(freq='H'):\n",
    "    d1min.index[len(d1min)-1].round(freq='H')+pd.Timedelta('1 hour')\n",
    "    ].copy()\n",
    "    d1hour_pre=expand_previous(d1hour,window)\n",
    "    d1hour_pre=d1hour_pre.drop(columns=['open', 'low','close','high','volume'])\n",
    "    d1hour_pre=d1hour_pre.add_suffix(\"_hour\")\n",
    "    d1min=pd.merge_asof(\n",
    "    d1min, d1hour_pre, on=None, left_on=None, right_on=None, left_index=True, \n",
    "    right_index=True, by=None, left_by=None, right_by=None, \n",
    "    suffixes=('', '_hour'),\n",
    "    tolerance=pd.Timedelta('1 hour'), allow_exact_matches=True, direction='backward')\n",
    "    return d1min\n",
    "\n",
    "\n",
    "def rapid5m_expand(df1m,df5m,window=2):\n",
    "    d1min=df1m.copy()\n",
    "    d5min=df5m.loc[\n",
    "    d1min.index[0].round(freq='5 min'):\n",
    "    d1min.index[len(d1min)-1].round(freq='5 min')+pd.Timedelta('5 min')\n",
    "    ].copy()\n",
    "    d5min_pre=expand_previous(d5min,window)\n",
    "    d5min_pre=d5min_pre.drop(columns=['open', 'low','close','high','volume'])\n",
    "    d5min_pre=d5min_pre.add_suffix(\"_5min\")\n",
    "    d1min=pd.merge_asof(\n",
    "    d1min, d5min_pre, on=None, left_on=None, right_on=None, left_index=True, \n",
    "    right_index=True, by=None, left_by=None, right_by=None, \n",
    "    suffixes=('', '_5min'),\n",
    "    tolerance=pd.Timedelta('5 min'), allow_exact_matches=True, direction='backward')\n",
    "    return d1min\n",
    "\n",
    "def rapid15m_expand(df1m,df15m,window=2):\n",
    "    d1min=df1m.copy()\n",
    "    d15min=df15m.loc[\n",
    "    d1min.index[0].round(freq='15 min'):\n",
    "    d1min.index[len(d1min)-1].round(freq='15 min')+pd.Timedelta('15 min')\n",
    "    ].copy()\n",
    "    d15min_pre=expand_previous(d15min,window)\n",
    "    d15min_pre=d15min_pre.drop(columns=['open', 'low','close','high','volume'])\n",
    "    d15min_pre=d15min_pre.add_suffix(\"_15min\")\n",
    "    d1min=pd.merge_asof(\n",
    "    d1min, d15min_pre, on=None, left_on=None, right_on=None, left_index=True, \n",
    "    right_index=True, by=None, left_by=None, right_by=None, \n",
    "    suffixes=('', '_15min'),\n",
    "    tolerance=pd.Timedelta('15 min'), allow_exact_matches=True, direction='backward')\n",
    "    return d1min\n",
    "\n",
    "def full_expand(df1m,df5m,df15m,df1h,df1d,window=10):\n",
    "    d1min=df1m.copy()\n",
    "    d1min=expand_previous(d1min,window=window)\n",
    "    d1min=rapid1d_expand(d1min,df1d,window)\n",
    "    d1min=rapid1h_expand(d1min,df1h,window)\n",
    "    d1min=rapid15m_expand(d1min,df15m,window)\n",
    "    d1min=rapid5m_expand(d1min,df5m,window)\n",
    "    return d1min\n",
    "\n",
    "def day_expand(data_full):\n",
    "    ser = pd.to_datetime(pd.Series(data_full.index))\n",
    "    data_full[\"day\"]=ser.dt.isocalendar().day.values\n",
    "    data_full[\"hour\"]=ser.dt.hour.values\n",
    "    data_full[\"minute\"]=ser.dt.minute.values\n",
    "\n",
    "# merging\n",
    "def pair_btc(pair=\"LTC/USDT\",window=2):\n",
    "    Pair_Full=full_expand(df_list1m[pair],df_list5m[pair],df_list15m[pair],df_list1h[pair],df_list1d[pair],window)\n",
    "    BTC_Full=full_expand(\n",
    "        df_list1m[\"BTC/USDT\"].loc[df_list1m[pair].iloc[0].name:\n",
    "        df_list1m[pair].iloc[len(df_list1m[pair])-1].name],\n",
    "        df_list5m[\"BTC/USDT\"],df_list15m[\"BTC/USDT\"],df_list1h[\"BTC/USDT\"],df_list1d[\"BTC/USDT\"],window)   \n",
    "    BTC_Full=BTC_Full.add_prefix(\"BTC_\")\n",
    "    Merged=pd.merge(Pair_Full, BTC_Full, left_index=True, how='outer',\n",
    "            right_index=True, suffixes=('', ''))\n",
    "    day_expand(Merged)\n",
    "    return Merged\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def buy_results(df,min_pct=BUY_PCT):\n",
    "    mino=min_pct*0.01\n",
    "    df[\"buy\"]=(\n",
    "        ((df[\"high\"].shift(periods=1, freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino )| ((\n",
    "          df[\"high\"].shift(periods=2, freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino )| ((\n",
    "          df[\"high\"].shift(periods=3, freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino)\n",
    "    ).replace({False: 0, True: 1}) \n",
    " \n",
    "def buy_results_gen(df,min_pct=BUY_PCT,window=3):\n",
    "    mino=min_pct*0.01\n",
    "    codep1='df[\"buy\"]=((('\n",
    "    for i in range(1,window):\n",
    "        codep1=codep1+'df[\"high\"].shift(periods='+str(i)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino )| (('\n",
    "    codep2='df[\"high\"].shift(periods='+str(window)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino)).replace({False: 0, True: 1})'\n",
    "    code=codep1+codep2\n",
    "    print(code)\n",
    "    exec(code)\n",
    "\n",
    "def buy_sell(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=3):\n",
    "    mino=BUY_PCT*0.01\n",
    "    maxo=-SELL_PCT*0.01\n",
    "    codep1='df[\"buy\"]=((('\n",
    "    for i in range(1,window):\n",
    "        codep1=codep1+'df[\"high\"].shift(periods='+str(i)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino )| (('\n",
    "    codep2='df[\"high\"].shift(periods='+str(window)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] >=mino)).replace({False: 0, True: 1})'\n",
    "    code=codep1+codep2\n",
    "    prerr(code)\n",
    "    exec(code)\n",
    "    codep1='df[\"sell\"]=((df[\"buy\"]==0)&(( '\n",
    "    for i in range(1,window):\n",
    "        codep1=codep1+'df[\"high\"].shift(periods='+str(i)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] <=maxo )& (('\n",
    "    codep2='df[\"high\"].shift(periods='+str(window)+', freq=None, axis=0, fill_value=None)-df[\"high\"])/df[\"high\"] <=maxo )).replace({False: 0, True: 1})'\n",
    "    code=codep1+codep2\n",
    "    prerr(code)\n",
    "    exec(code)\n",
    "    df[\"bs\"]=((df['buy']==1 )& (df['sell']==0)).replace({False: 0, True: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def Meta_expand(data_full,metadt,pair):\n",
    "    data_full[\"lunch_day\"]=int(-(pd.to_datetime(metadt[metadt[\"Pair\"] == pair][\"launch_minute\"])-pd.Timestamp('2020-01-01 00:00:00.000000')).dt.days)\n",
    "\n",
    "def mini_expand(pair=\"LTC/USDT\",i=0,j=10000,window=2):\n",
    "    Pair_Full=full_expand(df_list1m[pair].iloc[i:j],df_list5m[pair],df_list15m[pair],df_list1h[pair],df_list1d[pair],window)\n",
    "    BTC_Full=full_expand(\n",
    "        df_list1m[\"BTC/USDT\"].loc[Pair_Full.iloc[0].name-pd.Timedelta(str(window-1) +\" min\"):Pair_Full.iloc[len(Pair_Full)-1].name],\n",
    "        df_list5m[\"BTC/USDT\"],\n",
    "        df_list15m[\"BTC/USDT\"],\n",
    "        df_list1h[\"BTC/USDT\"],\n",
    "        df_list1d[\"BTC/USDT\"],\n",
    "        window)   \n",
    "    BTC_Full=BTC_Full.add_prefix(\"BTC_\")\n",
    "    Merged=pd.merge(Pair_Full, BTC_Full, left_index=True, how='left',\n",
    "            right_index=True, suffixes=('', ''))\n",
    "    day_expand(Merged)\n",
    "    return Merged\n",
    "\n",
    "def mini_expand2(pair=\"LTC/USDT\",i=0,j=10000,window=2,metadata=MetaData):\n",
    "    Pair_Full=full_expand(df_list1m[pair].iloc[i:j],df_list5m[pair],df_list15m[pair],df_list1h[pair],df_list1d[pair],window)\n",
    "    BTC_Full=full_expand(\n",
    "        df_list1m[\"BTC/USDT\"].loc[Pair_Full.iloc[0].name-pd.Timedelta(str(window-1) +\" min\"):Pair_Full.iloc[len(Pair_Full)-1].name],\n",
    "        df_list5m[\"BTC/USDT\"],\n",
    "        df_list15m[\"BTC/USDT\"],\n",
    "        df_list1h[\"BTC/USDT\"],\n",
    "        df_list1d[\"BTC/USDT\"],\n",
    "        window)   \n",
    "    BTC_Full=BTC_Full.add_prefix(\"BTC_\")\n",
    "    Merged=pd.merge(Pair_Full, BTC_Full, left_index=True, how='left',\n",
    "            right_index=True, suffixes=('', ''))\n",
    "    day_expand(Merged)\n",
    "    Meta_expand(Merged,metadata,pair)\n",
    "    buy_sell(Merged,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=7)\n",
    "    return Merged\n",
    "\n",
    "def mini_expand3(pair=\"LTC/USDT\",i=0,j=10000,window=2,metadata=MetaData,high_weight=3):\n",
    "    Pair_Full=full_expand(df_list1m[pair].iloc[i:j],df_list5m[pair],df_list15m[pair],df_list1h[pair],df_list1d[pair],window)\n",
    "    BTC_Full=full_expand(\n",
    "        df_list1m[\"BTC/USDT\"].loc[Pair_Full.iloc[0].name-pd.Timedelta(str(window-1) +\" min\"):Pair_Full.iloc[len(Pair_Full)-1].name],\n",
    "        df_list5m[\"BTC/USDT\"],\n",
    "        df_list15m[\"BTC/USDT\"],\n",
    "        df_list1h[\"BTC/USDT\"],\n",
    "        df_list1d[\"BTC/USDT\"],\n",
    "        window)   \n",
    "    BTC_Full=BTC_Full.add_prefix(\"BTC_\")\n",
    "    Merged=pd.merge(Pair_Full, BTC_Full, left_index=True, how='left',\n",
    "            right_index=True, suffixes=('', ''))\n",
    "    day_expand(Merged)\n",
    "    Meta_expand(Merged,metadata,pair)\n",
    "    buy_sell(Merged,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=MAX_FORCAST_SIZE)\n",
    "    Merged[\"high\"]=(Merged[\"open\"]+high_weight*Merged[\"high\"]+Merged[\"low\"]+Merged[\"close\"])/(3+high_weight)\n",
    "    Merged.rename(columns={\"high\":\"price\"},inplace = True)\n",
    "    Merged[\"BTC_high\"]=(Merged[\"BTC_open\"]+high_weight*Merged[\"BTC_high\"]+Merged[\"BTC_low\"]+Merged[\"BTC_close\"])/(3+high_weight)\n",
    "    Merged.rename(columns={\"BTC_high\":\"BTC_price\"},inplace = True)\n",
    "    Merged=Merged.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "    for key in Merged.keys():\n",
    "        if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "        key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            Merged[key]=(Merged[\"BTC_price\"]-Merged[key])/Merged[\"BTC_price\"]\n",
    "        if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "        key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "            Merged[key]=(Merged[\"price\"]-Merged[key])/Merged[\"price\"]\n",
    "    return Merged\n",
    "\n",
    "\n",
    "\n",
    "def slow_expand(pair=\"LTC/USDT\",i=0,j=100000,window=3):\n",
    "    df=mini_expand(pair=pair,i=i,j=j,window=window)\n",
    "    for mx in range(1,int(len(df_list1m[pair])/j)+1) :\n",
    "        df=pd.concat([df,\n",
    "        mini_expand(pair=pair,\n",
    "        i=(mx*j)-window,\n",
    "        j=(mx+1)*j,\n",
    "        window=window)],axis=0)\n",
    "    return df\n",
    "\n",
    "def pair_data_gen(pair=\"LTC/USDT\",i=0,j=100000,window=3,metadata=MetaData):\n",
    "    df=mini_expand3(pair=pair,i=i,j=j,window=window,metadata=metadata)\n",
    "    for mx in range(1,int(len(df_list1m[pair])/j)+1) :\n",
    "        df=pd.concat([df,\n",
    "        mini_expand3(pair=pair,\n",
    "        i=(mx*j)-window,\n",
    "        j=(mx+1)*j,\n",
    "        window=window,metadata=metadata)],axis=0)\n",
    "        # Meta_expand(df,metadata,pair)\n",
    "        # buy_sell(df,BUY_PCT=BUY_PCT,SELL_PCT=SELL_PCT,window=7)\n",
    "        #print(\"loop \"+str(mx)+\"--> size of df: \"+str(len(df)))\n",
    "    return df\n",
    "\n",
    "###\n",
    "\n",
    "def data_is_enough(df,days=10,window=10):\n",
    "    if days <= window:\n",
    "        return df[~df.isnull().any(axis=1) |(df[\"open-\"+str(days)+\"_day\"].isnull() & ~df[\"open-\"+str(window-1)+\"_hour\"].isnull()  & ~df[\"open-\"+str(window-1)+\"_5min\"].isnull() & ~df[\"open-\"+str(days-1)+\"_day\"].isnull())]\n",
    "    else:\n",
    "        prerr(\"number of days must be equal or lower than window\")\n",
    "        return df\n",
    "\n",
    "def data_cleanup(df):\n",
    "    return df[~df.isnull().any(axis=1)]\n",
    "    \n",
    "\n",
    "def data_shufler(df):\n",
    "    x = len(df)\n",
    "    df[\"num_index\"] = range(0, x, 1)\n",
    "    df.set_index(df['num_index'], inplace=True)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "    df= df.drop(\"num_index\",axis=1)\n",
    "    df = df.reindex(np.random.permutation(df.index))\n",
    "    return df\n",
    "    \n",
    "def data_np_shufler(df):\n",
    "    df = shuffle(df)\n",
    "    #df = df.reindex(np.random.permutation(df.index))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def data_chooser(df,weight=50,row_numbers=100000):\n",
    "    df=data_shufler(df)\n",
    "    if row_numbers>=len(df):\n",
    "        row_numbers=len(df)\n",
    "    df=pd.concat([df[df[\"buy\"]==1].iloc[:int(row_numbers*weight*0.01)],\n",
    "                 df[df[\"buy\"]==0].iloc[:int(row_numbers*(100-weight)*0.01)]])\n",
    "    df=data_shufler(df)\n",
    "    return df\n",
    "    \n",
    "\n",
    "def data_looper(pair_list=Binance_USDT_HALAL,window=15,buy_weight=50,sample_size=100000,min_days=10,buffer_size=100000):\n",
    "    xdf=pd.DataFrame()\n",
    "    for pair in pair_list:\n",
    "        if pair != \"BTC/USDT\":\n",
    "            print(\"working on: \"+pair)\n",
    "            df=pair_data_gen(pair=pair,i=0,j=buffer_size,window=window)\n",
    "            gc.collect()\n",
    "\n",
    "            df=data_is_enough(df,days=min_days,window=window)\n",
    "            gc.collect()\n",
    "\n",
    "            df=data_chooser(df,weight=buy_weight,row_numbers=sample_size)\n",
    "            gc.collect()\n",
    "\n",
    "            print(pair+\" is processed\")\n",
    "            xdf=pd.concat([xdf,df],axis=0)\n",
    "            del(df)\n",
    "            gc.collect()\n",
    "        else:\n",
    "            print(\"ignore BTC\")\n",
    "    return xdf\n",
    "\n",
    "def volume_cleaner(df):\n",
    "    VolRemover=[\"volume\",\"volume-1\",\"BTC_volume\",\"BTC_volume-1\"]\n",
    "    for key in df.keys():\n",
    "        if key.find(\"volume-1_\") != -1 :\n",
    "            VolRemover.append(key)\n",
    "    df=df.drop(columns=VolRemover)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test=mini_expand3(pair=\"LTC/USDT\",i=0,j=10000,window=100,metadata=MetaData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.isnull().any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Data genration Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "ww=WINDOW_SIZE\n",
    "gc.collect()\n",
    "df=data_looper(pair_list=Binance_USDT_HALAL,window=ww,buy_weight=50,sample_size=30000,min_days=3,buffer_size=300000)\n",
    "gc.collect()\n",
    "df=volume_cleaner(df)\n",
    "#df=data_shufler(df)\n",
    "#df=df.reindex(np.random.permutation(df.index))\n",
    "#df=df.reindex(np.random.permutation(df.index))\n",
    "gc.collect()\n",
    "df[\"num_index2\"] = range(0, len(df), 1)\n",
    "df.set_index(df['num_index2'], inplace=True)\n",
    "gc.collect()\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "gc.collect()\n",
    "df= df.drop(\"num_index2\",axis=1)\n",
    "try:\n",
    "    df= df.drop(\"num_index\",axis=1)\n",
    "except:\n",
    "    print(\"no numindex \")\n",
    "gc.collect()\n",
    "\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "df[\"index\"] = range(0, len(df), 1)\n",
    "df.set_index(df['index'], inplace=True)\n",
    "try:\n",
    "    df= df.drop(\"num_index2\",axis=1)\n",
    "except:\n",
    "    print(\"no numindex 2\")\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    df= df.drop(\"index\",axis=1)\n",
    "except:\n",
    "    print(\"index\")\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    df= df.drop([\"num_index2\",\"index\"],axis=1,index=False)\n",
    "except:\n",
    "    print(\"no numindex 2\")\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA_DIR+str(ww)+'_x3experments.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window=20\n",
    "#pair=\"GAL/USDT\"\n",
    "#df=mini_expand(\"LTC/USDT\",i=0,j=100000,window=20)\n",
    "#df=full_expand(df_list1m[pair].iloc[0:10000],df_list5m[pair],df_list15m[pair],df_list1h[pair],df_list1d[pair],window=20)\n",
    "#df=pair_data_gen(pair,i=0,j=100000,window=20,metadata=MetaData)\n",
    "#df=data_is_enough(df,days=min_days,window=window)\n",
    "#df=data_chooser(df,weight=20,row_numbers=100000)\n",
    "\n",
    "#df = expand_previous_err(df_list1d[pair],20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_list1d[pair]\n",
    "#df[\"high-1\"].iloc[window:len(df)]=df[\"high\"][window-i:len(df)-i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.reindex(np.random.permutation(df.index))\n",
    "#df=df.drop(\"num_index\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[100000][\"open-9_hour\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.out  df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df.isnull().any(axis=1) & (df[\"buy\"]==1)]\n",
    "#df[(df[\"buy\"]==1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ltc1m = expand_row(df_list1m[\"LTC/USDT\"], 3)\n",
    "#sys.getsizeof(allok)/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"D:\\+DATA+\\LTCtest.xlsx\", sheet_name='LTC', na_rep='', float_format=None,\n",
    "#  columns=None, header=True, index=True, index_label=None, startrow=0, startcol=0, engine=None, merge_cells=True, encoding=None, \n",
    "# inf_rep='inf', verbose=True, freeze_panes=None, storage_options=None)\n",
    "#df.to_csv(\"D:\\+DATA+\\LTCtest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b=ccxt.binance()\n",
    "#b.set_sandbox_mode(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Meta_expand(df,MetaData,pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dffull=pd.read_csv(DATA_DIR+str(WINDOW_SIZE)+'_Testing.csv',index_col=0)\n",
    "dffull=pd.read_csv(DATA_DIR+str(WINDOW_SIZE)+'_x3experments.csv',index_col=0)\n",
    "df=dffull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dffull=df\n",
    "# df=dffull[~dffull.isnull().any(axis=1)].drop(columns=[\"sell\",'bs'])\n",
    "# df\n",
    "df=dffull.drop(columns=[\"sell\",'buy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# part2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "import ccxt\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df.drop(columns=\"sell\")\n",
    "#df=df.drop(columns=[\"Unnamed: 0\"])\n",
    "#df.to_csv('D:/+DATA+/allok_w15_nosell.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sys.getsizeof(df)/(1024**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing impoted DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VolRemover=[\"volume\",\"volume-1\",\"BTC_volume-1\"]\n",
    "# for key in df.keys():\n",
    "#     if key.find(\"volume-1_\") != -1 :\n",
    "#         VolRemover.append(key)\n",
    "\n",
    "#     df=pd.concat([df1,df0],axis=0).drop(columns=VolRemover)\n",
    "\n",
    "# high_weight=3\n",
    "# df[\"high\"]=(df[\"open\"]+high_weight*df[\"high\"]+df[\"low\"]+df[\"close\"])/(3+high_weight)\n",
    "# df.rename(columns={\"high\":\"price\"},inplace = True)\n",
    "# df[\"BTC_high\"]=(df[\"BTC_open\"]+high_weight*df[\"BTC_high\"]+df[\"BTC_low\"]+df[\"BTC_close\"])/(3+high_weight)\n",
    "# df.rename(columns={\"BTC_high\":\"BTC_price\"},inplace = True)\n",
    "# df2=df.drop(columns=[\"BTC_open\",\"BTC_low\",\"BTC_close\",\"open\",\"low\",\"close\"])\n",
    "# del(df)\n",
    "# df=df2\n",
    "# #del(df2)\n",
    "# for key in df.keys():\n",
    "#     if key.find(\"BTC\")!=-1 and (key.find(\"open\")!=-1 or\n",
    "#     key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "#         df[key]=(df[\"BTC_price\"]-df[key])/df[\"BTC_price\"]\n",
    "#     if key.find(\"BTC\")==-1 and (key.find(\"open\")!=-1 or\n",
    "#     key.find(\"high\")!=-1 or key.find(\"low\")!=-1 or key.find(\"close\")!=-1):\n",
    "#         df[key]=(df[\"price\"]-df[key])/df[\"price\"]\n",
    "\n",
    "# df1=df[df[\"buy\"]==1]\n",
    "# df0=df[df[\"buy\"]==0].iloc[0:len(df1)]\n",
    "# #del(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#df=df.iloc[0:int(len(df)/3)]\n",
    "gc.collect()\n",
    "# df=df.reindex(np.random.permutation(df.index))\n",
    "# df=df.reindex(np.random.permutation(df.index))\n",
    "# sys.getsizeof(df)/(1024**2)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     VolRemover=[\"volume\",\"volume-1\",\"BTC_volume-1\",\"BTC_volume\"]\n",
    "#     for key in df.keys():\n",
    "#         if key.find(\"volume-1_\") != -1 :\n",
    "#             VolRemover.append(key)\n",
    "\n",
    "#         df=df.drop(columns=VolRemover)\n",
    "# except:\n",
    "#     try:\n",
    "#         df=df.drop(columns=[\"BTC_volume\"])\n",
    "#     except:\n",
    "\n",
    "#         print(\"no veol\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheking null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=df[~df.isnull().any(axis=1)]\n",
    "#dffull=df\n",
    "df=dffull[~dffull.isnull().any(axis=1)].drop(columns=[\"bs\",\"sell\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "high-1|low-1|open-1|close-1|high-10|low-10|open-10|close-10|volume-10|high-1_day|low-1_day|open-1_day|close-1_day|high-10_day|low-10_day|open-10_day|close-10_day|volume-10_day|high-1_hour|low-1_hour|open-1_hour|close-1_hour|high-10_hour|low-10_hour|open-10_hour|close-10_hour|volume-10_hour|high-1_15min|low-1_15min|open-1_15min|close-1_15min|high-10_15min|low-10_15min|open-10_15min|close-10_15min|volume-10_15min|high-1_5min|low-1_5min|open-1_5min|close-1_5min|high-10_5min|low-10_5min|open-10_5min|close-10_5min|volume-10_5min|BTC_high-1|BTC_low-1|BTC_open-1|BTC_close-1|BTC_high-10|BTC_low-10|BTC_open-10|BTC_close-10|BTC_volume-10|BTC_high-1_day|BTC_low-1_day|BTC_open-1_day|BTC_close-1_day|BTC_high-10_day|BTC_low-10_day|BTC_open-10_day|BTC_close-10_day|BTC_volume-10_day|BTC_high-1_hour|BTC_low-1_hour|BTC_open-1_hour|BTC_close-1_hour|BTC_high-10_hour|BTC_low-10_hour|BTC_open-10_hour|BTC_close-10_hour|BTC_volume-10_hour|BTC_high-1_15min|BTC_low-1_15min|BTC_open-1_15min|BTC_close-1_15min|BTC_high-10_15min|BTC_low-10_15min|BTC_open-10_15min|BTC_close-10_15min|BTC_volume-10_15min|BTC_high-1_5min|BTC_low-1_5min|BTC_open-1_5min|BTC_close-1_5min|BTC_high-10_5min|BTC_low-10_5min|BTC_open-10_5min|BTC_close-10_5min|BTC_volume-10_5min|"
     ]
    }
   ],
   "source": [
    "for k in df.keys():\n",
    "    if k.find(\"-1\") != -1 :print(k ,end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# starting numpy process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert Pandas DataFrame to numpy object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dt = df.iloc[0:int(len(df/1.5))].to_numpy()\n",
    "#dt = np.concatenate((dt,df.iloc[int(len(df/2)):].to_numpy()),axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "292"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dffull\n",
    "df=df.iloc[int(len(df/1.5)):]\n",
    "\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2912567, 497)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the rows Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.getsizeof(dt)/(1024**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.savetxt('np_shuffled_cryptodata_w15.csv', dt ,delimiter=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dt=np.genfromtxt('np_shuffled_cryptodata_w15.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "728141\n"
     ]
    }
   ],
   "source": [
    "index_20pct= int(0.25*len(dt[:,0]))\n",
    "print(index_20pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "XVALIDATION= dt[:index_20pct, :-1]\n",
    "YVALIDATION= dt[:index_20pct,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTRAIN= dt[index_20pct:, 0:-1]\n",
    "YTRAIN= dt[index_20pct:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenderalization (mean normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalization=None\n",
    "def normalize(dataset,file=Normalization_File):\n",
    "    global Normalization\n",
    "    try:\n",
    "        N=Normalization\n",
    "    except:\n",
    "        Normalization=None\n",
    "        print(\"clean Var\")\n",
    "    if(Normalization==None):\n",
    "        #print('Loading normalization from file')\n",
    "        with open(file) as json_file:\n",
    "            Normalization = json.load(json_file)\n",
    "    else:\n",
    "        #print('normalization is loaded')\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "with open(Normalization_File) as json_file:\n",
    "            print(Normalization_File)\n",
    "            Normalization = json.load(json_file)\n",
    "np.array(Normalization[\"mean\"])\n",
    "# mean=np.array(Normalization[\"mean\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean=np.array(Normalization[\"mean\"])\n",
    "# std=np.array(Normalization[\"std\"])\n",
    "#Normalization=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizing ...\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "\n",
    "if FIRST_NORM_FLAG:\n",
    "    print(\"normalizing ...\")\n",
    "    mean = XTRAIN.mean(axis=0)\n",
    "    std = XTRAIN.std(axis=0)\n",
    "\n",
    "\n",
    "\n",
    "    XTRAIN -= mean \n",
    "    XTRAIN /= std\n",
    "\n",
    "    XVALIDATION -=mean\n",
    "    XVALIDATION /= std\n",
    "    FIRST_NORM_FLAG=False\n",
    "else:print(\"already normalized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## V2 Testing ######################\n",
    "\n",
    "if FIRST_NORM_FLAG:\n",
    "    print(\"normalizing ...\")\n",
    "    dt[:,0:-1] -= Normalization[\"mean\"] \n",
    "    dt[:,0:-1] /= Normalization[\"std\"] \n",
    "\n",
    "  \n",
    "    FIRST_NORM_FLAG=False\n",
    "    FIRST_NORM_FLAG=False\n",
    "else:print(\"already normalized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt[3133,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/UltimeTradingBot/Data/007_w10_Norm_v1.json\n"
     ]
    }
   ],
   "source": [
    "######################### SAVIN NORM ################\n",
    "try:\n",
    "    Normalization={\"mean\":mean.tolist(),\"std\":std.tolist()}\n",
    "    with open(Normalization_File, 'w+') as fp:\n",
    "                json.dump(Normalization, fp,  indent=4)\n",
    "                print(Normalization_File)\n",
    "except:\n",
    "    print(\"error juppiter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/UltimeTradingBot/Data/007_w10_Norm_v1.json\n"
     ]
    }
   ],
   "source": [
    "Normalization={\"mean\":mean.tolist(),\"std\":std.tolist()}\n",
    "print(Normalization_File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2184426, 496)\n",
      "(2184426,)\n",
      "(728141, 496)\n",
      "(728141,)\n"
     ]
    }
   ],
   "source": [
    "print(XTRAIN.shape)\n",
    "print(YTRAIN.shape)\n",
    "print(XVALIDATION.shape)\n",
    "print(YVALIDATION.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3414/3414 [==============================] - 67s 19ms/step - loss: 0.1488 - accuracy: 0.9401 - val_loss: 0.1433 - val_accuracy: 0.9418\n",
      "Epoch 56/6000\n",
      "3414/3414 [==============================] - 64s 19ms/step - loss: 0.1484 - accuracy: 0.9403 - val_loss: 0.1438 - val_accuracy: 0.9417\n",
      "Epoch 57/6000\n",
      "3414/3414 [==============================] - 75s 22ms/step - loss: 0.1496 - accuracy: 0.9399 - val_loss: 0.1442 - val_accuracy: 0.9413\n",
      "Epoch 58/6000\n",
      "3414/3414 [==============================] - 70s 21ms/step - loss: 0.1487 - accuracy: 0.9404 - val_loss: 0.1444 - val_accuracy: 0.9421\n",
      "Epoch 59/6000\n",
      "3414/3414 [==============================] - 64s 19ms/step - loss: 0.1494 - accuracy: 0.9399 - val_loss: 0.1453 - val_accuracy: 0.9416\n",
      "Epoch 60/6000\n",
      "3414/3414 [==============================] - 64s 19ms/step - loss: 0.1497 - accuracy: 0.9399 - val_loss: 0.1456 - val_accuracy: 0.9414\n",
      "Epoch 61/6000\n",
      "3414/3414 [==============================] - 63s 18ms/step - loss: 0.1508 - accuracy: 0.9396 - val_loss: 0.1465 - val_accuracy: 0.9413\n",
      "Epoch 62/6000\n",
      "3414/3414 [==============================] - 63s 18ms/step - loss: 0.1503 - accuracy: 0.9399 - val_loss: 0.1452 - val_accuracy: 0.9412\n",
      "Epoch 63/6000\n",
      "3414/3414 [==============================] - 62s 18ms/step - loss: 0.1512 - accuracy: 0.9393 - val_loss: 0.1469 - val_accuracy: 0.9411\n",
      "Epoch 64/6000\n",
      "3414/3414 [==============================] - 62s 18ms/step - loss: 0.1506 - accuracy: 0.9397 - val_loss: 0.1469 - val_accuracy: 0.9415\n",
      "Epoch 65/6000\n",
      "3414/3414 [==============================] - 62s 18ms/step - loss: 0.1507 - accuracy: 0.9397 - val_loss: 0.1461 - val_accuracy: 0.9413\n",
      "Epoch 66/6000\n",
      "3414/3414 [==============================] - 62s 18ms/step - loss: 0.1528 - accuracy: 0.9393 - val_loss: 0.1466 - val_accuracy: 0.9411\n",
      "Epoch 67/6000\n",
      "3414/3414 [==============================] - 62s 18ms/step - loss: 0.1522 - accuracy: 0.9393 - val_loss: 0.1477 - val_accuracy: 0.9407\n",
      "Epoch 68/6000\n",
      "3414/3414 [==============================] - 65s 19ms/step - loss: 0.1533 - accuracy: 0.9389 - val_loss: 0.1507 - val_accuracy: 0.9401\n",
      "Epoch 69/6000\n",
      "3414/3414 [==============================] - 75s 22ms/step - loss: 0.1530 - accuracy: 0.9391 - val_loss: 0.1461 - val_accuracy: 0.9410\n",
      "Epoch 70/6000\n",
      "3414/3414 [==============================] - 68s 20ms/step - loss: 0.1527 - accuracy: 0.9392 - val_loss: 0.1496 - val_accuracy: 0.9398\n",
      "Epoch 71/6000\n",
      "3414/3414 [==============================] - 69s 20ms/step - loss: 0.1534 - accuracy: 0.9390 - val_loss: 0.1481 - val_accuracy: 0.9408\n",
      "Epoch 72/6000\n",
      "3414/3414 [==============================] - 68s 20ms/step - loss: 0.1534 - accuracy: 0.9391 - val_loss: 0.1482 - val_accuracy: 0.9406\n",
      "Epoch 73/6000\n",
      "3414/3414 [==============================] - 68s 20ms/step - loss: 0.1546 - accuracy: 0.9390 - val_loss: 0.1496 - val_accuracy: 0.9404\n",
      "Epoch 74/6000\n",
      "3414/3414 [==============================] - 68s 20ms/step - loss: 0.1563 - accuracy: 0.9384 - val_loss: 0.1508 - val_accuracy: 0.9402\n",
      "Epoch 75/6000\n",
      "3414/3414 [==============================] - 67s 20ms/step - loss: 0.1554 - accuracy: 0.9385 - val_loss: 0.1509 - val_accuracy: 0.9395\n",
      "Epoch 76/6000\n",
      "3414/3414 [==============================] - 65s 19ms/step - loss: 0.1557 - accuracy: 0.9382 - val_loss: 0.1503 - val_accuracy: 0.9401\n",
      "Epoch 77/6000\n",
      "3414/3414 [==============================] - 66s 19ms/step - loss: 0.1574 - accuracy: 0.9378 - val_loss: 0.1515 - val_accuracy: 0.9394\n",
      "Epoch 78/6000\n",
      "3414/3414 [==============================] - 66s 19ms/step - loss: 0.1578 - accuracy: 0.9379 - val_loss: 0.1540 - val_accuracy: 0.9388\n",
      "Epoch 78: early stopping\n",
      "##########################################################################\n",
      "------val_accuracy-----> 94.21 | 94.04 <----------accuracy----------\n"
     ]
    }
   ],
   "source": [
    "IN_DIM=len(mean)\n",
    "model = Sequential()\n",
    "# model.add(Dense(int(IN_DIM/2),input_dim=IN_DIM,activation='relu'))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(int(IN_DIM/4),activation='relu'))\n",
    "# model.add(Dropout(0.7))\n",
    "# model.add(Dense(int(IN_DIM/8),activation='relu'))\n",
    "#model.add(Dropout(0.7))\n",
    "model.add(Dense(int(IN_DIM/2),input_dim=IN_DIM,activation='softplus'))\n",
    "model.add(Dense(int(IN_DIM/2),activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Dense(int(IN_DIM/5),activation='softplus'))\n",
    "model.add(Dense(int(IN_DIM/4),activation='softmax'))\n",
    "model.add(Dense(int(IN_DIM/1),activation='softplus'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "print(model.summary())\n",
    "#model.compile(loss='binary_crossentropy',optimizer='rmsprop',metrics=['accuracy'])\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "callbacks_a = ModelCheckpoint(filepath=Model_FileName,monitor ='val_accuracy',save_best_only = True, save_weights = True)\n",
    "callbacks_b = EarlyStopping(monitor ='val_accuracy',mode='auto',patience=20,verbose=1)\n",
    "history = model.fit(XTRAIN,\n",
    "                YTRAIN,\n",
    "                validation_data=(XVALIDATION,YVALIDATION),\n",
    "                epochs=6000,\n",
    "                batch_size=128*5,\n",
    "                callbacks=[callbacks_a,callbacks_b])\n",
    "\n",
    "print('##########################################################################')\n",
    "print(f\"------val_accuracy-----> {'{0:.4g}'.format(max(history.history['val_accuracy'])*100)} | {'{0:.4g}'.format(max(history.history['accuracy'])*100)} <----------accuracy----------\")\n",
    "#94%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##########################################################################\n",
      "------val_accuracy-----> 94.21 | 94.04 <----------accuracy----------\n"
     ]
    }
   ],
   "source": [
    "print('##########################################################################')\n",
    "print(f\"------val_accuracy-----> {'{0:.4g}'.format(max(history.history['val_accuracy'])*100)} | {'{0:.4g}'.format(max(history.history['accuracy'])*100)} <----------accuracy----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>high-1</th>\n",
       "      <th>low-1</th>\n",
       "      <th>open-1</th>\n",
       "      <th>close-1</th>\n",
       "      <th>high-2</th>\n",
       "      <th>low-2</th>\n",
       "      <th>open-2</th>\n",
       "      <th>close-2</th>\n",
       "      <th>volume-2</th>\n",
       "      <th>...</th>\n",
       "      <th>BTC_high-10_5min</th>\n",
       "      <th>BTC_low-10_5min</th>\n",
       "      <th>BTC_open-10_5min</th>\n",
       "      <th>BTC_close-10_5min</th>\n",
       "      <th>BTC_volume-10_5min</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>minute</th>\n",
       "      <th>lunch_day</th>\n",
       "      <th>buy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  497 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [price, high-1, low-1, open-1, close-1, high-2, low-2, open-2, close-2, volume-2, high-3, low-3, open-3, close-3, volume-3, high-4, low-4, open-4, close-4, volume-4, high-5, low-5, open-5, close-5, volume-5, high-6, low-6, open-6, close-6, volume-6, high-7, low-7, open-7, close-7, volume-7, high-8, low-8, open-8, close-8, volume-8, high-9, low-9, open-9, close-9, volume-9, high-10, low-10, open-10, close-10, volume-10, high-1_day, low-1_day, open-1_day, close-1_day, high-2_day, low-2_day, open-2_day, close-2_day, volume-2_day, high-3_day, low-3_day, open-3_day, close-3_day, volume-3_day, high-4_day, low-4_day, open-4_day, close-4_day, volume-4_day, high-5_day, low-5_day, open-5_day, close-5_day, volume-5_day, high-6_day, low-6_day, open-6_day, close-6_day, volume-6_day, high-7_day, low-7_day, open-7_day, close-7_day, volume-7_day, high-8_day, low-8_day, open-8_day, close-8_day, volume-8_day, high-9_day, low-9_day, open-9_day, close-9_day, volume-9_day, high-10_day, low-10_day, open-10_day, close-10_day, volume-10_day, high-1_hour, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 497 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize(dt[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(Model_FileName)\n",
    "\n",
    "print(Model_FileName+' Loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2[0:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt2=tf.convert_to_tensor(dt, dtype=tf.float32)\n",
    "accuracy = model.evaluate(dt2[0:,:-1], dt2[0:,-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# make probability predictions with the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# round predictions \u001b[39;00m\n\u001b[1;32m      4\u001b[0m rounded \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mround\u001b[39m(x[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m predictions]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py:2278\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2274\u001b[0m                 callbacks\u001b[39m.\u001b[39mon_predict_batch_end(\n\u001b[1;32m   2275\u001b[0m                     end_step, {\u001b[39m\"\u001b[39m\u001b[39moutputs\u001b[39m\u001b[39m\"\u001b[39m: batch_outputs}\n\u001b[1;32m   2276\u001b[0m                 )\n\u001b[1;32m   2277\u001b[0m     \u001b[39mif\u001b[39;00m batch_outputs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2278\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2279\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `predict_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2280\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m(Empty batch_outputs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2281\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2282\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2283\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2284\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2285\u001b[0m         )\n\u001b[1;32m   2286\u001b[0m     callbacks\u001b[39m.\u001b[39mon_predict_end()\n\u001b[1;32m   2287\u001b[0m all_outputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure_up_to(\n\u001b[1;32m   2288\u001b[0m     batch_outputs, potentially_ragged_concat, outputs\n\u001b[1;32m   2289\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `predict_function` (Empty batch_outputs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "\n",
    "# make probability predictions with the model\n",
    "predictions = model.predict(df.iloc[0:-1])\n",
    "# round predictions \n",
    "rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(rounded)/len(rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YVALIDATION[1002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Buy_Dessision(input):\n",
    "    predictions = model.predict(XVALIDATION)\n",
    "    rounded = [round(x[0]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
